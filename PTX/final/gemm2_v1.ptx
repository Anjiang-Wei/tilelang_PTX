//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN65_INTERNAL_dc2a1f9f_34_gemm_BM64_BN64_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<49>;
	.reg .f32 	%f<369>;
	.reg .b32 	%r<511>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd9, [main_kernel_param_0];
	ld.param.u64 	%rd10, [main_kernel_param_1];
	cvta.to.global.u64 	%rd20, %rd9;
	mov.u32 	%r1, %tid.x;
	shr.s32 	%r15, %r1, 2;
	shl.b32 	%r16, %r15, 5;
	shr.u32 	%r17, %r1, 4;
	and.b32  	%r18, %r1, 2;
	shr.u32 	%r19, %r18, 1;
	add.s32 	%r20, %r19, %r17;
	shl.b32 	%r21, %r20, 4;
	and.b32  	%r22, %r21, 16;
	or.b32  	%r23, %r22, %r16;
	and.b32  	%r24, %r1, 8;
	shr.u32 	%r25, %r24, 3;
	add.s32 	%r26, %r25, %r1;
	shl.b32 	%r27, %r26, 3;
	and.b32  	%r28, %r27, 8;
	or.b32  	%r29, %r23, %r28;
	shl.b32 	%r30, %r29, 1;
	mov.u32 	%r31, buf_dyn_shmem;
	add.s32 	%r2, %r31, %r30;
	mov.u32 	%r32, %ctaid.y;
	shl.b32 	%r3, %r32, 16;
	shl.b32 	%r33, %r1, 3;
	and.b32  	%r34, %r33, 24;
	shr.s32 	%r35, %r1, 31;
	shr.u32 	%r36, %r35, 28;
	add.s32 	%r37, %r1, %r36;
	and.b32  	%r38, %r37, -16;
	sub.s32 	%r39, %r1, %r38;
	shr.u32 	%r40, %r39, 31;
	add.s32 	%r41, %r39, %r40;
	shr.s32 	%r42, %r41, 1;
	shr.s32 	%r43, %r41, 31;
	shr.u32 	%r44, %r43, 30;
	add.s32 	%r45, %r42, %r44;
	and.b32  	%r46, %r45, -4;
	sub.s32 	%r47, %r42, %r46;
	shr.u32 	%r48, %r37, 31;
	shr.s32 	%r49, %r37, 4;
	add.s32 	%r50, %r49, %r48;
	and.b32  	%r51, %r50, -2;
	sub.s32 	%r52, %r49, %r51;
	shl.b32 	%r53, %r47, 6;
	and.b32  	%r54, %r53, 192;
	shl.b32 	%r55, %r52, 3;
	and.b32  	%r56, %r55, 8;
	or.b32  	%r57, %r54, %r56;
	and.b32  	%r58, %r41, 134217726;
	sub.s32 	%r59, %r39, %r58;
	shl.b32 	%r60, %r59, 5;
	shr.s32 	%r61, %r39, 31;
	shr.u32 	%r62, %r61, 29;
	add.s32 	%r63, %r39, %r62;
	shl.b32 	%r64, %r63, 5;
	and.b32  	%r65, %r64, 2147483392;
	add.s32 	%r66, %r60, %r65;
	shr.u32 	%r67, %r35, 27;
	add.s32 	%r68, %r1, %r67;
	shr.u32 	%r69, %r68, 31;
	shr.s32 	%r70, %r68, 5;
	add.s32 	%r71, %r70, %r69;
	and.b32  	%r72, %r71, 4194302;
	sub.s32 	%r73, %r70, %r72;
	shl.b32 	%r74, %r73, 9;
	add.s32 	%r75, %r66, %r74;
	and.b32  	%r76, %r47, 2;
	setp.eq.s32 	%p1, %r76, 0;
	shr.u32 	%r77, %r54, 3;
	xor.b32  	%r78, %r57, %r77;
	add.s32 	%r79, %r75, %r78;
	shr.u32 	%r80, %r35, 29;
	add.s32 	%r81, %r1, %r80;
	and.b32  	%r82, %r81, -8;
	sub.s32 	%r83, %r1, %r82;
	shr.u32 	%r84, %r83, 31;
	add.s32 	%r85, %r83, %r84;
	shr.s32 	%r86, %r85, 1;
	mov.u32 	%r510, 0;
	shl.b32 	%r87, %r86, 6;
	and.b32  	%r88, %r87, 192;
	and.b32  	%r89, %r81, 8;
	or.b32  	%r90, %r88, %r89;
	and.b32  	%r91, %r85, 67108862;
	sub.s32 	%r92, %r83, %r91;
	shl.b32 	%r93, %r92, 5;
	shl.b32 	%r94, %r52, 10;
	shr.u32 	%r95, %r35, 26;
	add.s32 	%r96, %r1, %r95;
	shl.b32 	%r97, %r96, 2;
	and.b32  	%r98, %r97, 2147483392;
	add.s32 	%r99, %r94, %r98;
	add.s32 	%r100, %r99, %r93;
	and.b32  	%r101, %r86, 2;
	setp.eq.s32 	%p2, %r101, 0;
	shr.u32 	%r102, %r88, 3;
	xor.b32  	%r103, %r90, %r102;
	add.s32 	%r104, %r100, %r103;
	shl.b32 	%r105, %r79, 1;
	add.s32 	%r5, %r31, %r105;
	add.s32 	%r6, %r5, 2048;
	shl.b32 	%r106, %r104, 1;
	add.s32 	%r107, %r31, %r106;
	add.s32 	%r7, %r107, 4096;
	selp.b32 	%r108, 32, -32, %p1;
	add.s32 	%r8, %r5, %r108;
	add.s32 	%r9, %r8, 2048;
	selp.b32 	%r109, 32, -32, %p2;
	add.s32 	%r10, %r7, %r109;
	or.b32  	%r110, %r34, %r3;
	shl.b32 	%r111, %r15, 10;
	add.s32 	%r112, %r110, %r111;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r113, %r11, 16;
	or.b32  	%r114, %r34, %r113;
	add.s32 	%r115, %r114, %r111;
	cvta.to.global.u64 	%rd12, %rd10;
	mul.wide.s32 	%rd13, %r115, 2;
	add.s64 	%rd19, %rd12, %rd13;
	mul.wide.s32 	%rd4, %r112, 2;
	mov.f32 	%f353, 0f00000000;
	mov.f32 	%f354, %f353;
	mov.f32 	%f355, %f353;
	mov.f32 	%f356, %f353;
	mov.f32 	%f357, %f353;
	mov.f32 	%f358, %f353;
	mov.f32 	%f359, %f353;
	mov.f32 	%f360, %f353;
	mov.f32 	%f361, %f353;
	mov.f32 	%f362, %f353;
	mov.f32 	%f363, %f353;
	mov.f32 	%f364, %f353;
	mov.f32 	%f365, %f353;
	mov.f32 	%f366, %f353;
	mov.f32 	%f367, %f353;
	mov.f32 	%f368, %f353;

$L__BB0_1:
	bar.sync 	0;
	add.s64 	%rd14, %rd20, %rd4;
	ld.global.nc.v4.u32 	{%r428, %r429, %r430, %r431}, [%rd14];
	st.shared.v4.u32 	[%r2], {%r428, %r429, %r430, %r431};
	ld.global.nc.v4.u32 	{%r436, %r437, %r438, %r439}, [%rd19];
	st.shared.v4.u32 	[%r2+4096], {%r436, %r437, %r438, %r439};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r116, %r117, %r118, %r119}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r121, %r122, %r123, %r124}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r126, %r127, %r128, %r129}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r116,  %r117,  %r118,  %r119},{%r126,  %r127},{%f368, %f367, %f366, %f365};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r121,  %r122,  %r123,  %r124},{%r126,  %r127},{%f364, %f363, %f362, %f361};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r121,  %r122,  %r123,  %r124},{%r128,  %r129},{%f356, %f355, %f354, %f353};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r116,  %r117,  %r118,  %r119},{%r128,  %r129},{%f360, %f359, %f358, %f357};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r155, %r156, %r157, %r158}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r160, %r161, %r162, %r163}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r165, %r166, %r167, %r168}, [%r10];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r155,  %r156,  %r157,  %r158},{%r165,  %r166},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r160,  %r161,  %r162,  %r163},{%r165,  %r166},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r160,  %r161,  %r162,  %r163},{%r167,  %r168},{%f65, %f66, %f67, %f68};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r155,  %r156,  %r157,  %r158},{%r167,  %r168},{%f73, %f74, %f75, %f76};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r444, %r445, %r446, %r447}, [%rd14+64];
	st.shared.v4.u32 	[%r2], {%r444, %r445, %r446, %r447};
	ld.global.nc.v4.u32 	{%r452, %r453, %r454, %r455}, [%rd19+64];
	st.shared.v4.u32 	[%r2+4096], {%r452, %r453, %r454, %r455};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r194, %r195, %r196, %r197}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r199, %r200, %r201, %r202}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r204, %r205, %r206, %r207}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r194,  %r195,  %r196,  %r197},{%r204,  %r205},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r199,  %r200,  %r201,  %r202},{%r204,  %r205},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r199,  %r200,  %r201,  %r202},{%r206,  %r207},{%f97, %f98, %f99, %f100};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r194,  %r195,  %r196,  %r197},{%r206,  %r207},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r233, %r234, %r235, %r236}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r238, %r239, %r240, %r241}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r243, %r244, %r245, %r246}, [%r10];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f145,  %f146,  %f147,  %f148},{%r233,  %r234,  %r235,  %r236},{%r243,  %r244},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f153,  %f154,  %f155,  %f156},{%r238,  %r239,  %r240,  %r241},{%r243,  %r244},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f161,  %f162,  %f163,  %f164},{%r238,  %r239,  %r240,  %r241},{%r245,  %r246},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f169,  %f170,  %f171,  %f172},{%r233,  %r234,  %r235,  %r236},{%r245,  %r246},{%f137, %f138, %f139, %f140};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r460, %r461, %r462, %r463}, [%rd14+128];
	st.shared.v4.u32 	[%r2], {%r460, %r461, %r462, %r463};
	ld.global.nc.v4.u32 	{%r468, %r469, %r470, %r471}, [%rd19+128];
	st.shared.v4.u32 	[%r2+4096], {%r468, %r469, %r470, %r471};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r272, %r273, %r274, %r275}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r277, %r278, %r279, %r280}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r282, %r283, %r284, %r285}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f177,  %f178,  %f179,  %f180},{%r272,  %r273,  %r274,  %r275},{%r282,  %r283},{%f145, %f146, %f147, %f148};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f185,  %f186,  %f187,  %f188},{%r277,  %r278,  %r279,  %r280},{%r282,  %r283},{%f153, %f154, %f155, %f156};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f193,  %f194,  %f195,  %f196},{%r277,  %r278,  %r279,  %r280},{%r284,  %r285},{%f161, %f162, %f163, %f164};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f201,  %f202,  %f203,  %f204},{%r272,  %r273,  %r274,  %r275},{%r284,  %r285},{%f169, %f170, %f171, %f172};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r311, %r312, %r313, %r314}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r316, %r317, %r318, %r319}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r321, %r322, %r323, %r324}, [%r10];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f209,  %f210,  %f211,  %f212},{%r311,  %r312,  %r313,  %r314},{%r321,  %r322},{%f177, %f178, %f179, %f180};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f217,  %f218,  %f219,  %f220},{%r316,  %r317,  %r318,  %r319},{%r321,  %r322},{%f185, %f186, %f187, %f188};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f225,  %f226,  %f227,  %f228},{%r316,  %r317,  %r318,  %r319},{%r323,  %r324},{%f193, %f194, %f195, %f196};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f233,  %f234,  %f235,  %f236},{%r311,  %r312,  %r313,  %r314},{%r323,  %r324},{%f201, %f202, %f203, %f204};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r476, %r477, %r478, %r479}, [%rd14+192];
	st.shared.v4.u32 	[%r2], {%r476, %r477, %r478, %r479};
	ld.global.nc.v4.u32 	{%r484, %r485, %r486, %r487}, [%rd19+192];
	st.shared.v4.u32 	[%r2+4096], {%r484, %r485, %r486, %r487};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r350, %r351, %r352, %r353}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r355, %r356, %r357, %r358}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r360, %r361, %r362, %r363}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f241,  %f242,  %f243,  %f244},{%r350,  %r351,  %r352,  %r353},{%r360,  %r361},{%f209, %f210, %f211, %f212};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f249,  %f250,  %f251,  %f252},{%r355,  %r356,  %r357,  %r358},{%r360,  %r361},{%f217, %f218, %f219, %f220};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f257,  %f258,  %f259,  %f260},{%r355,  %r356,  %r357,  %r358},{%r362,  %r363},{%f225, %f226, %f227, %f228};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f265,  %f266,  %f267,  %f268},{%r350,  %r351,  %r352,  %r353},{%r362,  %r363},{%f233, %f234, %f235, %f236};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r389, %r390, %r391, %r392}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r394, %r395, %r396, %r397}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r399, %r400, %r401, %r402}, [%r10];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f368,  %f367,  %f366,  %f365},{%r389,  %r390,  %r391,  %r392},{%r399,  %r400},{%f241, %f242, %f243, %f244};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f364,  %f363,  %f362,  %f361},{%r394,  %r395,  %r396,  %r397},{%r399,  %r400},{%f249, %f250, %f251, %f252};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f356,  %f355,  %f354,  %f353},{%r394,  %r395,  %r396,  %r397},{%r401,  %r402},{%f257, %f258, %f259, %f260};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f360,  %f359,  %f358,  %f357},{%r389,  %r390,  %r391,  %r392},{%r401,  %r402},{%f265, %f266, %f267, %f268};

	// end inline asm
	add.s64 	%rd20, %rd20, 256;
	add.s64 	%rd19, %rd19, 256;
	add.s32 	%r510, %r510, 4;
	setp.ne.s32 	%p3, %r510, 32;
	@%p3 bra 	$L__BB0_1;

	mov.u32 	%r509, %tid.x;
	ld.param.u64 	%rd18, [main_kernel_param_2];
	cvta.to.global.u64 	%rd17, %rd18;
	and.b32  	%r508, %r509, 3;
	mov.u32 	%r507, %ctaid.y;
	shl.b32 	%r506, %r507, 16;
	mov.u32 	%r505, %ctaid.x;
	shl.b32 	%r492, %r509, 9;
	and.b32  	%r493, %r492, 16384;
	shl.b32 	%r494, %r509, 8;
	and.b32  	%r495, %r494, 7168;
	shr.s32 	%r496, %r509, 6;
	shl.b32 	%r497, %r496, 3;
	shl.b32 	%r498, %r505, 6;
	add.s32 	%r499, %r498, %r506;
	add.s32 	%r500, %r499, %r493;
	add.s32 	%r501, %r500, %r497;
	shl.b32 	%r502, %r508, 1;
	add.s32 	%r503, %r501, %r502;
	add.s32 	%r504, %r503, %r495;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f368;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f306, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f367;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f309, %rs4;}

	// end inline asm
	mul.wide.s32 	%rd15, %r504, 2;
	add.s64 	%rd16, %rd17, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f309;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f306;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f366;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f312, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f365;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f315, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f315;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f312;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16384], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f364;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f318, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f363;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f321, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f321;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f318;}

	// end inline asm
	st.global.v2.u16 	[%rd16+65536], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f362;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f324, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f361;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f327, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f327;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f324;}

	// end inline asm
	st.global.v2.u16 	[%rd16+81920], {%rs21, %rs24};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f360;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f330, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f359;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f333, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f333;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f330;}

	// end inline asm
	st.global.v2.u16 	[%rd16+64], {%rs27, %rs30};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f358;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f336, %rs31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f357;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f339, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f339;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f336;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16448], {%rs33, %rs36};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f356;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f342, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f355;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f345, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f345;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f342;}

	// end inline asm
	st.global.v2.u16 	[%rd16+65600], {%rs39, %rs42};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f354;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f348, %rs43;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f353;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f351, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f351;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f348;}

	// end inline asm
	st.global.v2.u16 	[%rd16+81984], {%rs45, %rs48};
	ret;

}

