//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<185>;
	.reg .b32 	%r<384>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_1];
	cvta.to.global.u64 	%rd19, %rd8;
	mov.u32 	%r8, %tid.x;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r8, 3;
	and.b32  	%r11, %r10, 24;
	shl.b32 	%r12, %r9, 15;
	and.b32  	%r13, %r12, 1015808;
	shr.s32 	%r14, %r8, 31;
	shr.u32 	%r15, %r14, 28;
	add.s32 	%r16, %r8, %r15;
	and.b32  	%r17, %r16, -16;
	sub.s32 	%r18, %r8, %r17;
	shr.u32 	%r19, %r18, 31;
	add.s32 	%r20, %r18, %r19;
	shr.s32 	%r21, %r20, 1;
	shr.s32 	%r22, %r20, 31;
	shr.u32 	%r23, %r22, 30;
	add.s32 	%r24, %r21, %r23;
	and.b32  	%r25, %r24, -4;
	sub.s32 	%r26, %r21, %r25;
	shr.u32 	%r27, %r16, 31;
	shr.s32 	%r28, %r16, 4;
	add.s32 	%r29, %r28, %r27;
	and.b32  	%r30, %r29, -2;
	sub.s32 	%r31, %r28, %r30;
	shl.b32 	%r32, %r26, 6;
	and.b32  	%r33, %r32, 192;
	shl.b32 	%r34, %r31, 3;
	and.b32  	%r35, %r34, 8;
	or.b32  	%r36, %r33, %r35;
	and.b32  	%r37, %r20, 134217726;
	sub.s32 	%r38, %r18, %r37;
	shl.b32 	%r39, %r38, 5;
	shr.s32 	%r40, %r18, 31;
	shr.u32 	%r41, %r40, 29;
	add.s32 	%r42, %r18, %r41;
	shl.b32 	%r43, %r42, 5;
	and.b32  	%r44, %r43, 2147483392;
	add.s32 	%r45, %r39, %r44;
	shr.u32 	%r46, %r14, 27;
	add.s32 	%r47, %r8, %r46;
	shr.u32 	%r48, %r47, 31;
	shr.s32 	%r49, %r47, 5;
	add.s32 	%r50, %r49, %r48;
	and.b32  	%r51, %r50, 4194302;
	sub.s32 	%r52, %r49, %r51;
	shl.b32 	%r53, %r52, 9;
	add.s32 	%r54, %r45, %r53;
	and.b32  	%r55, %r26, 2;
	setp.eq.s32 	%p1, %r55, 0;
	shr.u32 	%r56, %r33, 3;
	xor.b32  	%r57, %r36, %r56;
	add.s32 	%r58, %r54, %r57;
	shr.u32 	%r59, %r14, 29;
	add.s32 	%r60, %r8, %r59;
	and.b32  	%r61, %r60, -8;
	sub.s32 	%r62, %r8, %r61;
	shr.u32 	%r63, %r62, 31;
	add.s32 	%r64, %r62, %r63;
	shr.s32 	%r65, %r64, 1;
	mov.u32 	%r383, 0;
	shl.b32 	%r66, %r65, 6;
	and.b32  	%r67, %r66, 192;
	and.b32  	%r68, %r60, 8;
	or.b32  	%r69, %r67, %r68;
	and.b32  	%r70, %r64, 67108862;
	sub.s32 	%r71, %r62, %r70;
	shl.b32 	%r72, %r71, 5;
	shl.b32 	%r73, %r31, 9;
	shr.u32 	%r74, %r14, 26;
	add.s32 	%r75, %r8, %r74;
	shl.b32 	%r76, %r75, 2;
	and.b32  	%r77, %r76, 2147483392;
	add.s32 	%r78, %r73, %r77;
	add.s32 	%r79, %r78, %r72;
	and.b32  	%r80, %r65, 2;
	setp.eq.s32 	%p2, %r80, 0;
	shr.u32 	%r81, %r67, 3;
	xor.b32  	%r82, %r69, %r81;
	add.s32 	%r83, %r79, %r82;
	shl.b32 	%r84, %r58, 1;
	mov.u32 	%r85, buf_dyn_shmem;
	add.s32 	%r1, %r85, %r84;
	shl.b32 	%r86, %r83, 1;
	add.s32 	%r87, %r85, %r86;
	add.s32 	%r2, %r87, 2048;
	selp.b32 	%r88, 32, -32, %p1;
	add.s32 	%r3, %r1, %r88;
	selp.b32 	%r89, 32, -32, %p2;
	add.s32 	%r4, %r2, %r89;
	shl.b32 	%r90, %r8, 8;
	and.b32  	%r91, %r90, -1024;
	or.b32  	%r92, %r13, %r11;
	add.s32 	%r93, %r92, %r91;
	cvta.to.global.u64 	%rd10, %rd9;
	mul.wide.s32 	%rd11, %r93, 2;
	add.s64 	%rd18, %rd10, %rd11;
	mov.f32 	%f177, 0f00000000;
	mov.f32 	%f178, %f177;
	mov.f32 	%f179, %f177;
	mov.f32 	%f180, %f177;
	mov.f32 	%f181, %f177;
	mov.f32 	%f182, %f177;
	mov.f32 	%f183, %f177;
	mov.f32 	%f184, %f177;

$L__BB0_1:
	shl.b32 	%r274, %r9, 10;
	and.b32  	%r275, %r274, -32768;
	or.b32  	%r276, %r11, %r275;
	shr.s32 	%r277, %r8, 2;
	shl.b32 	%r278, %r277, 10;
	add.s32 	%r279, %r276, %r278;
	mul.wide.s32 	%rd12, %r279, 2;
	add.s64 	%rd13, %rd19, %rd12;
	and.b32  	%r280, %r8, 2;
	shr.u32 	%r281, %r280, 1;
	shr.u32 	%r282, %r8, 4;
	add.s32 	%r283, %r281, %r282;
	shl.b32 	%r284, %r283, 4;
	and.b32  	%r285, %r284, 16;
	shl.b32 	%r286, %r277, 5;
	or.b32  	%r287, %r285, %r286;
	and.b32  	%r288, %r8, 8;
	shr.u32 	%r289, %r288, 3;
	add.s32 	%r290, %r289, %r8;
	shl.b32 	%r291, %r290, 3;
	and.b32  	%r292, %r291, 8;
	or.b32  	%r293, %r287, %r292;
	shl.b32 	%r294, %r293, 1;
	add.s32 	%r296, %r85, %r294;
	ld.global.nc.v4.u32 	{%r297, %r298, %r299, %r300}, [%rd13];
	st.shared.v4.u32 	[%r296], {%r297, %r298, %r299, %r300};
	ld.global.nc.v4.u32 	{%r305, %r306, %r307, %r308}, [%rd18];
	st.shared.v4.u32 	[%r296+2048], {%r305, %r306, %r307, %r308};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r94, %r95, %r96, %r97}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r99, %r100, %r101, %r102}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f25,  %f26,  %f27,  %f28},{%r94,  %r95,  %r96,  %r97},{%r99,  %r100},{%f184, %f183, %f182, %f181};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f33,  %f34,  %f35,  %f36},{%r94,  %r95,  %r96,  %r97},{%r101,  %r102},{%f180, %f179, %f178, %f177};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r116, %r117, %r118, %r119}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r121, %r122, %r123, %r124}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f41,  %f42,  %f43,  %f44},{%r116,  %r117,  %r118,  %r119},{%r121,  %r122},{%f25, %f26, %f27, %f28};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r116,  %r117,  %r118,  %r119},{%r123,  %r124},{%f33, %f34, %f35, %f36};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r313, %r314, %r315, %r316}, [%rd13+64];
	st.shared.v4.u32 	[%r296], {%r313, %r314, %r315, %r316};
	ld.global.nc.v4.u32 	{%r321, %r322, %r323, %r324}, [%rd18+64];
	st.shared.v4.u32 	[%r296+2048], {%r321, %r322, %r323, %r324};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r138, %r139, %r140, %r141}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r143, %r144, %r145, %r146}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r138,  %r139,  %r140,  %r141},{%r143,  %r144},{%f41, %f42, %f43, %f44};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r138,  %r139,  %r140,  %r141},{%r145,  %r146},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r160, %r161, %r162, %r163}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r165, %r166, %r167, %r168}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r160,  %r161,  %r162,  %r163},{%r165,  %r166},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r160,  %r161,  %r162,  %r163},{%r167,  %r168},{%f65, %f66, %f67, %f68};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r329, %r330, %r331, %r332}, [%rd13+128];
	st.shared.v4.u32 	[%r296], {%r329, %r330, %r331, %r332};
	ld.global.nc.v4.u32 	{%r337, %r338, %r339, %r340}, [%rd18+128];
	st.shared.v4.u32 	[%r296+2048], {%r337, %r338, %r339, %r340};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r182, %r183, %r184, %r185}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r187, %r188, %r189, %r190}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r182,  %r183,  %r184,  %r185},{%r187,  %r188},{%f73, %f74, %f75, %f76};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r182,  %r183,  %r184,  %r185},{%r189,  %r190},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r204, %r205, %r206, %r207}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r209, %r210, %r211, %r212}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r204,  %r205,  %r206,  %r207},{%r209,  %r210},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r204,  %r205,  %r206,  %r207},{%r211,  %r212},{%f97, %f98, %f99, %f100};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r345, %r346, %r347, %r348}, [%rd13+192];
	st.shared.v4.u32 	[%r296], {%r345, %r346, %r347, %r348};
	ld.global.nc.v4.u32 	{%r353, %r354, %r355, %r356}, [%rd18+192];
	st.shared.v4.u32 	[%r296+2048], {%r353, %r354, %r355, %r356};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r226, %r227, %r228, %r229}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r231, %r232, %r233, %r234}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r226,  %r227,  %r228,  %r229},{%r231,  %r232},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r226,  %r227,  %r228,  %r229},{%r233,  %r234},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r248, %r249, %r250, %r251}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r253, %r254, %r255, %r256}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f184,  %f183,  %f182,  %f181},{%r248,  %r249,  %r250,  %r251},{%r253,  %r254},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f180,  %f179,  %f178,  %f177},{%r248,  %r249,  %r250,  %r251},{%r255,  %r256},{%f129, %f130, %f131, %f132};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd19, %rd19, 256;
	add.s64 	%rd18, %rd18, 256;
	add.s32 	%r383, %r383, 4;
	setp.ne.s32 	%p3, %r383, 32;
	@%p3 bra 	$L__BB0_1;

	shl.b32 	%r382, %r9, 10;
	and.b32  	%r381, %r382, -32768;
	ld.param.u64 	%rd17, [main_kernel_param_2];
	shl.b32 	%r380, %r8, 8;
	shl.b32 	%r362, %r8, 9;
	and.b32  	%r363, %r362, 16384;
	or.b32  	%r367, %r381, %r363;
	and.b32  	%r369, %r380, 7168;
	shl.b32 	%r370, %r9, 5;
	and.b32  	%r371, %r370, 992;
	shr.s32 	%r372, %r8, 6;
	shl.b32 	%r373, %r372, 3;
	shl.b32 	%r374, %r8, 1;
	and.b32  	%r375, %r374, 6;
	or.b32  	%r376, %r367, %r371;
	add.s32 	%r377, %r376, %r373;
	or.b32  	%r378, %r377, %r375;
	add.s32 	%r379, %r378, %r369;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f184;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f154, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f183;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f157, %rs4;}

	// end inline asm
	cvta.to.global.u64 	%rd14, %rd17;
	mul.wide.s32 	%rd15, %r379, 2;
	add.s64 	%rd16, %rd14, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f154;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f182;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f160, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f163, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f160;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16384], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f180;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f166, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f179;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f169, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f166;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f178;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f172, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f175, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f172;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16416], {%rs21, %rs24};
	ret;

}

