//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN64_INTERNAL_70b8aba1_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<49>;
	.reg .f32 	%f<241>;
	.reg .b32 	%r<341>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_1];
	ld.param.u64 	%rd7, [main_kernel_param_2];
	cvta.to.global.u64 	%rd10, %rd9;
	mov.u32 	%r11, %tid.x;
	shr.s32 	%r12, %r11, 2;
	shl.b32 	%r13, %r12, 5;
	shr.u32 	%r14, %r11, 4;
	and.b32  	%r15, %r11, 2;
	shr.u32 	%r16, %r15, 1;
	add.s32 	%r17, %r16, %r14;
	shl.b32 	%r18, %r17, 4;
	and.b32  	%r19, %r18, 16;
	and.b32  	%r20, %r11, 8;
	shr.u32 	%r21, %r20, 3;
	add.s32 	%r22, %r21, %r11;
	shl.b32 	%r23, %r22, 3;
	and.b32  	%r24, %r23, 8;
	or.b32  	%r25, %r19, %r13;
	or.b32  	%r26, %r25, %r24;
	mov.u32 	%r27, %ctaid.x;
	shl.b32 	%r28, %r27, 11;
	and.b32  	%r29, %r28, -131072;
	shl.b32 	%r30, %r11, 3;
	and.b32  	%r31, %r30, 24;
	shl.b32 	%r32, %r27, 16;
	and.b32  	%r33, %r32, 4128768;
	shr.s32 	%r34, %r11, 31;
	shr.u32 	%r35, %r34, 28;
	add.s32 	%r36, %r11, %r35;
	and.b32  	%r37, %r36, -16;
	sub.s32 	%r38, %r11, %r37;
	shr.u32 	%r39, %r38, 31;
	add.s32 	%r40, %r38, %r39;
	shr.s32 	%r41, %r40, 1;
	shr.s32 	%r42, %r40, 31;
	shr.u32 	%r43, %r42, 30;
	add.s32 	%r44, %r41, %r43;
	and.b32  	%r45, %r44, -4;
	sub.s32 	%r46, %r41, %r45;
	shr.u32 	%r47, %r36, 31;
	shr.s32 	%r48, %r36, 4;
	add.s32 	%r49, %r48, %r47;
	and.b32  	%r50, %r49, -2;
	sub.s32 	%r51, %r48, %r50;
	shl.b32 	%r52, %r46, 6;
	and.b32  	%r53, %r52, 192;
	shl.b32 	%r54, %r51, 3;
	and.b32  	%r55, %r54, 8;
	or.b32  	%r56, %r53, %r55;
	and.b32  	%r57, %r40, 134217726;
	sub.s32 	%r58, %r38, %r57;
	shl.b32 	%r59, %r58, 5;
	shr.s32 	%r60, %r38, 31;
	shr.u32 	%r61, %r60, 29;
	add.s32 	%r62, %r38, %r61;
	shl.b32 	%r63, %r62, 5;
	and.b32  	%r64, %r63, 2147483392;
	add.s32 	%r65, %r59, %r64;
	shr.u32 	%r66, %r34, 27;
	add.s32 	%r67, %r11, %r66;
	shr.u32 	%r68, %r67, 31;
	shr.s32 	%r69, %r67, 5;
	add.s32 	%r70, %r69, %r68;
	and.b32  	%r71, %r70, 4194302;
	sub.s32 	%r72, %r69, %r71;
	shl.b32 	%r73, %r72, 9;
	add.s32 	%r74, %r65, %r73;
	and.b32  	%r75, %r46, 2;
	setp.eq.s32 	%p1, %r75, 0;
	shr.u32 	%r76, %r53, 3;
	xor.b32  	%r77, %r56, %r76;
	add.s32 	%r78, %r74, %r77;
	shr.u32 	%r79, %r34, 29;
	add.s32 	%r80, %r11, %r79;
	and.b32  	%r81, %r80, -8;
	sub.s32 	%r82, %r11, %r81;
	shr.u32 	%r83, %r82, 31;
	add.s32 	%r84, %r82, %r83;
	shr.s32 	%r85, %r84, 1;
	mov.u32 	%r340, 0;
	shl.b32 	%r86, %r85, 6;
	and.b32  	%r87, %r86, 192;
	and.b32  	%r88, %r80, 8;
	or.b32  	%r89, %r87, %r88;
	and.b32  	%r90, %r84, 67108862;
	sub.s32 	%r91, %r82, %r90;
	shl.b32 	%r92, %r91, 5;
	shl.b32 	%r93, %r51, 9;
	shr.u32 	%r94, %r34, 26;
	add.s32 	%r95, %r11, %r94;
	shl.b32 	%r96, %r95, 2;
	and.b32  	%r97, %r96, 2147483392;
	add.s32 	%r98, %r93, %r97;
	add.s32 	%r99, %r98, %r92;
	and.b32  	%r100, %r85, 2;
	setp.eq.s32 	%p2, %r100, 0;
	shr.u32 	%r101, %r87, 3;
	xor.b32  	%r102, %r89, %r101;
	add.s32 	%r103, %r99, %r102;
	shl.b32 	%r104, %r26, 1;
	mov.u32 	%r105, buf_dyn_shmem;
	add.s32 	%r1, %r105, %r104;
	shl.b32 	%r106, %r78, 1;
	add.s32 	%r2, %r105, %r106;
	add.s32 	%r3, %r2, 2048;
	shl.b32 	%r107, %r103, 1;
	add.s32 	%r108, %r105, %r107;
	add.s32 	%r4, %r108, 4096;
	selp.b32 	%r109, 32, -32, %p1;
	add.s32 	%r5, %r2, %r109;
	add.s32 	%r6, %r5, 2048;
	selp.b32 	%r110, 32, -32, %p2;
	add.s32 	%r7, %r4, %r110;
	or.b32  	%r111, %r31, %r29;
	shl.b32 	%r112, %r12, 11;
	add.s32 	%r113, %r111, %r112;
	or.b32  	%r114, %r33, %r31;
	add.s32 	%r115, %r114, %r112;
	mul.wide.s32 	%rd11, %r115, 2;
	add.s64 	%rd18, %rd10, %rd11;
	cvta.to.global.u64 	%rd12, %rd8;
	mul.wide.s32 	%rd13, %r113, 2;
	add.s64 	%rd17, %rd12, %rd13;
	mov.f32 	%f225, 0f00000000;
	mov.f32 	%f226, %f225;
	mov.f32 	%f227, %f225;
	mov.f32 	%f228, %f225;
	mov.f32 	%f229, %f225;
	mov.f32 	%f230, %f225;
	mov.f32 	%f231, %f225;
	mov.f32 	%f232, %f225;
	mov.f32 	%f233, %f225;
	mov.f32 	%f234, %f225;
	mov.f32 	%f235, %f225;
	mov.f32 	%f236, %f225;
	mov.f32 	%f237, %f225;
	mov.f32 	%f238, %f225;
	mov.f32 	%f239, %f225;
	mov.f32 	%f240, %f225;

$L__BB0_1:
	ld.global.nc.v4.u32 	{%r272, %r273, %r274, %r275}, [%rd17];
	st.shared.v4.u32 	[%r1], {%r272, %r273, %r274, %r275};
	ld.global.nc.v4.u32 	{%r280, %r281, %r282, %r283}, [%rd17+131072];
	st.shared.v4.u32 	[%r1+2048], {%r280, %r281, %r282, %r283};
	ld.global.nc.v4.u32 	{%r288, %r289, %r290, %r291}, [%rd18];
	st.shared.v4.u32 	[%r1+4096], {%r288, %r289, %r290, %r291};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r116, %r117, %r118, %r119}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r121, %r122, %r123, %r124}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r126, %r127, %r128, %r129}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r116,  %r117,  %r118,  %r119},{%r126,  %r127},{%f240, %f239, %f238, %f237};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r121,  %r122,  %r123,  %r124},{%r126,  %r127},{%f236, %f235, %f234, %f233};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r121,  %r122,  %r123,  %r124},{%r128,  %r129},{%f228, %f227, %f226, %f225};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r116,  %r117,  %r118,  %r119},{%r128,  %r129},{%f232, %f231, %f230, %f229};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r155, %r156, %r157, %r158}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r160, %r161, %r162, %r163}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r165, %r166, %r167, %r168}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r155,  %r156,  %r157,  %r158},{%r165,  %r166},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r160,  %r161,  %r162,  %r163},{%r165,  %r166},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r160,  %r161,  %r162,  %r163},{%r167,  %r168},{%f65, %f66, %f67, %f68};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r155,  %r156,  %r157,  %r158},{%r167,  %r168},{%f73, %f74, %f75, %f76};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r296, %r297, %r298, %r299}, [%rd17+64];
	st.shared.v4.u32 	[%r1], {%r296, %r297, %r298, %r299};
	ld.global.nc.v4.u32 	{%r304, %r305, %r306, %r307}, [%rd17+131136];
	st.shared.v4.u32 	[%r1+2048], {%r304, %r305, %r306, %r307};
	ld.global.nc.v4.u32 	{%r312, %r313, %r314, %r315}, [%rd18+64];
	st.shared.v4.u32 	[%r1+4096], {%r312, %r313, %r314, %r315};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r194, %r195, %r196, %r197}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r199, %r200, %r201, %r202}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r204, %r205, %r206, %r207}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r194,  %r195,  %r196,  %r197},{%r204,  %r205},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r199,  %r200,  %r201,  %r202},{%r204,  %r205},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r199,  %r200,  %r201,  %r202},{%r206,  %r207},{%f97, %f98, %f99, %f100};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r194,  %r195,  %r196,  %r197},{%r206,  %r207},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r233, %r234, %r235, %r236}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r238, %r239, %r240, %r241}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r243, %r244, %r245, %r246}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f240,  %f239,  %f238,  %f237},{%r233,  %r234,  %r235,  %r236},{%r243,  %r244},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f236,  %f235,  %f234,  %f233},{%r238,  %r239,  %r240,  %r241},{%r243,  %r244},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f228,  %f227,  %f226,  %f225},{%r238,  %r239,  %r240,  %r241},{%r245,  %r246},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f232,  %f231,  %f230,  %f229},{%r233,  %r234,  %r235,  %r236},{%r245,  %r246},{%f137, %f138, %f139, %f140};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd18, %rd18, 128;
	add.s64 	%rd17, %rd17, 128;
	add.s32 	%r340, %r340, 2;
	setp.ne.s32 	%p3, %r340, 64;
	@%p3 bra 	$L__BB0_1;

	mov.u32 	%r339, %tid.x;
	shl.b32 	%r321, %r339, 10;
	and.b32  	%r322, %r321, 32768;
	shl.b32 	%r323, %r339, 9;
	and.b32  	%r324, %r323, 14336;
	shl.b32 	%r326, %r27, 5;
	and.b32  	%r327, %r326, 2016;
	shr.s32 	%r328, %r339, 6;
	shl.b32 	%r329, %r328, 3;
	shl.b32 	%r330, %r339, 1;
	and.b32  	%r331, %r330, 6;
	or.b32  	%r334, %r322, %r29;
	or.b32  	%r335, %r334, %r327;
	add.s32 	%r336, %r335, %r329;
	or.b32  	%r337, %r336, %r331;
	add.s32 	%r338, %r337, %r324;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f240;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f178, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f239;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f181, %rs4;}

	// end inline asm
	cvta.to.global.u64 	%rd14, %rd7;
	mul.wide.s32 	%rd15, %r338, 2;
	add.s64 	%rd16, %rd14, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f178;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f238;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f184, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f237;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f187, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f187;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f184;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32768], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f236;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f190, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f235;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f193, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f193;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f190;}

	// end inline asm
	st.global.v2.u16 	[%rd16+131072], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f234;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f196, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f233;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f199, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f199;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f196;}

	// end inline asm
	st.global.v2.u16 	[%rd16+163840], {%rs21, %rs24};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f232;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f202, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f231;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f205, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f205;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f202;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32], {%rs27, %rs30};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f230;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f208, %rs31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f229;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f211, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f211;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f208;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32800], {%rs33, %rs36};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f228;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f214, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f227;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f217, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f217;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f214;}

	// end inline asm
	st.global.v2.u16 	[%rd16+131104], {%rs39, %rs42};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f226;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f220, %rs43;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f225;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f223, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f223;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f220;}

	// end inline asm
	st.global.v2.u16 	[%rd16+163872], {%rs45, %rs48};
	ret;

}

