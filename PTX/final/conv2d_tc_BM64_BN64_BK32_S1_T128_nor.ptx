//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<231>;
	.reg .b16 	%rs<193>;
	.reg .f32 	%f<385>;
	.reg .b32 	%r<1193>;
	.reg .b64 	%rd<52>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_2];
	cvta.to.global.u64 	%rd51, %rd9;
	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r147, %tid.x;
	shr.s32 	%r1183, %r147, 3;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r148, %r2, 1;
	and.b32  	%r149, %r148, 2;
	and.b32  	%r150, %r147, 4;
	shr.u32 	%r151, %r150, 2;
	or.b32  	%r3, %r149, %r151;
	shl.b32 	%r152, %r2, 6;
	and.b32  	%r153, %r152, 64;
	shl.b32 	%r154, %r147, 3;
	and.b32  	%r155, %r154, 56;
	mov.u32 	%r1184, 16;
	shr.s32 	%r157, %r147, 31;
	shr.u32 	%r158, %r157, 28;
	add.s32 	%r159, %r147, %r158;
	and.b32  	%r160, %r159, -16;
	sub.s32 	%r161, %r147, %r160;
	shr.u32 	%r162, %r161, 31;
	add.s32 	%r163, %r161, %r162;
	shr.s32 	%r164, %r163, 1;
	shr.s32 	%r165, %r163, 31;
	shr.u32 	%r166, %r165, 30;
	add.s32 	%r167, %r164, %r166;
	and.b32  	%r168, %r167, -4;
	sub.s32 	%r169, %r164, %r168;
	shr.u32 	%r170, %r159, 31;
	shr.s32 	%r171, %r159, 4;
	add.s32 	%r172, %r171, %r170;
	and.b32  	%r173, %r172, -2;
	sub.s32 	%r174, %r171, %r173;
	shl.b32 	%r175, %r169, 6;
	and.b32  	%r176, %r175, 192;
	shl.b32 	%r177, %r174, 3;
	and.b32  	%r178, %r177, 8;
	or.b32  	%r179, %r176, %r178;
	and.b32  	%r180, %r163, 134217726;
	sub.s32 	%r181, %r161, %r180;
	shl.b32 	%r182, %r181, 5;
	shr.s32 	%r183, %r161, 31;
	shr.u32 	%r184, %r183, 29;
	add.s32 	%r185, %r161, %r184;
	shr.s32 	%r186, %r185, 3;
	shl.b32 	%r187, %r186, 8;
	add.s32 	%r188, %r182, %r187;
	shr.u32 	%r189, %r157, 27;
	add.s32 	%r190, %r147, %r189;
	shr.u32 	%r191, %r190, 31;
	shr.s32 	%r192, %r190, 5;
	add.s32 	%r193, %r192, %r191;
	and.b32  	%r194, %r193, 4194302;
	sub.s32 	%r195, %r192, %r194;
	shl.b32 	%r196, %r195, 9;
	add.s32 	%r197, %r188, %r196;
	and.b32  	%r198, %r169, 2;
	setp.eq.s32 	%p33, %r198, 0;
	shr.u32 	%r199, %r176, 3;
	xor.b32  	%r200, %r179, %r199;
	add.s32 	%r201, %r197, %r200;
	and.b32  	%r202, %r185, -8;
	sub.s32 	%r203, %r161, %r202;
	shr.u32 	%r204, %r157, 26;
	add.s32 	%r205, %r147, %r204;
	shl.b32 	%r206, %r203, 6;
	and.b32  	%r207, %r206, 448;
	shl.b32 	%r208, %r174, 4;
	and.b32  	%r209, %r208, 16;
	shr.u32 	%r210, %r205, 3;
	and.b32  	%r211, %r210, 8;
	or.b32  	%r212, %r209, %r211;
	or.b32  	%r213, %r212, %r207;
	shl.b32 	%r214, %r186, 9;
	and.b32  	%r215, %r203, 4;
	setp.eq.s32 	%p34, %r215, 0;
	shr.u32 	%r216, %r207, 3;
	xor.b32  	%r217, %r213, %r216;
	or.b32  	%r218, %r217, %r214;
	shr.s32 	%r5, %r2, 1;
	shl.b32 	%r219, %r5, 6;
	shr.s32 	%r220, %r147, 5;
	add.s32 	%r221, %r219, %r220;
	and.b32  	%r1182, %r147, 31;
	mul.hi.s32 	%r222, %r221, -1387167949;
	add.s32 	%r223, %r222, %r221;
	shr.u32 	%r224, %r223, 31;
	shr.s32 	%r225, %r223, 11;
	add.s32 	%r226, %r225, %r224;
	mul.lo.s32 	%r227, %r226, 3025;
	sub.s32 	%r228, %r221, %r227;
	mul.hi.s32 	%r229, %r228, 156180629;
	shr.u32 	%r230, %r229, 31;
	shr.s32 	%r231, %r229, 1;
	add.s32 	%r232, %r231, %r230;
	shl.b32 	%r7, %r232, 1;
	mul.hi.s32 	%r233, %r221, 156180629;
	shr.u32 	%r234, %r233, 31;
	shr.s32 	%r235, %r233, 1;
	add.s32 	%r236, %r235, %r234;
	mul.lo.s32 	%r237, %r236, 55;
	sub.s32 	%r238, %r221, %r237;
	shl.b32 	%r8, %r238, 1;
	mul.lo.s32 	%r239, %r238, 12;
	add.s32 	%r240, %r221, 4;
	mul.hi.s32 	%r241, %r240, -1387167949;
	add.s32 	%r242, %r241, %r240;
	shr.u32 	%r243, %r242, 31;
	shr.s32 	%r244, %r242, 11;
	add.s32 	%r245, %r244, %r243;
	mul.lo.s32 	%r246, %r245, 3025;
	sub.s32 	%r247, %r240, %r246;
	mul.hi.s32 	%r248, %r247, 156180629;
	shr.u32 	%r249, %r248, 31;
	shr.s32 	%r250, %r248, 1;
	add.s32 	%r251, %r250, %r249;
	shl.b32 	%r9, %r251, 1;
	mul.hi.s32 	%r252, %r240, 156180629;
	shr.u32 	%r253, %r252, 31;
	shr.s32 	%r254, %r252, 1;
	add.s32 	%r255, %r254, %r253;
	mul.lo.s32 	%r256, %r255, 55;
	sub.s32 	%r257, %r240, %r256;
	shl.b32 	%r10, %r257, 1;
	mul.lo.s32 	%r258, %r257, 12;
	add.s32 	%r259, %r221, 8;
	mul.hi.s32 	%r260, %r259, -1387167949;
	add.s32 	%r261, %r260, %r259;
	shr.u32 	%r262, %r261, 31;
	shr.s32 	%r263, %r261, 11;
	add.s32 	%r264, %r263, %r262;
	mul.lo.s32 	%r265, %r264, 3025;
	sub.s32 	%r266, %r259, %r265;
	mul.hi.s32 	%r267, %r266, 156180629;
	shr.u32 	%r268, %r267, 31;
	shr.s32 	%r269, %r267, 1;
	add.s32 	%r270, %r269, %r268;
	shl.b32 	%r11, %r270, 1;
	mul.hi.s32 	%r271, %r259, 156180629;
	shr.u32 	%r272, %r271, 31;
	shr.s32 	%r273, %r271, 1;
	add.s32 	%r274, %r273, %r272;
	mul.lo.s32 	%r275, %r274, 55;
	sub.s32 	%r276, %r259, %r275;
	shl.b32 	%r12, %r276, 1;
	mul.lo.s32 	%r277, %r276, 12;
	add.s32 	%r278, %r221, 12;
	mul.hi.s32 	%r279, %r278, -1387167949;
	add.s32 	%r280, %r279, %r278;
	shr.u32 	%r281, %r280, 31;
	shr.s32 	%r282, %r280, 11;
	add.s32 	%r283, %r282, %r281;
	mul.lo.s32 	%r284, %r283, 3025;
	sub.s32 	%r285, %r278, %r284;
	mul.hi.s32 	%r286, %r285, 156180629;
	shr.u32 	%r287, %r286, 31;
	shr.s32 	%r288, %r286, 1;
	add.s32 	%r289, %r288, %r287;
	shl.b32 	%r13, %r289, 1;
	mul.hi.s32 	%r290, %r278, 156180629;
	shr.u32 	%r291, %r290, 31;
	shr.s32 	%r292, %r290, 1;
	add.s32 	%r293, %r292, %r291;
	mul.lo.s32 	%r294, %r293, 55;
	sub.s32 	%r295, %r278, %r294;
	shl.b32 	%r14, %r295, 1;
	mul.lo.s32 	%r296, %r295, 12;
	add.s32 	%r297, %r221, 16;
	mul.hi.s32 	%r298, %r297, -1387167949;
	add.s32 	%r299, %r298, %r297;
	shr.u32 	%r300, %r299, 31;
	shr.s32 	%r301, %r299, 11;
	add.s32 	%r302, %r301, %r300;
	mul.lo.s32 	%r303, %r302, 3025;
	sub.s32 	%r304, %r297, %r303;
	mul.hi.s32 	%r305, %r304, 156180629;
	shr.u32 	%r306, %r305, 31;
	shr.s32 	%r307, %r305, 1;
	add.s32 	%r308, %r307, %r306;
	shl.b32 	%r15, %r308, 1;
	mul.hi.s32 	%r309, %r297, 156180629;
	shr.u32 	%r310, %r309, 31;
	shr.s32 	%r311, %r309, 1;
	add.s32 	%r312, %r311, %r310;
	mul.lo.s32 	%r313, %r312, 55;
	sub.s32 	%r314, %r297, %r313;
	shl.b32 	%r16, %r314, 1;
	mul.lo.s32 	%r315, %r314, 12;
	add.s32 	%r316, %r221, 20;
	mul.hi.s32 	%r317, %r316, -1387167949;
	add.s32 	%r318, %r317, %r316;
	shr.u32 	%r319, %r318, 31;
	shr.s32 	%r320, %r318, 11;
	add.s32 	%r321, %r320, %r319;
	mul.lo.s32 	%r322, %r321, 3025;
	sub.s32 	%r323, %r316, %r322;
	mul.hi.s32 	%r324, %r323, 156180629;
	shr.u32 	%r325, %r324, 31;
	shr.s32 	%r326, %r324, 1;
	add.s32 	%r327, %r326, %r325;
	shl.b32 	%r17, %r327, 1;
	mul.hi.s32 	%r328, %r316, 156180629;
	shr.u32 	%r329, %r328, 31;
	shr.s32 	%r330, %r328, 1;
	add.s32 	%r331, %r330, %r329;
	mul.lo.s32 	%r332, %r331, 55;
	sub.s32 	%r333, %r316, %r332;
	shl.b32 	%r18, %r333, 1;
	mul.lo.s32 	%r334, %r333, 12;
	add.s32 	%r335, %r221, 24;
	mul.hi.s32 	%r336, %r335, -1387167949;
	add.s32 	%r337, %r336, %r335;
	shr.u32 	%r338, %r337, 31;
	shr.s32 	%r339, %r337, 11;
	add.s32 	%r340, %r339, %r338;
	mul.lo.s32 	%r341, %r340, 3025;
	sub.s32 	%r342, %r335, %r341;
	mul.hi.s32 	%r343, %r342, 156180629;
	shr.u32 	%r344, %r343, 31;
	shr.s32 	%r345, %r343, 1;
	add.s32 	%r346, %r345, %r344;
	shl.b32 	%r19, %r346, 1;
	mul.hi.s32 	%r347, %r335, 156180629;
	shr.u32 	%r348, %r347, 31;
	shr.s32 	%r349, %r347, 1;
	add.s32 	%r350, %r349, %r348;
	mul.lo.s32 	%r351, %r350, 55;
	sub.s32 	%r352, %r335, %r351;
	shl.b32 	%r20, %r352, 1;
	mul.lo.s32 	%r353, %r352, 12;
	add.s32 	%r354, %r221, 28;
	mul.hi.s32 	%r355, %r354, -1387167949;
	add.s32 	%r356, %r355, %r354;
	shr.u32 	%r357, %r356, 31;
	shr.s32 	%r358, %r356, 11;
	add.s32 	%r359, %r358, %r357;
	mul.lo.s32 	%r360, %r359, 3025;
	sub.s32 	%r361, %r354, %r360;
	mul.hi.s32 	%r362, %r361, 156180629;
	shr.u32 	%r363, %r362, 31;
	shr.s32 	%r364, %r362, 1;
	add.s32 	%r365, %r364, %r363;
	shl.b32 	%r21, %r365, 1;
	mul.hi.s32 	%r366, %r354, 156180629;
	shr.u32 	%r367, %r366, 31;
	shr.s32 	%r368, %r366, 1;
	add.s32 	%r369, %r368, %r367;
	mul.lo.s32 	%r370, %r369, 55;
	sub.s32 	%r371, %r354, %r370;
	shl.b32 	%r22, %r371, 1;
	mul.lo.s32 	%r372, %r371, 12;
	add.s32 	%r373, %r221, 32;
	mul.hi.s32 	%r374, %r373, -1387167949;
	add.s32 	%r375, %r374, %r373;
	shr.u32 	%r376, %r375, 31;
	shr.s32 	%r377, %r375, 11;
	add.s32 	%r378, %r377, %r376;
	mul.lo.s32 	%r379, %r378, 3025;
	sub.s32 	%r380, %r373, %r379;
	mul.hi.s32 	%r381, %r380, 156180629;
	shr.u32 	%r382, %r381, 31;
	shr.s32 	%r383, %r381, 1;
	add.s32 	%r384, %r383, %r382;
	shl.b32 	%r23, %r384, 1;
	mul.hi.s32 	%r385, %r373, 156180629;
	shr.u32 	%r386, %r385, 31;
	shr.s32 	%r387, %r385, 1;
	add.s32 	%r388, %r387, %r386;
	mul.lo.s32 	%r389, %r388, 55;
	sub.s32 	%r390, %r373, %r389;
	shl.b32 	%r24, %r390, 1;
	mul.lo.s32 	%r391, %r390, 12;
	add.s32 	%r392, %r221, 36;
	mul.hi.s32 	%r393, %r392, -1387167949;
	add.s32 	%r394, %r393, %r392;
	shr.u32 	%r395, %r394, 31;
	shr.s32 	%r396, %r394, 11;
	add.s32 	%r397, %r396, %r395;
	mul.lo.s32 	%r398, %r397, 3025;
	sub.s32 	%r399, %r392, %r398;
	mul.hi.s32 	%r400, %r399, 156180629;
	shr.u32 	%r401, %r400, 31;
	shr.s32 	%r402, %r400, 1;
	add.s32 	%r403, %r402, %r401;
	shl.b32 	%r25, %r403, 1;
	mul.hi.s32 	%r404, %r392, 156180629;
	shr.u32 	%r405, %r404, 31;
	shr.s32 	%r406, %r404, 1;
	add.s32 	%r407, %r406, %r405;
	mul.lo.s32 	%r408, %r407, 55;
	sub.s32 	%r409, %r392, %r408;
	shl.b32 	%r26, %r409, 1;
	mul.lo.s32 	%r410, %r409, 12;
	add.s32 	%r411, %r221, 40;
	mul.hi.s32 	%r412, %r411, -1387167949;
	add.s32 	%r413, %r412, %r411;
	shr.u32 	%r414, %r413, 31;
	shr.s32 	%r415, %r413, 11;
	add.s32 	%r416, %r415, %r414;
	mul.lo.s32 	%r417, %r416, 3025;
	sub.s32 	%r418, %r411, %r417;
	mul.hi.s32 	%r419, %r418, 156180629;
	shr.u32 	%r420, %r419, 31;
	shr.s32 	%r421, %r419, 1;
	add.s32 	%r422, %r421, %r420;
	shl.b32 	%r27, %r422, 1;
	mul.hi.s32 	%r423, %r411, 156180629;
	shr.u32 	%r424, %r423, 31;
	shr.s32 	%r425, %r423, 1;
	add.s32 	%r426, %r425, %r424;
	mul.lo.s32 	%r427, %r426, 55;
	sub.s32 	%r428, %r411, %r427;
	shl.b32 	%r28, %r428, 1;
	mul.lo.s32 	%r429, %r428, 12;
	add.s32 	%r430, %r221, 44;
	mul.hi.s32 	%r431, %r430, -1387167949;
	add.s32 	%r432, %r431, %r430;
	shr.u32 	%r433, %r432, 31;
	shr.s32 	%r434, %r432, 11;
	add.s32 	%r435, %r434, %r433;
	mul.lo.s32 	%r436, %r435, 3025;
	sub.s32 	%r437, %r430, %r436;
	mul.hi.s32 	%r438, %r437, 156180629;
	shr.u32 	%r439, %r438, 31;
	shr.s32 	%r440, %r438, 1;
	add.s32 	%r441, %r440, %r439;
	shl.b32 	%r29, %r441, 1;
	mul.hi.s32 	%r442, %r430, 156180629;
	shr.u32 	%r443, %r442, 31;
	shr.s32 	%r444, %r442, 1;
	add.s32 	%r445, %r444, %r443;
	mul.lo.s32 	%r446, %r445, 55;
	sub.s32 	%r447, %r430, %r446;
	shl.b32 	%r30, %r447, 1;
	mul.lo.s32 	%r448, %r447, 12;
	add.s32 	%r449, %r221, 48;
	mul.hi.s32 	%r450, %r449, -1387167949;
	add.s32 	%r451, %r450, %r449;
	shr.u32 	%r452, %r451, 31;
	shr.s32 	%r453, %r451, 11;
	add.s32 	%r454, %r453, %r452;
	mul.lo.s32 	%r455, %r454, 3025;
	sub.s32 	%r456, %r449, %r455;
	mul.hi.s32 	%r457, %r456, 156180629;
	shr.u32 	%r458, %r457, 31;
	shr.s32 	%r459, %r457, 1;
	add.s32 	%r460, %r459, %r458;
	shl.b32 	%r31, %r460, 1;
	mul.hi.s32 	%r461, %r449, 156180629;
	shr.u32 	%r462, %r461, 31;
	shr.s32 	%r463, %r461, 1;
	add.s32 	%r464, %r463, %r462;
	mul.lo.s32 	%r465, %r464, 55;
	sub.s32 	%r466, %r449, %r465;
	shl.b32 	%r32, %r466, 1;
	mul.lo.s32 	%r467, %r466, 12;
	add.s32 	%r468, %r221, 52;
	mul.hi.s32 	%r469, %r468, -1387167949;
	add.s32 	%r470, %r469, %r468;
	shr.u32 	%r471, %r470, 31;
	shr.s32 	%r472, %r470, 11;
	add.s32 	%r473, %r472, %r471;
	mul.lo.s32 	%r474, %r473, 3025;
	sub.s32 	%r475, %r468, %r474;
	mul.hi.s32 	%r476, %r475, 156180629;
	shr.u32 	%r477, %r476, 31;
	shr.s32 	%r478, %r476, 1;
	add.s32 	%r479, %r478, %r477;
	shl.b32 	%r33, %r479, 1;
	mul.hi.s32 	%r480, %r468, 156180629;
	shr.u32 	%r481, %r480, 31;
	shr.s32 	%r482, %r480, 1;
	add.s32 	%r483, %r482, %r481;
	mul.lo.s32 	%r484, %r483, 55;
	sub.s32 	%r485, %r468, %r484;
	shl.b32 	%r34, %r485, 1;
	mul.lo.s32 	%r486, %r485, 12;
	add.s32 	%r487, %r221, 56;
	mul.hi.s32 	%r488, %r487, -1387167949;
	add.s32 	%r489, %r488, %r487;
	shr.u32 	%r490, %r489, 31;
	shr.s32 	%r491, %r489, 11;
	add.s32 	%r492, %r491, %r490;
	mul.lo.s32 	%r493, %r492, 3025;
	sub.s32 	%r494, %r487, %r493;
	mul.hi.s32 	%r495, %r494, 156180629;
	shr.u32 	%r496, %r495, 31;
	shr.s32 	%r497, %r495, 1;
	add.s32 	%r498, %r497, %r496;
	shl.b32 	%r35, %r498, 1;
	mul.hi.s32 	%r499, %r487, 156180629;
	shr.u32 	%r500, %r499, 31;
	shr.s32 	%r501, %r499, 1;
	add.s32 	%r502, %r501, %r500;
	mul.lo.s32 	%r503, %r502, 55;
	sub.s32 	%r504, %r487, %r503;
	shl.b32 	%r36, %r504, 1;
	mul.lo.s32 	%r505, %r504, 12;
	add.s32 	%r506, %r221, 60;
	mul.hi.s32 	%r507, %r506, -1387167949;
	add.s32 	%r508, %r507, %r506;
	shr.u32 	%r509, %r508, 31;
	shr.s32 	%r510, %r508, 11;
	add.s32 	%r511, %r510, %r509;
	mul.lo.s32 	%r512, %r511, 3025;
	sub.s32 	%r513, %r506, %r512;
	mul.hi.s32 	%r514, %r513, 156180629;
	shr.u32 	%r515, %r514, 31;
	shr.s32 	%r516, %r514, 1;
	add.s32 	%r517, %r516, %r515;
	shl.b32 	%r37, %r517, 1;
	mul.hi.s32 	%r518, %r506, 156180629;
	shr.u32 	%r519, %r518, 31;
	shr.s32 	%r520, %r518, 1;
	add.s32 	%r521, %r520, %r519;
	mul.lo.s32 	%r522, %r521, 55;
	sub.s32 	%r523, %r506, %r522;
	shl.b32 	%r38, %r523, 1;
	mul.lo.s32 	%r524, %r523, 12;
	shl.b32 	%r525, %r201, 1;
	mov.u32 	%r526, buf_dyn_shmem;
	add.s32 	%r527, %r526, %r525;
	add.s32 	%r39, %r527, 4096;
	shl.b32 	%r528, %r218, 1;
	add.s32 	%r40, %r526, %r528;
	selp.b32 	%r529, 64, -64, %p34;
	add.s32 	%r41, %r40, %r529;
	selp.b32 	%r530, 32, -32, %p33;
	add.s32 	%r42, %r39, %r530;
	add.s32 	%r43, %r42, 2048;
	add.s32 	%r44, %r41, 2048;
	mad.lo.s32 	%r531, %r517, 2688, %r524;
	mad.lo.s32 	%r532, %r511, 150528, %r531;
	add.s32 	%r533, %r532, %r1182;
	add.s32 	%r1181, %r533, -1350;
	mad.lo.s32 	%r534, %r498, 2688, %r505;
	mad.lo.s32 	%r535, %r492, 150528, %r534;
	add.s32 	%r536, %r535, %r1182;
	add.s32 	%r1180, %r536, -1350;
	mad.lo.s32 	%r537, %r479, 2688, %r486;
	mad.lo.s32 	%r538, %r473, 150528, %r537;
	add.s32 	%r539, %r538, %r1182;
	add.s32 	%r1179, %r539, -1350;
	mad.lo.s32 	%r540, %r460, 2688, %r467;
	mad.lo.s32 	%r541, %r454, 150528, %r540;
	add.s32 	%r542, %r541, %r1182;
	add.s32 	%r1178, %r542, -1350;
	mad.lo.s32 	%r543, %r441, 2688, %r448;
	mad.lo.s32 	%r544, %r435, 150528, %r543;
	add.s32 	%r545, %r544, %r1182;
	add.s32 	%r1177, %r545, -1350;
	mad.lo.s32 	%r546, %r422, 2688, %r429;
	mad.lo.s32 	%r547, %r416, 150528, %r546;
	add.s32 	%r548, %r547, %r1182;
	add.s32 	%r1176, %r548, -1350;
	mad.lo.s32 	%r549, %r403, 2688, %r410;
	mad.lo.s32 	%r550, %r397, 150528, %r549;
	add.s32 	%r551, %r550, %r1182;
	add.s32 	%r1175, %r551, -1350;
	mad.lo.s32 	%r552, %r384, 2688, %r391;
	mad.lo.s32 	%r553, %r378, 150528, %r552;
	add.s32 	%r554, %r553, %r1182;
	add.s32 	%r1174, %r554, -1350;
	mad.lo.s32 	%r555, %r365, 2688, %r372;
	mad.lo.s32 	%r556, %r359, 150528, %r555;
	add.s32 	%r557, %r556, %r1182;
	add.s32 	%r1173, %r557, -1350;
	mad.lo.s32 	%r558, %r346, 2688, %r353;
	mad.lo.s32 	%r559, %r340, 150528, %r558;
	add.s32 	%r560, %r559, %r1182;
	add.s32 	%r1172, %r560, -1350;
	mad.lo.s32 	%r561, %r327, 2688, %r334;
	mad.lo.s32 	%r562, %r321, 150528, %r561;
	add.s32 	%r563, %r562, %r1182;
	add.s32 	%r1171, %r563, -1350;
	mad.lo.s32 	%r564, %r308, 2688, %r315;
	mad.lo.s32 	%r565, %r302, 150528, %r564;
	add.s32 	%r566, %r565, %r1182;
	add.s32 	%r1170, %r566, -1350;
	mad.lo.s32 	%r567, %r289, 2688, %r296;
	mad.lo.s32 	%r568, %r283, 150528, %r567;
	add.s32 	%r569, %r568, %r1182;
	add.s32 	%r1169, %r569, -1350;
	mad.lo.s32 	%r570, %r270, 2688, %r277;
	mad.lo.s32 	%r571, %r264, 150528, %r570;
	add.s32 	%r572, %r571, %r1182;
	add.s32 	%r1168, %r572, -1350;
	mad.lo.s32 	%r573, %r251, 2688, %r258;
	mad.lo.s32 	%r574, %r245, 150528, %r573;
	add.s32 	%r575, %r574, %r1182;
	add.s32 	%r1167, %r575, -1350;
	mad.lo.s32 	%r576, %r232, 2688, %r239;
	mad.lo.s32 	%r577, %r226, 150528, %r576;
	add.s32 	%r578, %r577, %r1182;
	add.s32 	%r1166, %r578, -1350;
	mov.f32 	%f353, 0f00000000;
	add.s32 	%r587, %r147, 2048;
	and.b32  	%r588, %r587, 2147483616;
	and.b32  	%r589, %r147, 7;
	or.b32  	%r590, %r588, %r589;
	and.b32  	%r591, %r147, 8;
	shr.u32 	%r592, %r591, 3;
	shr.u32 	%r593, %r147, 6;
	add.s32 	%r594, %r592, %r593;
	shl.b32 	%r595, %r594, 3;
	and.b32  	%r596, %r595, 8;
	or.b32  	%r597, %r590, %r596;
	and.b32  	%r598, %r147, 16;
	or.b32  	%r599, %r597, %r598;
	shl.b32 	%r600, %r599, 1;
	add.s32 	%r602, %r526, %r600;
	mov.f32 	%f354, %f353;
	mov.f32 	%f355, %f353;
	mov.f32 	%f356, %f353;
	mov.f32 	%f357, %f353;
	mov.f32 	%f358, %f353;
	mov.f32 	%f359, %f353;
	mov.f32 	%f360, %f353;
	mov.f32 	%f361, %f353;
	mov.f32 	%f362, %f353;
	mov.f32 	%f363, %f353;
	mov.f32 	%f364, %f353;
	mov.f32 	%f365, %f353;
	mov.f32 	%f366, %f353;
	mov.f32 	%f367, %f353;
	mov.f32 	%f368, %f353;
	mov.f32 	%f369, %f353;
	mov.f32 	%f370, %f353;
	mov.f32 	%f371, %f353;
	mov.f32 	%f372, %f353;
	mov.f32 	%f373, %f353;
	mov.f32 	%f374, %f353;
	mov.f32 	%f375, %f353;
	mov.f32 	%f376, %f353;
	mov.f32 	%f377, %f353;
	mov.f32 	%f378, %f353;
	mov.f32 	%f379, %f353;
	mov.f32 	%f380, %f353;
	mov.f32 	%f381, %f353;
	mov.f32 	%f382, %f353;
	mov.f32 	%f383, %f353;
	mov.f32 	%f384, %f353;

$L__BB0_1:
	mul.wide.u32 	%rd10, %r1182, 1041204193;
	shr.u64 	%rd11, %rd10, 35;
	cvt.u32.u64 	%r579, %rd11;
	mul.lo.s32 	%r80, %r579, 639;
	mad.lo.s32 	%r580, %r579, -33, %r1182;
	mul.wide.u32 	%rd12, %r580, -1431655765;
	shr.u64 	%rd13, %rd12, 34;
	cvt.u32.u64 	%r581, %rd13;
	add.s32 	%r81, %r581, -1;
	shr.u64 	%rd14, %rd10, 36;
	cvt.u32.u64 	%r82, %rd14;
	add.s32 	%r582, %r7, %r82;
	add.s32 	%r83, %r582, -1;
	setp.lt.u32 	%p35, %r83, 112;
	@%p35 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_2;

$L__BB0_3:
	add.s32 	%r583, %r81, %r8;
	setp.lt.u32 	%p37, %r583, 112;
	setp.lt.u32 	%p38, %r1182, 363;
	and.pred  	%p215, %p38, %p37;
	bra.uni 	$L__BB0_4;

$L__BB0_2:
	mov.pred 	%p215, 0;

$L__BB0_4:
	add.s32 	%r584, %r81, %r8;
	setp.lt.u32 	%p39, %r584, 112;
	and.pred  	%p41, %p39, %p35;
	setp.lt.s32 	%p42, %r2, 9454;
	and.pred  	%p43, %p41, %p42;
	and.pred  	%p44, %p215, %p43;
	@%p44 bra 	$L__BB0_6;
	bra.uni 	$L__BB0_5;

$L__BB0_6:
	add.s32 	%r585, %r1166, %r80;
	mul.wide.s32 	%rd15, %r585, 2;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.nc.u16 	%rs177, [%rd16];
	bra.uni 	$L__BB0_7;

$L__BB0_5:
	mov.f32 	%f97, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f97;}

	// end inline asm

$L__BB0_7:
	st.shared.u16 	[%r602], %rs177;
	add.s32 	%r603, %r9, %r82;
	add.s32 	%r84, %r603, -1;
	setp.lt.u32 	%p45, %r84, 112;
	@%p45 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_8;

$L__BB0_9:
	add.s32 	%r604, %r81, %r10;
	setp.lt.u32 	%p47, %r604, 112;
	setp.lt.u32 	%p48, %r1182, 363;
	and.pred  	%p216, %p48, %p47;
	bra.uni 	$L__BB0_10;

$L__BB0_8:
	mov.pred 	%p216, 0;

$L__BB0_10:
	add.s32 	%r605, %r81, %r10;
	setp.lt.u32 	%p49, %r605, 112;
	and.pred  	%p51, %p49, %p45;
	and.pred  	%p53, %p51, %p42;
	and.pred  	%p54, %p216, %p53;
	@%p54 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;

$L__BB0_12:
	add.s32 	%r606, %r1167, %r80;
	mul.wide.s32 	%rd17, %r606, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.u16 	%rs178, [%rd18];
	bra.uni 	$L__BB0_13;

$L__BB0_11:
	mov.f32 	%f98, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f98;}

	// end inline asm

$L__BB0_13:
	and.b32  	%r1156, %r147, 7;
	not.b32 	%r608, %r147;
	and.b32  	%r618, %r608, 16;
	or.b32  	%r619, %r618, %r596;
	or.b32  	%r620, %r619, %r1156;
	or.b32  	%r621, %r620, %r588;
	shl.b32 	%r622, %r621, 1;
	add.s32 	%r624, %r526, %r622;
	st.shared.u16 	[%r624+256], %rs178;
	add.s32 	%r625, %r11, %r82;
	add.s32 	%r85, %r625, -1;
	setp.lt.u32 	%p55, %r85, 112;
	@%p55 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_14;

$L__BB0_15:
	add.s32 	%r626, %r81, %r12;
	setp.lt.u32 	%p57, %r626, 112;
	setp.lt.u32 	%p58, %r1182, 363;
	and.pred  	%p217, %p58, %p57;
	bra.uni 	$L__BB0_16;

$L__BB0_14:
	mov.pred 	%p217, 0;

$L__BB0_16:
	add.s32 	%r627, %r81, %r12;
	setp.lt.u32 	%p59, %r627, 112;
	and.pred  	%p61, %p59, %p55;
	and.pred  	%p63, %p61, %p42;
	and.pred  	%p64, %p217, %p63;
	@%p64 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_17;

$L__BB0_18:
	add.s32 	%r628, %r1168, %r80;
	mul.wide.s32 	%rd19, %r628, 2;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.u16 	%rs179, [%rd20];
	bra.uni 	$L__BB0_19;

$L__BB0_17:
	mov.f32 	%f99, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f99;}

	// end inline asm

$L__BB0_19:
	st.shared.u16 	[%r602+512], %rs179;
	add.s32 	%r646, %r13, %r82;
	add.s32 	%r86, %r646, -1;
	setp.lt.u32 	%p65, %r86, 112;
	@%p65 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;

$L__BB0_21:
	add.s32 	%r647, %r81, %r14;
	setp.lt.u32 	%p67, %r647, 112;
	setp.lt.u32 	%p68, %r1182, 363;
	and.pred  	%p218, %p68, %p67;
	bra.uni 	$L__BB0_22;

$L__BB0_20:
	mov.pred 	%p218, 0;

$L__BB0_22:
	add.s32 	%r648, %r81, %r14;
	setp.lt.u32 	%p69, %r648, 112;
	and.pred  	%p71, %p69, %p65;
	and.pred  	%p73, %p71, %p42;
	and.pred  	%p74, %p218, %p73;
	@%p74 bra 	$L__BB0_24;
	bra.uni 	$L__BB0_23;

$L__BB0_24:
	add.s32 	%r649, %r1169, %r80;
	mul.wide.s32 	%rd21, %r649, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.u16 	%rs180, [%rd22];
	bra.uni 	$L__BB0_25;

$L__BB0_23:
	mov.f32 	%f100, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f100;}

	// end inline asm

$L__BB0_25:
	st.shared.u16 	[%r624+768], %rs180;
	add.s32 	%r668, %r15, %r82;
	add.s32 	%r87, %r668, -1;
	setp.lt.u32 	%p75, %r87, 112;
	@%p75 bra 	$L__BB0_27;
	bra.uni 	$L__BB0_26;

$L__BB0_27:
	add.s32 	%r669, %r81, %r16;
	setp.lt.u32 	%p77, %r669, 112;
	setp.lt.u32 	%p78, %r1182, 363;
	and.pred  	%p219, %p78, %p77;
	bra.uni 	$L__BB0_28;

$L__BB0_26:
	mov.pred 	%p219, 0;

$L__BB0_28:
	add.s32 	%r670, %r81, %r16;
	setp.lt.u32 	%p79, %r670, 112;
	and.pred  	%p81, %p79, %p75;
	and.pred  	%p83, %p81, %p42;
	and.pred  	%p84, %p219, %p83;
	@%p84 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_29;

$L__BB0_30:
	add.s32 	%r671, %r1170, %r80;
	mul.wide.s32 	%rd23, %r671, 2;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u16 	%rs181, [%rd24];
	bra.uni 	$L__BB0_31;

$L__BB0_29:
	mov.f32 	%f101, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f101;}

	// end inline asm

$L__BB0_31:
	st.shared.u16 	[%r602+1024], %rs181;
	add.s32 	%r689, %r17, %r82;
	add.s32 	%r88, %r689, -1;
	setp.lt.u32 	%p85, %r88, 112;
	@%p85 bra 	$L__BB0_33;
	bra.uni 	$L__BB0_32;

$L__BB0_33:
	add.s32 	%r690, %r81, %r18;
	setp.lt.u32 	%p87, %r690, 112;
	setp.lt.u32 	%p88, %r1182, 363;
	and.pred  	%p220, %p88, %p87;
	bra.uni 	$L__BB0_34;

$L__BB0_32:
	mov.pred 	%p220, 0;

$L__BB0_34:
	add.s32 	%r691, %r81, %r18;
	setp.lt.u32 	%p89, %r691, 112;
	and.pred  	%p91, %p89, %p85;
	and.pred  	%p93, %p91, %p42;
	and.pred  	%p94, %p220, %p93;
	@%p94 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_35;

$L__BB0_36:
	add.s32 	%r692, %r1171, %r80;
	mul.wide.s32 	%rd25, %r692, 2;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.u16 	%rs182, [%rd26];
	bra.uni 	$L__BB0_37;

$L__BB0_35:
	mov.f32 	%f102, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs182, %f102;}

	// end inline asm

$L__BB0_37:
	st.shared.u16 	[%r624+1280], %rs182;
	add.s32 	%r711, %r19, %r82;
	add.s32 	%r89, %r711, -1;
	setp.lt.u32 	%p95, %r89, 112;
	@%p95 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_38;

$L__BB0_39:
	add.s32 	%r712, %r81, %r20;
	setp.lt.u32 	%p97, %r712, 112;
	setp.lt.u32 	%p98, %r1182, 363;
	and.pred  	%p221, %p98, %p97;
	bra.uni 	$L__BB0_40;

$L__BB0_38:
	mov.pred 	%p221, 0;

$L__BB0_40:
	add.s32 	%r713, %r81, %r20;
	setp.lt.u32 	%p99, %r713, 112;
	and.pred  	%p101, %p99, %p95;
	and.pred  	%p103, %p101, %p42;
	and.pred  	%p104, %p221, %p103;
	@%p104 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_41;

$L__BB0_42:
	add.s32 	%r714, %r1172, %r80;
	mul.wide.s32 	%rd27, %r714, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.u16 	%rs183, [%rd28];
	bra.uni 	$L__BB0_43;

$L__BB0_41:
	mov.f32 	%f103, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs183, %f103;}

	// end inline asm

$L__BB0_43:
	st.shared.u16 	[%r602+1536], %rs183;
	add.s32 	%r732, %r21, %r82;
	add.s32 	%r90, %r732, -1;
	setp.lt.u32 	%p105, %r90, 112;
	@%p105 bra 	$L__BB0_45;
	bra.uni 	$L__BB0_44;

$L__BB0_45:
	add.s32 	%r733, %r81, %r22;
	setp.lt.u32 	%p107, %r733, 112;
	setp.lt.u32 	%p108, %r1182, 363;
	and.pred  	%p222, %p108, %p107;
	bra.uni 	$L__BB0_46;

$L__BB0_44:
	mov.pred 	%p222, 0;

$L__BB0_46:
	add.s32 	%r734, %r81, %r22;
	setp.lt.u32 	%p109, %r734, 112;
	and.pred  	%p111, %p109, %p105;
	and.pred  	%p113, %p111, %p42;
	and.pred  	%p114, %p222, %p113;
	@%p114 bra 	$L__BB0_48;
	bra.uni 	$L__BB0_47;

$L__BB0_48:
	add.s32 	%r735, %r1173, %r80;
	mul.wide.s32 	%rd29, %r735, 2;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.u16 	%rs184, [%rd30];
	bra.uni 	$L__BB0_49;

$L__BB0_47:
	mov.f32 	%f104, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs184, %f104;}

	// end inline asm

$L__BB0_49:
	st.shared.u16 	[%r624+1792], %rs184;
	add.s32 	%r754, %r23, %r82;
	add.s32 	%r91, %r754, -1;
	setp.lt.u32 	%p115, %r91, 112;
	@%p115 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_50;

$L__BB0_51:
	add.s32 	%r755, %r81, %r24;
	setp.lt.u32 	%p117, %r755, 112;
	setp.lt.u32 	%p118, %r1182, 363;
	and.pred  	%p223, %p118, %p117;
	bra.uni 	$L__BB0_52;

$L__BB0_50:
	mov.pred 	%p223, 0;

$L__BB0_52:
	add.s32 	%r756, %r81, %r24;
	setp.lt.u32 	%p119, %r756, 112;
	and.pred  	%p121, %p119, %p115;
	and.pred  	%p123, %p121, %p42;
	and.pred  	%p124, %p223, %p123;
	@%p124 bra 	$L__BB0_54;
	bra.uni 	$L__BB0_53;

$L__BB0_54:
	add.s32 	%r757, %r1174, %r80;
	mul.wide.s32 	%rd31, %r757, 2;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u16 	%rs185, [%rd32];
	bra.uni 	$L__BB0_55;

$L__BB0_53:
	mov.f32 	%f105, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs185, %f105;}

	// end inline asm

$L__BB0_55:
	st.shared.u16 	[%r602+2048], %rs185;
	add.s32 	%r775, %r25, %r82;
	add.s32 	%r92, %r775, -1;
	setp.lt.u32 	%p125, %r92, 112;
	@%p125 bra 	$L__BB0_57;
	bra.uni 	$L__BB0_56;

$L__BB0_57:
	add.s32 	%r776, %r81, %r26;
	setp.lt.u32 	%p127, %r776, 112;
	setp.lt.u32 	%p128, %r1182, 363;
	and.pred  	%p224, %p128, %p127;
	bra.uni 	$L__BB0_58;

$L__BB0_56:
	mov.pred 	%p224, 0;

$L__BB0_58:
	add.s32 	%r777, %r81, %r26;
	setp.lt.u32 	%p129, %r777, 112;
	and.pred  	%p131, %p129, %p125;
	setp.lt.s32 	%p132, %r2, 9452;
	and.pred  	%p133, %p131, %p132;
	and.pred  	%p134, %p224, %p133;
	@%p134 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_59;

$L__BB0_60:
	add.s32 	%r778, %r1175, %r80;
	mul.wide.s32 	%rd33, %r778, 2;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.u16 	%rs186, [%rd34];
	bra.uni 	$L__BB0_61;

$L__BB0_59:
	mov.f32 	%f106, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs186, %f106;}

	// end inline asm

$L__BB0_61:
	st.shared.u16 	[%r624+2304], %rs186;
	add.s32 	%r797, %r27, %r82;
	add.s32 	%r93, %r797, -1;
	setp.lt.u32 	%p135, %r93, 112;
	@%p135 bra 	$L__BB0_63;
	bra.uni 	$L__BB0_62;

$L__BB0_63:
	add.s32 	%r798, %r81, %r28;
	setp.lt.u32 	%p137, %r798, 112;
	setp.lt.u32 	%p138, %r1182, 363;
	and.pred  	%p225, %p138, %p137;
	bra.uni 	$L__BB0_64;

$L__BB0_62:
	mov.pred 	%p225, 0;

$L__BB0_64:
	add.s32 	%r799, %r81, %r28;
	setp.lt.u32 	%p139, %r799, 112;
	and.pred  	%p141, %p139, %p135;
	and.pred  	%p143, %p141, %p132;
	and.pred  	%p144, %p225, %p143;
	@%p144 bra 	$L__BB0_66;
	bra.uni 	$L__BB0_65;

$L__BB0_66:
	add.s32 	%r800, %r1176, %r80;
	mul.wide.s32 	%rd35, %r800, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.nc.u16 	%rs187, [%rd36];
	bra.uni 	$L__BB0_67;

$L__BB0_65:
	mov.f32 	%f107, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs187, %f107;}

	// end inline asm

$L__BB0_67:
	st.shared.u16 	[%r602+2560], %rs187;
	add.s32 	%r818, %r29, %r82;
	add.s32 	%r94, %r818, -1;
	setp.lt.u32 	%p145, %r94, 112;
	@%p145 bra 	$L__BB0_69;
	bra.uni 	$L__BB0_68;

$L__BB0_69:
	add.s32 	%r819, %r81, %r30;
	setp.lt.u32 	%p147, %r819, 112;
	setp.lt.u32 	%p148, %r1182, 363;
	and.pred  	%p226, %p148, %p147;
	bra.uni 	$L__BB0_70;

$L__BB0_68:
	mov.pred 	%p226, 0;

$L__BB0_70:
	add.s32 	%r820, %r81, %r30;
	setp.lt.u32 	%p149, %r820, 112;
	and.pred  	%p151, %p149, %p145;
	and.pred  	%p153, %p151, %p132;
	and.pred  	%p154, %p226, %p153;
	@%p154 bra 	$L__BB0_72;
	bra.uni 	$L__BB0_71;

$L__BB0_72:
	add.s32 	%r821, %r1177, %r80;
	mul.wide.s32 	%rd37, %r821, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs188, [%rd38];
	bra.uni 	$L__BB0_73;

$L__BB0_71:
	mov.f32 	%f108, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs188, %f108;}

	// end inline asm

$L__BB0_73:
	st.shared.u16 	[%r624+2816], %rs188;
	add.s32 	%r840, %r31, %r82;
	add.s32 	%r95, %r840, -1;
	setp.lt.u32 	%p155, %r95, 112;
	@%p155 bra 	$L__BB0_75;
	bra.uni 	$L__BB0_74;

$L__BB0_75:
	add.s32 	%r841, %r81, %r32;
	setp.lt.u32 	%p157, %r841, 112;
	setp.lt.u32 	%p158, %r1182, 363;
	and.pred  	%p227, %p158, %p157;
	bra.uni 	$L__BB0_76;

$L__BB0_74:
	mov.pred 	%p227, 0;

$L__BB0_76:
	add.s32 	%r842, %r81, %r32;
	setp.lt.u32 	%p159, %r842, 112;
	and.pred  	%p161, %p159, %p155;
	and.pred  	%p163, %p161, %p132;
	and.pred  	%p164, %p227, %p163;
	@%p164 bra 	$L__BB0_78;
	bra.uni 	$L__BB0_77;

$L__BB0_78:
	add.s32 	%r843, %r1178, %r80;
	mul.wide.s32 	%rd39, %r843, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.nc.u16 	%rs189, [%rd40];
	bra.uni 	$L__BB0_79;

$L__BB0_77:
	mov.f32 	%f109, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs189, %f109;}

	// end inline asm

$L__BB0_79:
	st.shared.u16 	[%r602+3072], %rs189;
	add.s32 	%r861, %r33, %r82;
	add.s32 	%r96, %r861, -1;
	setp.lt.u32 	%p165, %r96, 112;
	@%p165 bra 	$L__BB0_81;
	bra.uni 	$L__BB0_80;

$L__BB0_81:
	add.s32 	%r862, %r81, %r34;
	setp.lt.u32 	%p167, %r862, 112;
	setp.lt.u32 	%p168, %r1182, 363;
	and.pred  	%p228, %p168, %p167;
	bra.uni 	$L__BB0_82;

$L__BB0_80:
	mov.pred 	%p228, 0;

$L__BB0_82:
	add.s32 	%r863, %r81, %r34;
	setp.lt.u32 	%p169, %r863, 112;
	and.pred  	%p171, %p169, %p165;
	and.pred  	%p173, %p171, %p132;
	and.pred  	%p174, %p228, %p173;
	@%p174 bra 	$L__BB0_84;
	bra.uni 	$L__BB0_83;

$L__BB0_84:
	add.s32 	%r864, %r1179, %r80;
	mul.wide.s32 	%rd41, %r864, 2;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.nc.u16 	%rs190, [%rd42];
	bra.uni 	$L__BB0_85;

$L__BB0_83:
	mov.f32 	%f110, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs190, %f110;}

	// end inline asm

$L__BB0_85:
	st.shared.u16 	[%r624+3328], %rs190;
	add.s32 	%r883, %r35, %r82;
	add.s32 	%r97, %r883, -1;
	setp.lt.u32 	%p175, %r97, 112;
	@%p175 bra 	$L__BB0_87;
	bra.uni 	$L__BB0_86;

$L__BB0_87:
	add.s32 	%r884, %r81, %r36;
	setp.lt.u32 	%p177, %r884, 112;
	setp.lt.u32 	%p178, %r1182, 363;
	and.pred  	%p229, %p178, %p177;
	bra.uni 	$L__BB0_88;

$L__BB0_86:
	mov.pred 	%p229, 0;

$L__BB0_88:
	add.s32 	%r885, %r81, %r36;
	setp.lt.u32 	%p179, %r885, 112;
	and.pred  	%p181, %p179, %p175;
	and.pred  	%p183, %p181, %p132;
	and.pred  	%p184, %p229, %p183;
	@%p184 bra 	$L__BB0_90;
	bra.uni 	$L__BB0_89;

$L__BB0_90:
	add.s32 	%r886, %r1180, %r80;
	mul.wide.s32 	%rd43, %r886, 2;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.nc.u16 	%rs191, [%rd44];
	bra.uni 	$L__BB0_91;

$L__BB0_89:
	mov.f32 	%f111, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs191, %f111;}

	// end inline asm

$L__BB0_91:
	st.shared.u16 	[%r602+3584], %rs191;
	add.s32 	%r903, %r37, %r82;
	add.s32 	%r99, %r903, -1;
	setp.lt.u32 	%p185, %r99, 112;
	@%p185 bra 	$L__BB0_93;
	bra.uni 	$L__BB0_92;

$L__BB0_93:
	add.s32 	%r904, %r81, %r38;
	setp.lt.u32 	%p187, %r904, 112;
	setp.lt.u32 	%p188, %r1182, 363;
	and.pred  	%p230, %p188, %p187;
	bra.uni 	$L__BB0_94;

$L__BB0_92:
	mov.pred 	%p230, 0;

$L__BB0_94:
	add.s32 	%r905, %r81, %r38;
	setp.lt.u32 	%p189, %r905, 112;
	and.pred  	%p191, %p189, %p185;
	and.pred  	%p193, %p191, %p132;
	and.pred  	%p194, %p230, %p193;
	@%p194 bra 	$L__BB0_96;
	bra.uni 	$L__BB0_95;

$L__BB0_96:
	add.s32 	%r906, %r1181, %r80;
	mul.wide.s32 	%rd45, %r906, 2;
	add.s64 	%rd46, %rd2, %rd45;
	ld.global.nc.u16 	%rs192, [%rd46];
	bra.uni 	$L__BB0_97;

$L__BB0_95:
	mov.f32 	%f112, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs192, %f112;}

	// end inline asm

$L__BB0_97:
	st.shared.u16 	[%r624+3840], %rs192;
	setp.lt.s32 	%p195, %r1183, 363;
	setp.ne.s32 	%p196, %r3, 3;
	and.pred  	%p197, %p196, %p195;
	or.b32  	%r928, %r153, %r155;
	shr.s32 	%r929, %r147, 3;
	mad.lo.s32 	%r930, %r929, 96, %r928;
	mul.wide.s32 	%rd47, %r930, 2;
	add.s64 	%rd4, %rd51, %rd47;
	@%p197 bra 	$L__BB0_99;
	bra.uni 	$L__BB0_98;

$L__BB0_99:
	ld.global.nc.v4.u32 	{%r1185, %r1186, %r1187, %r1188}, [%rd4];
	bra.uni 	$L__BB0_100;

$L__BB0_98:
	mov.f32 	%f120, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs67, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f120;}

	// end inline asm
	mov.b32 	%r1188, {%rs71, %rs72};
	mov.b32 	%r1187, {%rs69, %rs70};
	mov.b32 	%r1186, {%rs67, %rs68};
	mov.b32 	%r1185, {%rs65, %rs66};

$L__BB0_100:
	setp.ne.s32 	%p214, %r3, 3;
	shl.b32 	%r1155, %r147, 3;
	and.b32  	%r935, %r147, 32;
	shr.u32 	%r936, %r935, 5;
	add.s32 	%r939, %r936, %r151;
	shl.b32 	%r940, %r939, 5;
	and.b32  	%r941, %r940, 32;
	and.b32  	%r943, %r1155, 2147483584;
	or.b32  	%r944, %r941, %r943;
	and.b32  	%r945, %r147, 2;
	shr.u32 	%r946, %r945, 1;
	shr.u32 	%r947, %r147, 4;
	add.s32 	%r948, %r946, %r947;
	shl.b32 	%r949, %r948, 4;
	and.b32  	%r950, %r949, 16;
	or.b32  	%r951, %r944, %r950;
	add.s32 	%r954, %r592, %r147;
	shl.b32 	%r955, %r954, 3;
	and.b32  	%r956, %r955, 8;
	or.b32  	%r957, %r951, %r956;
	shl.b32 	%r958, %r957, 1;
	add.s32 	%r112, %r526, %r958;
	st.shared.v4.u32 	[%r112], {%r1185, %r1186, %r1187, %r1188};
	add.s32 	%r960, %r1183, 16;
	setp.lt.s32 	%p198, %r960, 363;
	and.pred  	%p200, %p214, %p198;
	@%p200 bra 	$L__BB0_102;
	bra.uni 	$L__BB0_101;

$L__BB0_102:
	ld.global.nc.v4.u32 	{%r1189, %r1190, %r1191, %r1192}, [%rd4+3072];
	bra.uni 	$L__BB0_103;

$L__BB0_101:
	mov.f32 	%f128, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs76, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs77, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f128;}

	// end inline asm
	mov.b32 	%r1192, {%rs79, %rs80};
	mov.b32 	%r1191, {%rs77, %rs78};
	mov.b32 	%r1190, {%rs75, %rs76};
	mov.b32 	%r1189, {%rs73, %rs74};

$L__BB0_103:
	add.s32 	%r1047, %r40, 2048;
	add.s32 	%r974, %r39, 2048;
	st.shared.v4.u32 	[%r112+2048], {%r1189, %r1190, %r1191, %r1192};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r965, %r966, %r967, %r968}, [%r39];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r970, %r971, %r972, %r973}, [%r974];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r975, %r976, %r977, %r978}, [%r40];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r980, %r981, %r982, %r983}, [%r41];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r965,  %r966,  %r967,  %r968},{%r975,  %r976},{%f384, %f383, %f382, %f381};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r970,  %r971,  %r972,  %r973},{%r975,  %r976},{%f380, %f379, %f378, %f377};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f145,  %f146,  %f147,  %f148},{%r970,  %r971,  %r972,  %r973},{%r977,  %r978},{%f372, %f371, %f370, %f369};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f153,  %f154,  %f155,  %f156},{%r965,  %r966,  %r967,  %r968},{%r977,  %r978},{%f376, %f375, %f374, %f373};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f161,  %f162,  %f163,  %f164},{%r965,  %r966,  %r967,  %r968},{%r980,  %r981},{%f368, %f367, %f366, %f365};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f169,  %f170,  %f171,  %f172},{%r970,  %r971,  %r972,  %r973},{%r980,  %r981},{%f364, %f363, %f362, %f361};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f177,  %f178,  %f179,  %f180},{%r970,  %r971,  %r972,  %r973},{%r982,  %r983},{%f356, %f355, %f354, %f353};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f185,  %f186,  %f187,  %f188},{%r965,  %r966,  %r967,  %r968},{%r982,  %r983},{%f360, %f359, %f358, %f357};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1033, %r1034, %r1035, %r1036}, [%r42];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1038, %r1039, %r1040, %r1041}, [%r43];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1043, %r1044, %r1045, %r1046}, [%r1047];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1048, %r1049, %r1050, %r1051}, [%r44];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f384,  %f383,  %f382,  %f381},{%r1033,  %r1034,  %r1035,  %r1036},{%r1043,  %r1044},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f380,  %f379,  %f378,  %f377},{%r1038,  %r1039,  %r1040,  %r1041},{%r1043,  %r1044},{%f137, %f138, %f139, %f140};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f372,  %f371,  %f370,  %f369},{%r1038,  %r1039,  %r1040,  %r1041},{%r1045,  %r1046},{%f145, %f146, %f147, %f148};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f376,  %f375,  %f374,  %f373},{%r1033,  %r1034,  %r1035,  %r1036},{%r1045,  %r1046},{%f153, %f154, %f155, %f156};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f368,  %f367,  %f366,  %f365},{%r1033,  %r1034,  %r1035,  %r1036},{%r1048,  %r1049},{%f161, %f162, %f163, %f164};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f364,  %f363,  %f362,  %f361},{%r1038,  %r1039,  %r1040,  %r1041},{%r1048,  %r1049},{%f169, %f170, %f171, %f172};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f356,  %f355,  %f354,  %f353},{%r1038,  %r1039,  %r1040,  %r1041},{%r1050,  %r1051},{%f177, %f178, %f179, %f180};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f360,  %f359,  %f358,  %f357},{%r1033,  %r1034,  %r1035,  %r1036},{%r1050,  %r1051},{%f185, %f186, %f187, %f188};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd51, %rd51, 6144;
	add.s32 	%r1183, %r1183, 32;
	add.s32 	%r1182, %r1182, 32;
	add.s32 	%r1181, %r1181, 32;
	add.s32 	%r1180, %r1180, 32;
	add.s32 	%r1179, %r1179, 32;
	add.s32 	%r1178, %r1178, 32;
	add.s32 	%r1177, %r1177, 32;
	add.s32 	%r1176, %r1176, 32;
	add.s32 	%r1175, %r1175, 32;
	add.s32 	%r1174, %r1174, 32;
	add.s32 	%r1173, %r1173, 32;
	add.s32 	%r1172, %r1172, 32;
	add.s32 	%r1171, %r1171, 32;
	add.s32 	%r1170, %r1170, 32;
	add.s32 	%r1169, %r1169, 32;
	add.s32 	%r1168, %r1168, 32;
	add.s32 	%r1167, %r1167, 32;
	add.s32 	%r1166, %r1166, 32;
	add.s32 	%r1184, %r1184, 32;
	setp.ne.s32 	%p201, %r1184, 400;
	@%p201 bra 	$L__BB0_1;

	ld.param.u64 	%rd50, [main_kernel_param_1];
	shl.b32 	%r1165, %r2, 6;
	and.b32  	%r1164, %r1165, 64;
	shr.s32 	%r1163, %r147, 3;
	shl.b32 	%r1162, %r147, 3;
	and.b32  	%r1161, %r1162, 56;
	mad.lo.s32 	%r1160, %r1163, 96, %r1164;
	add.s32 	%r1159, %r1160, %r1161;
	shr.s32 	%r1158, %r147, 5;
	shr.s32 	%r1157, %r2, 1;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f384;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f258, %rs81;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f383;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f261, %rs84;}

	// end inline asm
	shl.b32 	%r1102, %r147, 5;
	and.b32  	%r1103, %r1102, 1024;
	shl.b32 	%r1104, %r147, 4;
	and.b32  	%r1105, %r1104, 448;
	shr.s32 	%r1106, %r147, 6;
	shl.b32 	%r1107, %r1106, 3;
	shl.b32 	%r1108, %r147, 1;
	and.b32  	%r1109, %r1108, 6;
	add.s32 	%r1110, %r1107, %r1103;
	or.b32  	%r1111, %r1110, %r1109;
	add.s32 	%r1112, %r1111, %r1105;
	shl.b32 	%r1113, %r1112, 1;
	add.s32 	%r1115, %r526, %r1113;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs86, %f261;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs83, %f258;}

	// end inline asm
	st.shared.v2.u16 	[%r1115], {%rs83, %rs86};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f382;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f264, %rs87;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f381;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f267, %rs90;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs92, %f267;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f264;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+1024], {%rs89, %rs92};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f380;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f270, %rs93;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs96, %f379;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f273, %rs96;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs98, %f273;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs95, %f270;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+4096], {%rs95, %rs98};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs99, %f378;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f276, %rs99;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f377;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f279, %rs102;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs104, %f279;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs101, %f276;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+5120], {%rs101, %rs104};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs105, %f376;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f282, %rs105;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f375;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f285, %rs108;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f285;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs107, %f282;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+32], {%rs107, %rs110};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs111, %f374;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f288, %rs111;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f373;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f291, %rs114;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs116, %f291;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs113, %f288;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+1056], {%rs113, %rs116};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f372;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f294, %rs117;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f371;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f297, %rs120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs122, %f297;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs119, %f294;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+4128], {%rs119, %rs122};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs123, %f370;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f300, %rs123;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f369;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f303, %rs126;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs128, %f303;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs125, %f300;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+5152], {%rs125, %rs128};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs129, %f368;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f306, %rs129;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs132, %f367;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f309, %rs132;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs134, %f309;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs131, %f306;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+64], {%rs131, %rs134};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs135, %f366;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f312, %rs135;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs138, %f365;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f315, %rs138;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs140, %f315;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs137, %f312;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+1088], {%rs137, %rs140};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs141, %f364;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f318, %rs141;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs144, %f363;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f321, %rs144;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs146, %f321;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs143, %f318;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+4160], {%rs143, %rs146};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs147, %f362;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f324, %rs147;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs150, %f361;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f327, %rs150;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs152, %f327;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs149, %f324;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+5184], {%rs149, %rs152};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs153, %f360;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f330, %rs153;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs156, %f359;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f333, %rs156;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs158, %f333;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs155, %f330;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+96], {%rs155, %rs158};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs159, %f358;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f336, %rs159;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs162, %f357;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f339, %rs162;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs164, %f339;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs161, %f336;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+1120], {%rs161, %rs164};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs165, %f356;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f342, %rs165;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs168, %f355;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f345, %rs168;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs170, %f345;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs167, %f342;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+4192], {%rs167, %rs170};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs171, %f354;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f348, %rs171;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs174, %f353;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f351, %rs174;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs176, %f351;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs173, %f348;}

	// end inline asm
	st.shared.v2.u16 	[%r1115+5216], {%rs173, %rs176};
	bar.sync 	0;
	shl.b32 	%r1117, %r1157, 4;
	add.s32 	%r144, %r1117, %r1158;
	mad.lo.s32 	%r1118, %r1157, 6144, %r1159;
	setp.gt.s32 	%p202, %r144, 75624;
	setp.eq.s32 	%p203, %r3, 3;
	or.pred  	%p204, %p202, %p203;
	add.s32 	%r145, %r526, %r1104;
	cvta.to.global.u64 	%rd48, %rd50;
	mul.wide.s32 	%rd49, %r1118, 2;
	add.s64 	%rd6, %rd48, %rd49;
	@%p204 bra 	$L__BB0_106;

	ld.shared.v4.u32 	{%r1119, %r1120, %r1121, %r1122}, [%r145];
	st.global.v4.u32 	[%rd6], {%r1119, %r1120, %r1121, %r1122};

$L__BB0_106:
	add.s32 	%r1127, %r144, 4;
	setp.gt.s32 	%p206, %r1127, 75624;
	or.pred  	%p207, %p206, %p203;
	@%p207 bra 	$L__BB0_108;

	ld.shared.v4.u32 	{%r1128, %r1129, %r1130, %r1131}, [%r145+2048];
	st.global.v4.u32 	[%rd6+3072], {%r1128, %r1129, %r1130, %r1131};

$L__BB0_108:
	add.s32 	%r1136, %r144, 8;
	setp.gt.s32 	%p209, %r1136, 75624;
	or.pred  	%p210, %p209, %p203;
	@%p210 bra 	$L__BB0_110;

	ld.shared.v4.u32 	{%r1137, %r1138, %r1139, %r1140}, [%r145+4096];
	st.global.v4.u32 	[%rd6+6144], {%r1137, %r1138, %r1139, %r1140};

$L__BB0_110:
	add.s32 	%r1145, %r144, 12;
	setp.gt.s32 	%p212, %r1145, 75624;
	or.pred  	%p213, %p212, %p203;
	@%p213 bra 	$L__BB0_112;

	ld.shared.v4.u32 	{%r1146, %r1147, %r1148, %r1149}, [%r145+6144];
	st.global.v4.u32 	[%rd6+9216], {%r1146, %r1147, %r1148, %r1149};

$L__BB0_112:
	ret;

}

