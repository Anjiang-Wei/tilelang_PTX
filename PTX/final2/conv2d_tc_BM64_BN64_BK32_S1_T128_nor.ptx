//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<228>;
	.reg .b16 	%rs<193>;
	.reg .f32 	%f<385>;
	.reg .b32 	%r<1202>;
	.reg .b64 	%rd<52>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_2];
	cvta.to.global.u64 	%rd51, %rd9;
	cvta.to.global.u64 	%rd2, %rd8;
	mov.u32 	%r145, %tid.x;
	shr.s32 	%r1192, %r145, 3;
	mov.u32 	%r1193, 16;
	shr.s32 	%r146, %r145, 31;
	shr.u32 	%r147, %r146, 28;
	add.s32 	%r148, %r145, %r147;
	and.b32  	%r149, %r148, -16;
	sub.s32 	%r150, %r145, %r149;
	shr.u32 	%r151, %r150, 31;
	add.s32 	%r152, %r150, %r151;
	shr.s32 	%r153, %r152, 1;
	shr.s32 	%r154, %r152, 31;
	shr.u32 	%r155, %r154, 30;
	add.s32 	%r156, %r153, %r155;
	and.b32  	%r157, %r156, -4;
	sub.s32 	%r158, %r153, %r157;
	shr.u32 	%r159, %r148, 31;
	shr.s32 	%r160, %r148, 4;
	add.s32 	%r161, %r160, %r159;
	and.b32  	%r162, %r161, -2;
	sub.s32 	%r163, %r160, %r162;
	shl.b32 	%r164, %r158, 6;
	and.b32  	%r165, %r164, 192;
	shl.b32 	%r166, %r163, 3;
	and.b32  	%r167, %r166, 8;
	or.b32  	%r168, %r165, %r167;
	and.b32  	%r169, %r152, 134217726;
	sub.s32 	%r170, %r150, %r169;
	shl.b32 	%r171, %r170, 5;
	shr.s32 	%r172, %r150, 31;
	shr.u32 	%r173, %r172, 29;
	add.s32 	%r174, %r150, %r173;
	shr.s32 	%r175, %r174, 3;
	shl.b32 	%r176, %r175, 8;
	add.s32 	%r177, %r171, %r176;
	shr.u32 	%r178, %r146, 27;
	add.s32 	%r179, %r145, %r178;
	shr.u32 	%r180, %r179, 31;
	shr.s32 	%r181, %r179, 5;
	add.s32 	%r182, %r181, %r180;
	and.b32  	%r183, %r182, 4194302;
	sub.s32 	%r184, %r181, %r183;
	shl.b32 	%r185, %r184, 9;
	add.s32 	%r186, %r177, %r185;
	and.b32  	%r187, %r158, 2;
	setp.eq.s32 	%p34, %r187, 0;
	shr.u32 	%r188, %r165, 3;
	xor.b32  	%r189, %r168, %r188;
	add.s32 	%r190, %r186, %r189;
	and.b32  	%r191, %r174, -8;
	sub.s32 	%r192, %r150, %r191;
	shr.u32 	%r193, %r146, 26;
	add.s32 	%r194, %r145, %r193;
	shl.b32 	%r195, %r192, 6;
	and.b32  	%r196, %r195, 448;
	shl.b32 	%r197, %r163, 4;
	and.b32  	%r198, %r197, 16;
	shr.u32 	%r199, %r194, 3;
	and.b32  	%r200, %r199, 8;
	or.b32  	%r201, %r198, %r200;
	or.b32  	%r202, %r201, %r196;
	shl.b32 	%r203, %r175, 9;
	and.b32  	%r204, %r192, 4;
	setp.eq.s32 	%p35, %r204, 0;
	shr.u32 	%r205, %r196, 3;
	xor.b32  	%r206, %r202, %r205;
	or.b32  	%r207, %r206, %r203;
	shr.s32 	%r208, %r145, 5;
	mov.u32 	%r2, %ctaid.y;
	shl.b32 	%r209, %r2, 6;
	add.s32 	%r210, %r208, %r209;
	and.b32  	%r1191, %r145, 31;
	mul.hi.s32 	%r211, %r210, -1387167949;
	add.s32 	%r212, %r211, %r210;
	shr.u32 	%r213, %r212, 31;
	shr.s32 	%r214, %r212, 11;
	add.s32 	%r215, %r214, %r213;
	mul.lo.s32 	%r216, %r215, 3025;
	sub.s32 	%r217, %r210, %r216;
	mul.hi.s32 	%r218, %r217, 156180629;
	shr.u32 	%r219, %r218, 31;
	shr.s32 	%r220, %r218, 1;
	add.s32 	%r221, %r220, %r219;
	shl.b32 	%r4, %r221, 1;
	mul.hi.s32 	%r222, %r210, 156180629;
	shr.u32 	%r223, %r222, 31;
	shr.s32 	%r224, %r222, 1;
	add.s32 	%r225, %r224, %r223;
	mul.lo.s32 	%r226, %r225, 55;
	sub.s32 	%r227, %r210, %r226;
	shl.b32 	%r5, %r227, 1;
	mul.lo.s32 	%r228, %r227, 12;
	add.s32 	%r229, %r210, 4;
	mul.hi.s32 	%r230, %r229, -1387167949;
	add.s32 	%r231, %r230, %r229;
	shr.u32 	%r232, %r231, 31;
	shr.s32 	%r233, %r231, 11;
	add.s32 	%r234, %r233, %r232;
	mul.lo.s32 	%r235, %r234, 3025;
	sub.s32 	%r236, %r229, %r235;
	mul.hi.s32 	%r237, %r236, 156180629;
	shr.u32 	%r238, %r237, 31;
	shr.s32 	%r239, %r237, 1;
	add.s32 	%r240, %r239, %r238;
	shl.b32 	%r6, %r240, 1;
	mul.hi.s32 	%r241, %r229, 156180629;
	shr.u32 	%r242, %r241, 31;
	shr.s32 	%r243, %r241, 1;
	add.s32 	%r244, %r243, %r242;
	mul.lo.s32 	%r245, %r244, 55;
	sub.s32 	%r246, %r229, %r245;
	shl.b32 	%r7, %r246, 1;
	mul.lo.s32 	%r247, %r246, 12;
	add.s32 	%r248, %r210, 8;
	mul.hi.s32 	%r249, %r248, -1387167949;
	add.s32 	%r250, %r249, %r248;
	shr.u32 	%r251, %r250, 31;
	shr.s32 	%r252, %r250, 11;
	add.s32 	%r253, %r252, %r251;
	mul.lo.s32 	%r254, %r253, 3025;
	sub.s32 	%r255, %r248, %r254;
	mul.hi.s32 	%r256, %r255, 156180629;
	shr.u32 	%r257, %r256, 31;
	shr.s32 	%r258, %r256, 1;
	add.s32 	%r259, %r258, %r257;
	shl.b32 	%r8, %r259, 1;
	mul.hi.s32 	%r260, %r248, 156180629;
	shr.u32 	%r261, %r260, 31;
	shr.s32 	%r262, %r260, 1;
	add.s32 	%r263, %r262, %r261;
	mul.lo.s32 	%r264, %r263, 55;
	sub.s32 	%r265, %r248, %r264;
	shl.b32 	%r9, %r265, 1;
	mul.lo.s32 	%r266, %r265, 12;
	add.s32 	%r267, %r210, 12;
	mul.hi.s32 	%r268, %r267, -1387167949;
	add.s32 	%r269, %r268, %r267;
	shr.u32 	%r270, %r269, 31;
	shr.s32 	%r271, %r269, 11;
	add.s32 	%r272, %r271, %r270;
	mul.lo.s32 	%r273, %r272, 3025;
	sub.s32 	%r274, %r267, %r273;
	mul.hi.s32 	%r275, %r274, 156180629;
	shr.u32 	%r276, %r275, 31;
	shr.s32 	%r277, %r275, 1;
	add.s32 	%r278, %r277, %r276;
	shl.b32 	%r10, %r278, 1;
	mul.hi.s32 	%r279, %r267, 156180629;
	shr.u32 	%r280, %r279, 31;
	shr.s32 	%r281, %r279, 1;
	add.s32 	%r282, %r281, %r280;
	mul.lo.s32 	%r283, %r282, 55;
	sub.s32 	%r284, %r267, %r283;
	shl.b32 	%r11, %r284, 1;
	mul.lo.s32 	%r285, %r284, 12;
	add.s32 	%r286, %r210, 16;
	mul.hi.s32 	%r287, %r286, -1387167949;
	add.s32 	%r288, %r287, %r286;
	shr.u32 	%r289, %r288, 31;
	shr.s32 	%r290, %r288, 11;
	add.s32 	%r291, %r290, %r289;
	mul.lo.s32 	%r292, %r291, 3025;
	sub.s32 	%r293, %r286, %r292;
	mul.hi.s32 	%r294, %r293, 156180629;
	shr.u32 	%r295, %r294, 31;
	shr.s32 	%r296, %r294, 1;
	add.s32 	%r297, %r296, %r295;
	shl.b32 	%r12, %r297, 1;
	mul.hi.s32 	%r298, %r286, 156180629;
	shr.u32 	%r299, %r298, 31;
	shr.s32 	%r300, %r298, 1;
	add.s32 	%r301, %r300, %r299;
	mul.lo.s32 	%r302, %r301, 55;
	sub.s32 	%r303, %r286, %r302;
	shl.b32 	%r13, %r303, 1;
	mul.lo.s32 	%r304, %r303, 12;
	add.s32 	%r305, %r210, 20;
	mul.hi.s32 	%r306, %r305, -1387167949;
	add.s32 	%r307, %r306, %r305;
	shr.u32 	%r308, %r307, 31;
	shr.s32 	%r309, %r307, 11;
	add.s32 	%r310, %r309, %r308;
	mul.lo.s32 	%r311, %r310, 3025;
	sub.s32 	%r312, %r305, %r311;
	mul.hi.s32 	%r313, %r312, 156180629;
	shr.u32 	%r314, %r313, 31;
	shr.s32 	%r315, %r313, 1;
	add.s32 	%r316, %r315, %r314;
	shl.b32 	%r14, %r316, 1;
	mul.hi.s32 	%r317, %r305, 156180629;
	shr.u32 	%r318, %r317, 31;
	shr.s32 	%r319, %r317, 1;
	add.s32 	%r320, %r319, %r318;
	mul.lo.s32 	%r321, %r320, 55;
	sub.s32 	%r322, %r305, %r321;
	shl.b32 	%r15, %r322, 1;
	mul.lo.s32 	%r323, %r322, 12;
	add.s32 	%r324, %r210, 24;
	mul.hi.s32 	%r325, %r324, -1387167949;
	add.s32 	%r326, %r325, %r324;
	shr.u32 	%r327, %r326, 31;
	shr.s32 	%r328, %r326, 11;
	add.s32 	%r329, %r328, %r327;
	mul.lo.s32 	%r330, %r329, 3025;
	sub.s32 	%r331, %r324, %r330;
	mul.hi.s32 	%r332, %r331, 156180629;
	shr.u32 	%r333, %r332, 31;
	shr.s32 	%r334, %r332, 1;
	add.s32 	%r335, %r334, %r333;
	shl.b32 	%r16, %r335, 1;
	mul.hi.s32 	%r336, %r324, 156180629;
	shr.u32 	%r337, %r336, 31;
	shr.s32 	%r338, %r336, 1;
	add.s32 	%r339, %r338, %r337;
	mul.lo.s32 	%r340, %r339, 55;
	sub.s32 	%r341, %r324, %r340;
	shl.b32 	%r17, %r341, 1;
	mul.lo.s32 	%r342, %r341, 12;
	add.s32 	%r343, %r210, 28;
	mul.hi.s32 	%r344, %r343, -1387167949;
	add.s32 	%r345, %r344, %r343;
	shr.u32 	%r346, %r345, 31;
	shr.s32 	%r347, %r345, 11;
	add.s32 	%r348, %r347, %r346;
	mul.lo.s32 	%r349, %r348, 3025;
	sub.s32 	%r350, %r343, %r349;
	mul.hi.s32 	%r351, %r350, 156180629;
	shr.u32 	%r352, %r351, 31;
	shr.s32 	%r353, %r351, 1;
	add.s32 	%r354, %r353, %r352;
	shl.b32 	%r18, %r354, 1;
	mul.hi.s32 	%r355, %r343, 156180629;
	shr.u32 	%r356, %r355, 31;
	shr.s32 	%r357, %r355, 1;
	add.s32 	%r358, %r357, %r356;
	mul.lo.s32 	%r359, %r358, 55;
	sub.s32 	%r360, %r343, %r359;
	shl.b32 	%r19, %r360, 1;
	mul.lo.s32 	%r361, %r360, 12;
	add.s32 	%r362, %r210, 32;
	mul.hi.s32 	%r363, %r362, -1387167949;
	add.s32 	%r364, %r363, %r362;
	shr.u32 	%r365, %r364, 31;
	shr.s32 	%r366, %r364, 11;
	add.s32 	%r367, %r366, %r365;
	mul.lo.s32 	%r368, %r367, 3025;
	sub.s32 	%r369, %r362, %r368;
	mul.hi.s32 	%r370, %r369, 156180629;
	shr.u32 	%r371, %r370, 31;
	shr.s32 	%r372, %r370, 1;
	add.s32 	%r373, %r372, %r371;
	shl.b32 	%r20, %r373, 1;
	mul.hi.s32 	%r374, %r362, 156180629;
	shr.u32 	%r375, %r374, 31;
	shr.s32 	%r376, %r374, 1;
	add.s32 	%r377, %r376, %r375;
	mul.lo.s32 	%r378, %r377, 55;
	sub.s32 	%r379, %r362, %r378;
	shl.b32 	%r21, %r379, 1;
	mul.lo.s32 	%r380, %r379, 12;
	add.s32 	%r381, %r210, 36;
	mul.hi.s32 	%r382, %r381, -1387167949;
	add.s32 	%r383, %r382, %r381;
	shr.u32 	%r384, %r383, 31;
	shr.s32 	%r385, %r383, 11;
	add.s32 	%r386, %r385, %r384;
	mul.lo.s32 	%r387, %r386, 3025;
	sub.s32 	%r388, %r381, %r387;
	mul.hi.s32 	%r389, %r388, 156180629;
	shr.u32 	%r390, %r389, 31;
	shr.s32 	%r391, %r389, 1;
	add.s32 	%r392, %r391, %r390;
	shl.b32 	%r22, %r392, 1;
	mul.hi.s32 	%r393, %r381, 156180629;
	shr.u32 	%r394, %r393, 31;
	shr.s32 	%r395, %r393, 1;
	add.s32 	%r396, %r395, %r394;
	mul.lo.s32 	%r397, %r396, 55;
	sub.s32 	%r398, %r381, %r397;
	shl.b32 	%r23, %r398, 1;
	mul.lo.s32 	%r399, %r398, 12;
	add.s32 	%r400, %r210, 40;
	mul.hi.s32 	%r401, %r400, -1387167949;
	add.s32 	%r402, %r401, %r400;
	shr.u32 	%r403, %r402, 31;
	shr.s32 	%r404, %r402, 11;
	add.s32 	%r405, %r404, %r403;
	mul.lo.s32 	%r406, %r405, 3025;
	sub.s32 	%r407, %r400, %r406;
	mul.hi.s32 	%r408, %r407, 156180629;
	shr.u32 	%r409, %r408, 31;
	shr.s32 	%r410, %r408, 1;
	add.s32 	%r411, %r410, %r409;
	shl.b32 	%r24, %r411, 1;
	mul.hi.s32 	%r412, %r400, 156180629;
	shr.u32 	%r413, %r412, 31;
	shr.s32 	%r414, %r412, 1;
	add.s32 	%r415, %r414, %r413;
	mul.lo.s32 	%r416, %r415, 55;
	sub.s32 	%r417, %r400, %r416;
	shl.b32 	%r25, %r417, 1;
	mul.lo.s32 	%r418, %r417, 12;
	add.s32 	%r419, %r210, 44;
	mul.hi.s32 	%r420, %r419, -1387167949;
	add.s32 	%r421, %r420, %r419;
	shr.u32 	%r422, %r421, 31;
	shr.s32 	%r423, %r421, 11;
	add.s32 	%r424, %r423, %r422;
	mul.lo.s32 	%r425, %r424, 3025;
	sub.s32 	%r426, %r419, %r425;
	mul.hi.s32 	%r427, %r426, 156180629;
	shr.u32 	%r428, %r427, 31;
	shr.s32 	%r429, %r427, 1;
	add.s32 	%r430, %r429, %r428;
	shl.b32 	%r26, %r430, 1;
	mul.hi.s32 	%r431, %r419, 156180629;
	shr.u32 	%r432, %r431, 31;
	shr.s32 	%r433, %r431, 1;
	add.s32 	%r434, %r433, %r432;
	mul.lo.s32 	%r435, %r434, 55;
	sub.s32 	%r436, %r419, %r435;
	shl.b32 	%r27, %r436, 1;
	mul.lo.s32 	%r437, %r436, 12;
	add.s32 	%r438, %r210, 48;
	mul.hi.s32 	%r439, %r438, -1387167949;
	add.s32 	%r440, %r439, %r438;
	shr.u32 	%r441, %r440, 31;
	shr.s32 	%r442, %r440, 11;
	add.s32 	%r443, %r442, %r441;
	mul.lo.s32 	%r444, %r443, 3025;
	sub.s32 	%r445, %r438, %r444;
	mul.hi.s32 	%r446, %r445, 156180629;
	shr.u32 	%r447, %r446, 31;
	shr.s32 	%r448, %r446, 1;
	add.s32 	%r449, %r448, %r447;
	shl.b32 	%r28, %r449, 1;
	mul.hi.s32 	%r450, %r438, 156180629;
	shr.u32 	%r451, %r450, 31;
	shr.s32 	%r452, %r450, 1;
	add.s32 	%r453, %r452, %r451;
	mul.lo.s32 	%r454, %r453, 55;
	sub.s32 	%r455, %r438, %r454;
	shl.b32 	%r29, %r455, 1;
	mul.lo.s32 	%r456, %r455, 12;
	add.s32 	%r457, %r210, 52;
	mul.hi.s32 	%r458, %r457, -1387167949;
	add.s32 	%r459, %r458, %r457;
	shr.u32 	%r460, %r459, 31;
	shr.s32 	%r461, %r459, 11;
	add.s32 	%r462, %r461, %r460;
	mul.lo.s32 	%r463, %r462, 3025;
	sub.s32 	%r464, %r457, %r463;
	mul.hi.s32 	%r465, %r464, 156180629;
	shr.u32 	%r466, %r465, 31;
	shr.s32 	%r467, %r465, 1;
	add.s32 	%r468, %r467, %r466;
	shl.b32 	%r30, %r468, 1;
	mul.hi.s32 	%r469, %r457, 156180629;
	shr.u32 	%r470, %r469, 31;
	shr.s32 	%r471, %r469, 1;
	add.s32 	%r472, %r471, %r470;
	mul.lo.s32 	%r473, %r472, 55;
	sub.s32 	%r474, %r457, %r473;
	shl.b32 	%r31, %r474, 1;
	mul.lo.s32 	%r475, %r474, 12;
	add.s32 	%r476, %r210, 56;
	mul.hi.s32 	%r477, %r476, -1387167949;
	add.s32 	%r478, %r477, %r476;
	shr.u32 	%r479, %r478, 31;
	shr.s32 	%r480, %r478, 11;
	add.s32 	%r481, %r480, %r479;
	mul.lo.s32 	%r482, %r481, 3025;
	sub.s32 	%r483, %r476, %r482;
	mul.hi.s32 	%r484, %r483, 156180629;
	shr.u32 	%r485, %r484, 31;
	shr.s32 	%r486, %r484, 1;
	add.s32 	%r487, %r486, %r485;
	shl.b32 	%r32, %r487, 1;
	mul.hi.s32 	%r488, %r476, 156180629;
	shr.u32 	%r489, %r488, 31;
	shr.s32 	%r490, %r488, 1;
	add.s32 	%r491, %r490, %r489;
	mul.lo.s32 	%r492, %r491, 55;
	sub.s32 	%r493, %r476, %r492;
	shl.b32 	%r33, %r493, 1;
	mul.lo.s32 	%r494, %r493, 12;
	add.s32 	%r495, %r210, 60;
	mul.hi.s32 	%r496, %r495, -1387167949;
	add.s32 	%r497, %r496, %r495;
	shr.u32 	%r498, %r497, 31;
	shr.s32 	%r499, %r497, 11;
	add.s32 	%r500, %r499, %r498;
	mul.lo.s32 	%r501, %r500, 3025;
	sub.s32 	%r502, %r495, %r501;
	mul.hi.s32 	%r503, %r502, 156180629;
	shr.u32 	%r504, %r503, 31;
	shr.s32 	%r505, %r503, 1;
	add.s32 	%r506, %r505, %r504;
	shl.b32 	%r34, %r506, 1;
	mul.hi.s32 	%r507, %r495, 156180629;
	shr.u32 	%r508, %r507, 31;
	shr.s32 	%r509, %r507, 1;
	add.s32 	%r510, %r509, %r508;
	mul.lo.s32 	%r511, %r510, 55;
	sub.s32 	%r512, %r495, %r511;
	shl.b32 	%r35, %r512, 1;
	mul.lo.s32 	%r513, %r512, 12;
	shl.b32 	%r514, %r190, 1;
	mov.u32 	%r515, buf_dyn_shmem;
	add.s32 	%r516, %r515, %r514;
	add.s32 	%r36, %r516, 4096;
	shl.b32 	%r517, %r207, 1;
	add.s32 	%r37, %r515, %r517;
	selp.b32 	%r518, 64, -64, %p35;
	add.s32 	%r38, %r37, %r518;
	selp.b32 	%r519, 32, -32, %p34;
	add.s32 	%r39, %r36, %r519;
	add.s32 	%r40, %r39, 2048;
	add.s32 	%r41, %r38, 2048;
	mad.lo.s32 	%r520, %r506, 2688, %r513;
	mad.lo.s32 	%r521, %r500, 150528, %r520;
	add.s32 	%r522, %r521, %r1191;
	add.s32 	%r1190, %r522, -1350;
	mad.lo.s32 	%r523, %r487, 2688, %r494;
	mad.lo.s32 	%r524, %r481, 150528, %r523;
	add.s32 	%r525, %r524, %r1191;
	add.s32 	%r1189, %r525, -1350;
	mad.lo.s32 	%r526, %r468, 2688, %r475;
	mad.lo.s32 	%r527, %r462, 150528, %r526;
	add.s32 	%r528, %r527, %r1191;
	add.s32 	%r1188, %r528, -1350;
	mad.lo.s32 	%r529, %r449, 2688, %r456;
	mad.lo.s32 	%r530, %r443, 150528, %r529;
	add.s32 	%r531, %r530, %r1191;
	add.s32 	%r1187, %r531, -1350;
	mad.lo.s32 	%r532, %r430, 2688, %r437;
	mad.lo.s32 	%r533, %r424, 150528, %r532;
	add.s32 	%r534, %r533, %r1191;
	add.s32 	%r1186, %r534, -1350;
	mad.lo.s32 	%r535, %r411, 2688, %r418;
	mad.lo.s32 	%r536, %r405, 150528, %r535;
	add.s32 	%r537, %r536, %r1191;
	add.s32 	%r1185, %r537, -1350;
	mad.lo.s32 	%r538, %r392, 2688, %r399;
	mad.lo.s32 	%r539, %r386, 150528, %r538;
	add.s32 	%r540, %r539, %r1191;
	add.s32 	%r1184, %r540, -1350;
	mad.lo.s32 	%r541, %r373, 2688, %r380;
	mad.lo.s32 	%r542, %r367, 150528, %r541;
	add.s32 	%r543, %r542, %r1191;
	add.s32 	%r1183, %r543, -1350;
	mad.lo.s32 	%r544, %r354, 2688, %r361;
	mad.lo.s32 	%r545, %r348, 150528, %r544;
	add.s32 	%r546, %r545, %r1191;
	add.s32 	%r1182, %r546, -1350;
	mad.lo.s32 	%r547, %r335, 2688, %r342;
	mad.lo.s32 	%r548, %r329, 150528, %r547;
	add.s32 	%r549, %r548, %r1191;
	add.s32 	%r1181, %r549, -1350;
	mad.lo.s32 	%r550, %r316, 2688, %r323;
	mad.lo.s32 	%r551, %r310, 150528, %r550;
	add.s32 	%r552, %r551, %r1191;
	add.s32 	%r1180, %r552, -1350;
	mad.lo.s32 	%r553, %r297, 2688, %r304;
	mad.lo.s32 	%r554, %r291, 150528, %r553;
	add.s32 	%r555, %r554, %r1191;
	add.s32 	%r1179, %r555, -1350;
	mad.lo.s32 	%r556, %r278, 2688, %r285;
	mad.lo.s32 	%r557, %r272, 150528, %r556;
	add.s32 	%r558, %r557, %r1191;
	add.s32 	%r1178, %r558, -1350;
	mad.lo.s32 	%r559, %r259, 2688, %r266;
	mad.lo.s32 	%r560, %r253, 150528, %r559;
	add.s32 	%r561, %r560, %r1191;
	add.s32 	%r1177, %r561, -1350;
	mad.lo.s32 	%r562, %r240, 2688, %r247;
	mad.lo.s32 	%r563, %r234, 150528, %r562;
	add.s32 	%r564, %r563, %r1191;
	add.s32 	%r1176, %r564, -1350;
	mad.lo.s32 	%r565, %r221, 2688, %r228;
	mad.lo.s32 	%r566, %r215, 150528, %r565;
	add.s32 	%r567, %r566, %r1191;
	add.s32 	%r1175, %r567, -1350;
	mov.f32 	%f353, 0f00000000;
	add.s32 	%r576, %r145, 2048;
	and.b32  	%r577, %r576, 2147483616;
	and.b32  	%r578, %r145, 7;
	or.b32  	%r579, %r577, %r578;
	and.b32  	%r580, %r145, 8;
	shr.u32 	%r581, %r580, 3;
	shr.u32 	%r582, %r145, 6;
	add.s32 	%r583, %r581, %r582;
	shl.b32 	%r584, %r583, 3;
	and.b32  	%r585, %r584, 8;
	or.b32  	%r586, %r579, %r585;
	and.b32  	%r587, %r145, 16;
	or.b32  	%r588, %r586, %r587;
	shl.b32 	%r589, %r588, 1;
	add.s32 	%r591, %r515, %r589;
	mov.f32 	%f354, %f353;
	mov.f32 	%f355, %f353;
	mov.f32 	%f356, %f353;
	mov.f32 	%f357, %f353;
	mov.f32 	%f358, %f353;
	mov.f32 	%f359, %f353;
	mov.f32 	%f360, %f353;
	mov.f32 	%f361, %f353;
	mov.f32 	%f362, %f353;
	mov.f32 	%f363, %f353;
	mov.f32 	%f364, %f353;
	mov.f32 	%f365, %f353;
	mov.f32 	%f366, %f353;
	mov.f32 	%f367, %f353;
	mov.f32 	%f368, %f353;
	mov.f32 	%f369, %f353;
	mov.f32 	%f370, %f353;
	mov.f32 	%f371, %f353;
	mov.f32 	%f372, %f353;
	mov.f32 	%f373, %f353;
	mov.f32 	%f374, %f353;
	mov.f32 	%f375, %f353;
	mov.f32 	%f376, %f353;
	mov.f32 	%f377, %f353;
	mov.f32 	%f378, %f353;
	mov.f32 	%f379, %f353;
	mov.f32 	%f380, %f353;
	mov.f32 	%f381, %f353;
	mov.f32 	%f382, %f353;
	mov.f32 	%f383, %f353;
	mov.f32 	%f384, %f353;

$L__BB0_1:
	mul.wide.u32 	%rd10, %r1191, 1041204193;
	shr.u64 	%rd11, %rd10, 35;
	cvt.u32.u64 	%r568, %rd11;
	mul.lo.s32 	%r77, %r568, 639;
	mad.lo.s32 	%r569, %r568, -33, %r1191;
	mul.wide.u32 	%rd12, %r569, -1431655765;
	shr.u64 	%rd13, %rd12, 34;
	cvt.u32.u64 	%r570, %rd13;
	add.s32 	%r78, %r570, -1;
	shr.u64 	%rd14, %rd10, 36;
	cvt.u32.u64 	%r79, %rd14;
	add.s32 	%r571, %r4, %r79;
	add.s32 	%r80, %r571, -1;
	setp.lt.u32 	%p36, %r80, 112;
	@%p36 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_2;

$L__BB0_3:
	add.s32 	%r572, %r78, %r5;
	setp.lt.u32 	%p38, %r572, 112;
	setp.lt.u32 	%p39, %r1191, 363;
	and.pred  	%p212, %p39, %p38;
	bra.uni 	$L__BB0_4;

$L__BB0_2:
	mov.pred 	%p212, 0;

$L__BB0_4:
	add.s32 	%r573, %r78, %r5;
	setp.lt.u32 	%p40, %r573, 112;
	and.pred  	%p42, %p40, %p36;
	setp.lt.s32 	%p43, %r2, 4727;
	and.pred  	%p44, %p42, %p43;
	and.pred  	%p45, %p212, %p44;
	@%p45 bra 	$L__BB0_6;
	bra.uni 	$L__BB0_5;

$L__BB0_6:
	add.s32 	%r574, %r1175, %r77;
	mul.wide.s32 	%rd15, %r574, 2;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.nc.u16 	%rs177, [%rd16];
	bra.uni 	$L__BB0_7;

$L__BB0_5:
	mov.f32 	%f97, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f97;}

	// end inline asm

$L__BB0_7:
	st.shared.u16 	[%r591], %rs177;
	add.s32 	%r592, %r6, %r79;
	add.s32 	%r81, %r592, -1;
	setp.lt.u32 	%p46, %r81, 112;
	@%p46 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_8;

$L__BB0_9:
	add.s32 	%r593, %r78, %r7;
	setp.lt.u32 	%p48, %r593, 112;
	setp.lt.u32 	%p49, %r1191, 363;
	and.pred  	%p213, %p49, %p48;
	bra.uni 	$L__BB0_10;

$L__BB0_8:
	mov.pred 	%p213, 0;

$L__BB0_10:
	add.s32 	%r594, %r78, %r7;
	setp.lt.u32 	%p50, %r594, 112;
	and.pred  	%p52, %p50, %p46;
	and.pred  	%p54, %p52, %p43;
	and.pred  	%p55, %p213, %p54;
	@%p55 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_11;

$L__BB0_12:
	add.s32 	%r595, %r1176, %r77;
	mul.wide.s32 	%rd17, %r595, 2;
	add.s64 	%rd18, %rd2, %rd17;
	ld.global.nc.u16 	%rs178, [%rd18];
	bra.uni 	$L__BB0_13;

$L__BB0_11:
	mov.f32 	%f98, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f98;}

	// end inline asm

$L__BB0_13:
	and.b32  	%r1164, %r145, 7;
	not.b32 	%r597, %r145;
	and.b32  	%r607, %r597, 16;
	or.b32  	%r608, %r607, %r585;
	or.b32  	%r609, %r608, %r1164;
	or.b32  	%r610, %r609, %r577;
	shl.b32 	%r611, %r610, 1;
	add.s32 	%r613, %r515, %r611;
	st.shared.u16 	[%r613+256], %rs178;
	add.s32 	%r614, %r8, %r79;
	add.s32 	%r82, %r614, -1;
	setp.lt.u32 	%p56, %r82, 112;
	@%p56 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_14;

$L__BB0_15:
	add.s32 	%r615, %r78, %r9;
	setp.lt.u32 	%p58, %r615, 112;
	setp.lt.u32 	%p59, %r1191, 363;
	and.pred  	%p214, %p59, %p58;
	bra.uni 	$L__BB0_16;

$L__BB0_14:
	mov.pred 	%p214, 0;

$L__BB0_16:
	add.s32 	%r616, %r78, %r9;
	setp.lt.u32 	%p60, %r616, 112;
	and.pred  	%p62, %p60, %p56;
	and.pred  	%p64, %p62, %p43;
	and.pred  	%p65, %p214, %p64;
	@%p65 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_17;

$L__BB0_18:
	add.s32 	%r617, %r1177, %r77;
	mul.wide.s32 	%rd19, %r617, 2;
	add.s64 	%rd20, %rd2, %rd19;
	ld.global.nc.u16 	%rs179, [%rd20];
	bra.uni 	$L__BB0_19;

$L__BB0_17:
	mov.f32 	%f99, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs179, %f99;}

	// end inline asm

$L__BB0_19:
	st.shared.u16 	[%r591+512], %rs179;
	add.s32 	%r635, %r10, %r79;
	add.s32 	%r83, %r635, -1;
	setp.lt.u32 	%p66, %r83, 112;
	@%p66 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;

$L__BB0_21:
	add.s32 	%r636, %r78, %r11;
	setp.lt.u32 	%p68, %r636, 112;
	setp.lt.u32 	%p69, %r1191, 363;
	and.pred  	%p215, %p69, %p68;
	bra.uni 	$L__BB0_22;

$L__BB0_20:
	mov.pred 	%p215, 0;

$L__BB0_22:
	add.s32 	%r637, %r78, %r11;
	setp.lt.u32 	%p70, %r637, 112;
	and.pred  	%p72, %p70, %p66;
	and.pred  	%p74, %p72, %p43;
	and.pred  	%p75, %p215, %p74;
	@%p75 bra 	$L__BB0_24;
	bra.uni 	$L__BB0_23;

$L__BB0_24:
	add.s32 	%r638, %r1178, %r77;
	mul.wide.s32 	%rd21, %r638, 2;
	add.s64 	%rd22, %rd2, %rd21;
	ld.global.nc.u16 	%rs180, [%rd22];
	bra.uni 	$L__BB0_25;

$L__BB0_23:
	mov.f32 	%f100, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f100;}

	// end inline asm

$L__BB0_25:
	st.shared.u16 	[%r613+768], %rs180;
	add.s32 	%r657, %r12, %r79;
	add.s32 	%r84, %r657, -1;
	setp.lt.u32 	%p76, %r84, 112;
	@%p76 bra 	$L__BB0_27;
	bra.uni 	$L__BB0_26;

$L__BB0_27:
	add.s32 	%r658, %r78, %r13;
	setp.lt.u32 	%p78, %r658, 112;
	setp.lt.u32 	%p79, %r1191, 363;
	and.pred  	%p216, %p79, %p78;
	bra.uni 	$L__BB0_28;

$L__BB0_26:
	mov.pred 	%p216, 0;

$L__BB0_28:
	add.s32 	%r659, %r78, %r13;
	setp.lt.u32 	%p80, %r659, 112;
	and.pred  	%p82, %p80, %p76;
	and.pred  	%p84, %p82, %p43;
	and.pred  	%p85, %p216, %p84;
	@%p85 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_29;

$L__BB0_30:
	add.s32 	%r660, %r1179, %r77;
	mul.wide.s32 	%rd23, %r660, 2;
	add.s64 	%rd24, %rd2, %rd23;
	ld.global.nc.u16 	%rs181, [%rd24];
	bra.uni 	$L__BB0_31;

$L__BB0_29:
	mov.f32 	%f101, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f101;}

	// end inline asm

$L__BB0_31:
	st.shared.u16 	[%r591+1024], %rs181;
	add.s32 	%r678, %r14, %r79;
	add.s32 	%r85, %r678, -1;
	setp.lt.u32 	%p86, %r85, 112;
	@%p86 bra 	$L__BB0_33;
	bra.uni 	$L__BB0_32;

$L__BB0_33:
	add.s32 	%r679, %r78, %r15;
	setp.lt.u32 	%p88, %r679, 112;
	setp.lt.u32 	%p89, %r1191, 363;
	and.pred  	%p217, %p89, %p88;
	bra.uni 	$L__BB0_34;

$L__BB0_32:
	mov.pred 	%p217, 0;

$L__BB0_34:
	add.s32 	%r680, %r78, %r15;
	setp.lt.u32 	%p90, %r680, 112;
	and.pred  	%p92, %p90, %p86;
	and.pred  	%p94, %p92, %p43;
	and.pred  	%p95, %p217, %p94;
	@%p95 bra 	$L__BB0_36;
	bra.uni 	$L__BB0_35;

$L__BB0_36:
	add.s32 	%r681, %r1180, %r77;
	mul.wide.s32 	%rd25, %r681, 2;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.u16 	%rs182, [%rd26];
	bra.uni 	$L__BB0_37;

$L__BB0_35:
	mov.f32 	%f102, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs182, %f102;}

	// end inline asm

$L__BB0_37:
	st.shared.u16 	[%r613+1280], %rs182;
	add.s32 	%r700, %r16, %r79;
	add.s32 	%r86, %r700, -1;
	setp.lt.u32 	%p96, %r86, 112;
	@%p96 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_38;

$L__BB0_39:
	add.s32 	%r701, %r78, %r17;
	setp.lt.u32 	%p98, %r701, 112;
	setp.lt.u32 	%p99, %r1191, 363;
	and.pred  	%p218, %p99, %p98;
	bra.uni 	$L__BB0_40;

$L__BB0_38:
	mov.pred 	%p218, 0;

$L__BB0_40:
	add.s32 	%r702, %r78, %r17;
	setp.lt.u32 	%p100, %r702, 112;
	and.pred  	%p102, %p100, %p96;
	and.pred  	%p104, %p102, %p43;
	and.pred  	%p105, %p218, %p104;
	@%p105 bra 	$L__BB0_42;
	bra.uni 	$L__BB0_41;

$L__BB0_42:
	add.s32 	%r703, %r1181, %r77;
	mul.wide.s32 	%rd27, %r703, 2;
	add.s64 	%rd28, %rd2, %rd27;
	ld.global.nc.u16 	%rs183, [%rd28];
	bra.uni 	$L__BB0_43;

$L__BB0_41:
	mov.f32 	%f103, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs183, %f103;}

	// end inline asm

$L__BB0_43:
	st.shared.u16 	[%r591+1536], %rs183;
	add.s32 	%r721, %r18, %r79;
	add.s32 	%r87, %r721, -1;
	setp.lt.u32 	%p106, %r87, 112;
	@%p106 bra 	$L__BB0_45;
	bra.uni 	$L__BB0_44;

$L__BB0_45:
	add.s32 	%r722, %r78, %r19;
	setp.lt.u32 	%p108, %r722, 112;
	setp.lt.u32 	%p109, %r1191, 363;
	and.pred  	%p219, %p109, %p108;
	bra.uni 	$L__BB0_46;

$L__BB0_44:
	mov.pred 	%p219, 0;

$L__BB0_46:
	add.s32 	%r723, %r78, %r19;
	setp.lt.u32 	%p110, %r723, 112;
	and.pred  	%p112, %p110, %p106;
	and.pred  	%p114, %p112, %p43;
	and.pred  	%p115, %p219, %p114;
	@%p115 bra 	$L__BB0_48;
	bra.uni 	$L__BB0_47;

$L__BB0_48:
	add.s32 	%r724, %r1182, %r77;
	mul.wide.s32 	%rd29, %r724, 2;
	add.s64 	%rd30, %rd2, %rd29;
	ld.global.nc.u16 	%rs184, [%rd30];
	bra.uni 	$L__BB0_49;

$L__BB0_47:
	mov.f32 	%f104, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs184, %f104;}

	// end inline asm

$L__BB0_49:
	st.shared.u16 	[%r613+1792], %rs184;
	add.s32 	%r743, %r20, %r79;
	add.s32 	%r88, %r743, -1;
	setp.lt.u32 	%p116, %r88, 112;
	@%p116 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_50;

$L__BB0_51:
	add.s32 	%r744, %r78, %r21;
	setp.lt.u32 	%p118, %r744, 112;
	setp.lt.u32 	%p119, %r1191, 363;
	and.pred  	%p220, %p119, %p118;
	bra.uni 	$L__BB0_52;

$L__BB0_50:
	mov.pred 	%p220, 0;

$L__BB0_52:
	add.s32 	%r745, %r78, %r21;
	setp.lt.u32 	%p120, %r745, 112;
	and.pred  	%p122, %p120, %p116;
	and.pred  	%p124, %p122, %p43;
	and.pred  	%p125, %p220, %p124;
	@%p125 bra 	$L__BB0_54;
	bra.uni 	$L__BB0_53;

$L__BB0_54:
	add.s32 	%r746, %r1183, %r77;
	mul.wide.s32 	%rd31, %r746, 2;
	add.s64 	%rd32, %rd2, %rd31;
	ld.global.nc.u16 	%rs185, [%rd32];
	bra.uni 	$L__BB0_55;

$L__BB0_53:
	mov.f32 	%f105, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs185, %f105;}

	// end inline asm

$L__BB0_55:
	st.shared.u16 	[%r591+2048], %rs185;
	add.s32 	%r764, %r22, %r79;
	add.s32 	%r89, %r764, -1;
	setp.lt.u32 	%p126, %r89, 112;
	@%p126 bra 	$L__BB0_57;
	bra.uni 	$L__BB0_56;

$L__BB0_57:
	add.s32 	%r765, %r78, %r23;
	setp.lt.u32 	%p128, %r765, 112;
	setp.lt.u32 	%p129, %r1191, 363;
	and.pred  	%p221, %p129, %p128;
	bra.uni 	$L__BB0_58;

$L__BB0_56:
	mov.pred 	%p221, 0;

$L__BB0_58:
	add.s32 	%r766, %r78, %r23;
	setp.lt.u32 	%p130, %r766, 112;
	and.pred  	%p132, %p130, %p126;
	setp.lt.s32 	%p133, %r2, 4726;
	and.pred  	%p134, %p132, %p133;
	and.pred  	%p135, %p221, %p134;
	@%p135 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_59;

$L__BB0_60:
	add.s32 	%r767, %r1184, %r77;
	mul.wide.s32 	%rd33, %r767, 2;
	add.s64 	%rd34, %rd2, %rd33;
	ld.global.nc.u16 	%rs186, [%rd34];
	bra.uni 	$L__BB0_61;

$L__BB0_59:
	mov.f32 	%f106, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs186, %f106;}

	// end inline asm

$L__BB0_61:
	st.shared.u16 	[%r613+2304], %rs186;
	add.s32 	%r786, %r24, %r79;
	add.s32 	%r90, %r786, -1;
	setp.lt.u32 	%p136, %r90, 112;
	@%p136 bra 	$L__BB0_63;
	bra.uni 	$L__BB0_62;

$L__BB0_63:
	add.s32 	%r787, %r78, %r25;
	setp.lt.u32 	%p138, %r787, 112;
	setp.lt.u32 	%p139, %r1191, 363;
	and.pred  	%p222, %p139, %p138;
	bra.uni 	$L__BB0_64;

$L__BB0_62:
	mov.pred 	%p222, 0;

$L__BB0_64:
	add.s32 	%r788, %r78, %r25;
	setp.lt.u32 	%p140, %r788, 112;
	and.pred  	%p142, %p140, %p136;
	and.pred  	%p144, %p142, %p133;
	and.pred  	%p145, %p222, %p144;
	@%p145 bra 	$L__BB0_66;
	bra.uni 	$L__BB0_65;

$L__BB0_66:
	add.s32 	%r789, %r1185, %r77;
	mul.wide.s32 	%rd35, %r789, 2;
	add.s64 	%rd36, %rd2, %rd35;
	ld.global.nc.u16 	%rs187, [%rd36];
	bra.uni 	$L__BB0_67;

$L__BB0_65:
	mov.f32 	%f107, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs187, %f107;}

	// end inline asm

$L__BB0_67:
	st.shared.u16 	[%r591+2560], %rs187;
	add.s32 	%r807, %r26, %r79;
	add.s32 	%r91, %r807, -1;
	setp.lt.u32 	%p146, %r91, 112;
	@%p146 bra 	$L__BB0_69;
	bra.uni 	$L__BB0_68;

$L__BB0_69:
	add.s32 	%r808, %r78, %r27;
	setp.lt.u32 	%p148, %r808, 112;
	setp.lt.u32 	%p149, %r1191, 363;
	and.pred  	%p223, %p149, %p148;
	bra.uni 	$L__BB0_70;

$L__BB0_68:
	mov.pred 	%p223, 0;

$L__BB0_70:
	add.s32 	%r809, %r78, %r27;
	setp.lt.u32 	%p150, %r809, 112;
	and.pred  	%p152, %p150, %p146;
	and.pred  	%p154, %p152, %p133;
	and.pred  	%p155, %p223, %p154;
	@%p155 bra 	$L__BB0_72;
	bra.uni 	$L__BB0_71;

$L__BB0_72:
	add.s32 	%r810, %r1186, %r77;
	mul.wide.s32 	%rd37, %r810, 2;
	add.s64 	%rd38, %rd2, %rd37;
	ld.global.nc.u16 	%rs188, [%rd38];
	bra.uni 	$L__BB0_73;

$L__BB0_71:
	mov.f32 	%f108, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs188, %f108;}

	// end inline asm

$L__BB0_73:
	st.shared.u16 	[%r613+2816], %rs188;
	add.s32 	%r829, %r28, %r79;
	add.s32 	%r92, %r829, -1;
	setp.lt.u32 	%p156, %r92, 112;
	@%p156 bra 	$L__BB0_75;
	bra.uni 	$L__BB0_74;

$L__BB0_75:
	add.s32 	%r830, %r78, %r29;
	setp.lt.u32 	%p158, %r830, 112;
	setp.lt.u32 	%p159, %r1191, 363;
	and.pred  	%p224, %p159, %p158;
	bra.uni 	$L__BB0_76;

$L__BB0_74:
	mov.pred 	%p224, 0;

$L__BB0_76:
	add.s32 	%r831, %r78, %r29;
	setp.lt.u32 	%p160, %r831, 112;
	and.pred  	%p162, %p160, %p156;
	and.pred  	%p164, %p162, %p133;
	and.pred  	%p165, %p224, %p164;
	@%p165 bra 	$L__BB0_78;
	bra.uni 	$L__BB0_77;

$L__BB0_78:
	add.s32 	%r832, %r1187, %r77;
	mul.wide.s32 	%rd39, %r832, 2;
	add.s64 	%rd40, %rd2, %rd39;
	ld.global.nc.u16 	%rs189, [%rd40];
	bra.uni 	$L__BB0_79;

$L__BB0_77:
	mov.f32 	%f109, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs189, %f109;}

	// end inline asm

$L__BB0_79:
	st.shared.u16 	[%r591+3072], %rs189;
	add.s32 	%r850, %r30, %r79;
	add.s32 	%r93, %r850, -1;
	setp.lt.u32 	%p166, %r93, 112;
	@%p166 bra 	$L__BB0_81;
	bra.uni 	$L__BB0_80;

$L__BB0_81:
	add.s32 	%r851, %r78, %r31;
	setp.lt.u32 	%p168, %r851, 112;
	setp.lt.u32 	%p169, %r1191, 363;
	and.pred  	%p225, %p169, %p168;
	bra.uni 	$L__BB0_82;

$L__BB0_80:
	mov.pred 	%p225, 0;

$L__BB0_82:
	add.s32 	%r852, %r78, %r31;
	setp.lt.u32 	%p170, %r852, 112;
	and.pred  	%p172, %p170, %p166;
	and.pred  	%p174, %p172, %p133;
	and.pred  	%p175, %p225, %p174;
	@%p175 bra 	$L__BB0_84;
	bra.uni 	$L__BB0_83;

$L__BB0_84:
	add.s32 	%r853, %r1188, %r77;
	mul.wide.s32 	%rd41, %r853, 2;
	add.s64 	%rd42, %rd2, %rd41;
	ld.global.nc.u16 	%rs190, [%rd42];
	bra.uni 	$L__BB0_85;

$L__BB0_83:
	mov.f32 	%f110, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs190, %f110;}

	// end inline asm

$L__BB0_85:
	st.shared.u16 	[%r613+3328], %rs190;
	add.s32 	%r872, %r32, %r79;
	add.s32 	%r94, %r872, -1;
	setp.lt.u32 	%p176, %r94, 112;
	@%p176 bra 	$L__BB0_87;
	bra.uni 	$L__BB0_86;

$L__BB0_87:
	add.s32 	%r873, %r78, %r33;
	setp.lt.u32 	%p178, %r873, 112;
	setp.lt.u32 	%p179, %r1191, 363;
	and.pred  	%p226, %p179, %p178;
	bra.uni 	$L__BB0_88;

$L__BB0_86:
	mov.pred 	%p226, 0;

$L__BB0_88:
	add.s32 	%r874, %r78, %r33;
	setp.lt.u32 	%p180, %r874, 112;
	and.pred  	%p182, %p180, %p176;
	and.pred  	%p184, %p182, %p133;
	and.pred  	%p185, %p226, %p184;
	@%p185 bra 	$L__BB0_90;
	bra.uni 	$L__BB0_89;

$L__BB0_90:
	add.s32 	%r875, %r1189, %r77;
	mul.wide.s32 	%rd43, %r875, 2;
	add.s64 	%rd44, %rd2, %rd43;
	ld.global.nc.u16 	%rs191, [%rd44];
	bra.uni 	$L__BB0_91;

$L__BB0_89:
	mov.f32 	%f111, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs191, %f111;}

	// end inline asm

$L__BB0_91:
	st.shared.u16 	[%r591+3584], %rs191;
	add.s32 	%r892, %r34, %r79;
	add.s32 	%r96, %r892, -1;
	setp.lt.u32 	%p186, %r96, 112;
	@%p186 bra 	$L__BB0_93;
	bra.uni 	$L__BB0_92;

$L__BB0_93:
	add.s32 	%r893, %r78, %r35;
	setp.lt.u32 	%p188, %r893, 112;
	setp.lt.u32 	%p189, %r1191, 363;
	and.pred  	%p227, %p189, %p188;
	bra.uni 	$L__BB0_94;

$L__BB0_92:
	mov.pred 	%p227, 0;

$L__BB0_94:
	add.s32 	%r894, %r78, %r35;
	setp.lt.u32 	%p190, %r894, 112;
	and.pred  	%p192, %p190, %p186;
	and.pred  	%p194, %p192, %p133;
	and.pred  	%p195, %p227, %p194;
	@%p195 bra 	$L__BB0_96;
	bra.uni 	$L__BB0_95;

$L__BB0_96:
	add.s32 	%r895, %r1190, %r77;
	mul.wide.s32 	%rd45, %r895, 2;
	add.s64 	%rd46, %rd2, %rd45;
	ld.global.nc.u16 	%rs192, [%rd46];
	bra.uni 	$L__BB0_97;

$L__BB0_95:
	mov.f32 	%f112, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs192, %f112;}

	// end inline asm

$L__BB0_97:
	and.b32  	%r897, %r145, 4;
	shr.u32 	%r898, %r897, 2;
	mov.u32 	%r899, %ctaid.x;
	shl.b32 	%r900, %r899, 1;
	or.b32  	%r97, %r898, %r900;
	setp.lt.s32 	%p196, %r97, 3;
	st.shared.u16 	[%r613+3840], %rs192;
	setp.lt.s32 	%p197, %r1192, 363;
	and.pred  	%p198, %p196, %p197;
	shl.b32 	%r917, %r145, 3;
	and.b32  	%r918, %r917, 56;
	shr.s32 	%r919, %r145, 3;
	mad.lo.s32 	%r920, %r919, 96, %r918;
	shl.b32 	%r921, %r899, 6;
	add.s32 	%r922, %r920, %r921;
	mul.wide.s32 	%rd47, %r922, 2;
	add.s64 	%rd4, %rd51, %rd47;
	@%p198 bra 	$L__BB0_99;
	bra.uni 	$L__BB0_98;

$L__BB0_99:
	ld.global.nc.v4.u32 	{%r1194, %r1195, %r1196, %r1197}, [%rd4];
	bra.uni 	$L__BB0_100;

$L__BB0_98:
	mov.f32 	%f120, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs67, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f120;}

	// end inline asm
	mov.b32 	%r1197, {%rs71, %rs72};
	mov.b32 	%r1196, {%rs69, %rs70};
	mov.b32 	%r1195, {%rs67, %rs68};
	mov.b32 	%r1194, {%rs65, %rs66};

$L__BB0_100:
	and.b32  	%r1163, %r145, 4;
	shr.u32 	%r1162, %r1163, 2;
	mov.u32 	%r1161, %ctaid.x;
	shl.b32 	%r1160, %r1161, 1;
	or.b32  	%r1159, %r1162, %r1160;
	setp.lt.s32 	%p211, %r1159, 3;
	shl.b32 	%r1158, %r145, 3;
	and.b32  	%r927, %r145, 32;
	shr.u32 	%r928, %r927, 5;
	add.s32 	%r931, %r928, %r1162;
	shl.b32 	%r932, %r931, 5;
	and.b32  	%r933, %r932, 32;
	and.b32  	%r935, %r1158, 2147483584;
	or.b32  	%r936, %r933, %r935;
	and.b32  	%r937, %r145, 2;
	shr.u32 	%r938, %r937, 1;
	shr.u32 	%r939, %r145, 4;
	add.s32 	%r940, %r938, %r939;
	shl.b32 	%r941, %r940, 4;
	and.b32  	%r942, %r941, 16;
	or.b32  	%r943, %r936, %r942;
	add.s32 	%r946, %r581, %r145;
	shl.b32 	%r947, %r946, 3;
	and.b32  	%r948, %r947, 8;
	or.b32  	%r949, %r943, %r948;
	shl.b32 	%r950, %r949, 1;
	add.s32 	%r110, %r515, %r950;
	st.shared.v4.u32 	[%r110], {%r1194, %r1195, %r1196, %r1197};
	add.s32 	%r952, %r1192, 16;
	setp.lt.s32 	%p199, %r952, 363;
	and.pred  	%p201, %p211, %p199;
	@%p201 bra 	$L__BB0_102;
	bra.uni 	$L__BB0_101;

$L__BB0_102:
	ld.global.nc.v4.u32 	{%r1198, %r1199, %r1200, %r1201}, [%rd4+3072];
	bra.uni 	$L__BB0_103;

$L__BB0_101:
	mov.f32 	%f128, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs76, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs77, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f128;}

	// end inline asm
	mov.b32 	%r1201, {%rs79, %rs80};
	mov.b32 	%r1200, {%rs77, %rs78};
	mov.b32 	%r1199, {%rs75, %rs76};
	mov.b32 	%r1198, {%rs73, %rs74};

$L__BB0_103:
	add.s32 	%r1039, %r37, 2048;
	add.s32 	%r966, %r36, 2048;
	st.shared.v4.u32 	[%r110+2048], {%r1198, %r1199, %r1200, %r1201};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r957, %r958, %r959, %r960}, [%r36];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r962, %r963, %r964, %r965}, [%r966];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r967, %r968, %r969, %r970}, [%r37];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r972, %r973, %r974, %r975}, [%r38];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r957,  %r958,  %r959,  %r960},{%r967,  %r968},{%f384, %f383, %f382, %f381};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r962,  %r963,  %r964,  %r965},{%r967,  %r968},{%f380, %f379, %f378, %f377};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f145,  %f146,  %f147,  %f148},{%r962,  %r963,  %r964,  %r965},{%r969,  %r970},{%f372, %f371, %f370, %f369};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f153,  %f154,  %f155,  %f156},{%r957,  %r958,  %r959,  %r960},{%r969,  %r970},{%f376, %f375, %f374, %f373};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f161,  %f162,  %f163,  %f164},{%r957,  %r958,  %r959,  %r960},{%r972,  %r973},{%f368, %f367, %f366, %f365};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f169,  %f170,  %f171,  %f172},{%r962,  %r963,  %r964,  %r965},{%r972,  %r973},{%f364, %f363, %f362, %f361};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f177,  %f178,  %f179,  %f180},{%r962,  %r963,  %r964,  %r965},{%r974,  %r975},{%f356, %f355, %f354, %f353};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f185,  %f186,  %f187,  %f188},{%r957,  %r958,  %r959,  %r960},{%r974,  %r975},{%f360, %f359, %f358, %f357};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1025, %r1026, %r1027, %r1028}, [%r39];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1030, %r1031, %r1032, %r1033}, [%r40];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1035, %r1036, %r1037, %r1038}, [%r1039];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1040, %r1041, %r1042, %r1043}, [%r41];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f384,  %f383,  %f382,  %f381},{%r1025,  %r1026,  %r1027,  %r1028},{%r1035,  %r1036},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f380,  %f379,  %f378,  %f377},{%r1030,  %r1031,  %r1032,  %r1033},{%r1035,  %r1036},{%f137, %f138, %f139, %f140};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f372,  %f371,  %f370,  %f369},{%r1030,  %r1031,  %r1032,  %r1033},{%r1037,  %r1038},{%f145, %f146, %f147, %f148};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f376,  %f375,  %f374,  %f373},{%r1025,  %r1026,  %r1027,  %r1028},{%r1037,  %r1038},{%f153, %f154, %f155, %f156};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f368,  %f367,  %f366,  %f365},{%r1025,  %r1026,  %r1027,  %r1028},{%r1040,  %r1041},{%f161, %f162, %f163, %f164};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f364,  %f363,  %f362,  %f361},{%r1030,  %r1031,  %r1032,  %r1033},{%r1040,  %r1041},{%f169, %f170, %f171, %f172};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f356,  %f355,  %f354,  %f353},{%r1030,  %r1031,  %r1032,  %r1033},{%r1042,  %r1043},{%f177, %f178, %f179, %f180};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f360,  %f359,  %f358,  %f357},{%r1025,  %r1026,  %r1027,  %r1028},{%r1042,  %r1043},{%f185, %f186, %f187, %f188};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd51, %rd51, 6144;
	add.s32 	%r1192, %r1192, 32;
	add.s32 	%r1191, %r1191, 32;
	add.s32 	%r1190, %r1190, 32;
	add.s32 	%r1189, %r1189, 32;
	add.s32 	%r1188, %r1188, 32;
	add.s32 	%r1187, %r1187, 32;
	add.s32 	%r1186, %r1186, 32;
	add.s32 	%r1185, %r1185, 32;
	add.s32 	%r1184, %r1184, 32;
	add.s32 	%r1183, %r1183, 32;
	add.s32 	%r1182, %r1182, 32;
	add.s32 	%r1181, %r1181, 32;
	add.s32 	%r1180, %r1180, 32;
	add.s32 	%r1179, %r1179, 32;
	add.s32 	%r1178, %r1178, 32;
	add.s32 	%r1177, %r1177, 32;
	add.s32 	%r1176, %r1176, 32;
	add.s32 	%r1175, %r1175, 32;
	add.s32 	%r1193, %r1193, 32;
	setp.ne.s32 	%p202, %r1193, 400;
	@%p202 bra 	$L__BB0_1;

	mov.u32 	%r1174, %ctaid.x;
	ld.param.u64 	%rd50, [main_kernel_param_1];
	and.b32  	%r1173, %r145, 4;
	shl.b32 	%r1172, %r1174, 1;
	shr.u32 	%r1171, %r1173, 2;
	or.b32  	%r1170, %r1171, %r1172;
	shl.b32 	%r1169, %r145, 3;
	and.b32  	%r1168, %r1169, 56;
	shl.b32 	%r1167, %r1174, 6;
	shr.s32 	%r1166, %r145, 3;
	shr.s32 	%r1165, %r145, 5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f384;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f258, %rs81;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f383;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f261, %rs84;}

	// end inline asm
	shl.b32 	%r1099, %r145, 5;
	and.b32  	%r1100, %r1099, 1024;
	shl.b32 	%r1101, %r145, 4;
	and.b32  	%r1102, %r1101, 448;
	shr.s32 	%r1103, %r145, 6;
	shl.b32 	%r1104, %r1103, 3;
	shl.b32 	%r1105, %r145, 1;
	and.b32  	%r1106, %r1105, 6;
	add.s32 	%r1107, %r1104, %r1100;
	or.b32  	%r1108, %r1107, %r1106;
	add.s32 	%r1109, %r1108, %r1102;
	shl.b32 	%r1110, %r1109, 1;
	add.s32 	%r1112, %r515, %r1110;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs86, %f261;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs83, %f258;}

	// end inline asm
	st.shared.v2.u16 	[%r1112], {%rs83, %rs86};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f382;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f264, %rs87;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f381;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f267, %rs90;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs92, %f267;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f264;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+1024], {%rs89, %rs92};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f380;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f270, %rs93;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs96, %f379;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f273, %rs96;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs98, %f273;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs95, %f270;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+4096], {%rs95, %rs98};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs99, %f378;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f276, %rs99;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f377;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f279, %rs102;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs104, %f279;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs101, %f276;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+5120], {%rs101, %rs104};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs105, %f376;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f282, %rs105;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f375;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f285, %rs108;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f285;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs107, %f282;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+32], {%rs107, %rs110};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs111, %f374;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f288, %rs111;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f373;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f291, %rs114;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs116, %f291;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs113, %f288;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+1056], {%rs113, %rs116};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f372;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f294, %rs117;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f371;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f297, %rs120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs122, %f297;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs119, %f294;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+4128], {%rs119, %rs122};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs123, %f370;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f300, %rs123;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f369;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f303, %rs126;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs128, %f303;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs125, %f300;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+5152], {%rs125, %rs128};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs129, %f368;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f306, %rs129;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs132, %f367;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f309, %rs132;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs134, %f309;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs131, %f306;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+64], {%rs131, %rs134};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs135, %f366;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f312, %rs135;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs138, %f365;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f315, %rs138;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs140, %f315;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs137, %f312;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+1088], {%rs137, %rs140};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs141, %f364;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f318, %rs141;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs144, %f363;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f321, %rs144;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs146, %f321;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs143, %f318;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+4160], {%rs143, %rs146};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs147, %f362;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f324, %rs147;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs150, %f361;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f327, %rs150;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs152, %f327;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs149, %f324;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+5184], {%rs149, %rs152};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs153, %f360;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f330, %rs153;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs156, %f359;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f333, %rs156;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs158, %f333;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs155, %f330;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+96], {%rs155, %rs158};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs159, %f358;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f336, %rs159;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs162, %f357;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f339, %rs162;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs164, %f339;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs161, %f336;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+1120], {%rs161, %rs164};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs165, %f356;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f342, %rs165;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs168, %f355;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f345, %rs168;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs170, %f345;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs167, %f342;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+4192], {%rs167, %rs170};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs171, %f354;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f348, %rs171;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs174, %f353;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f351, %rs174;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs176, %f351;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs173, %f348;}

	// end inline asm
	st.shared.v2.u16 	[%r1112+5216], {%rs173, %rs176};
	bar.sync 	0;
	shl.b32 	%r1114, %r2, 4;
	add.s32 	%r142, %r1165, %r1114;
	mad.lo.s32 	%r1117, %r1166, 96, %r1167;
	add.s32 	%r1120, %r1117, %r1168;
	mad.lo.s32 	%r1121, %r2, 6144, %r1120;
	setp.gt.s32 	%p203, %r142, 75624;
	setp.gt.s32 	%p33, %r1170, 2;
	or.pred  	%p204, %p203, %p33;
	add.s32 	%r143, %r515, %r1101;
	cvta.to.global.u64 	%rd48, %rd50;
	mul.wide.s32 	%rd49, %r1121, 2;
	add.s64 	%rd6, %rd48, %rd49;
	@%p204 bra 	$L__BB0_106;

	ld.shared.v4.u32 	{%r1122, %r1123, %r1124, %r1125}, [%r143];
	st.global.v4.u32 	[%rd6], {%r1122, %r1123, %r1124, %r1125};

$L__BB0_106:
	add.s32 	%r1130, %r142, 4;
	setp.gt.s32 	%p205, %r1130, 75624;
	or.pred  	%p206, %p205, %p33;
	@%p206 bra 	$L__BB0_108;

	ld.shared.v4.u32 	{%r1131, %r1132, %r1133, %r1134}, [%r143+2048];
	st.global.v4.u32 	[%rd6+3072], {%r1131, %r1132, %r1133, %r1134};

$L__BB0_108:
	add.s32 	%r1139, %r142, 8;
	setp.gt.s32 	%p207, %r1139, 75624;
	or.pred  	%p208, %p207, %p33;
	@%p208 bra 	$L__BB0_110;

	ld.shared.v4.u32 	{%r1140, %r1141, %r1142, %r1143}, [%r143+4096];
	st.global.v4.u32 	[%rd6+6144], {%r1140, %r1141, %r1142, %r1143};

$L__BB0_110:
	add.s32 	%r1148, %r142, 12;
	setp.gt.s32 	%p209, %r1148, 75624;
	or.pred  	%p210, %p209, %p33;
	@%p210 bra 	$L__BB0_112;

	ld.shared.v4.u32 	{%r1149, %r1150, %r1151, %r1152}, [%r143+6144];
	st.global.v4.u32 	[%rd6+9216], {%r1149, %r1150, %r1151, %r1152};

$L__BB0_112:
	ret;

}

