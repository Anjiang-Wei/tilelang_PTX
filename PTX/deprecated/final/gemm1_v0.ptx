//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN65_INTERNAL_c4fd94b9_34_gemm_BM32_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<13>;
	.reg .f32 	%f<93>;
	.reg .b32 	%r<270>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd9, [main_kernel_param_0];
	ld.param.u64 	%rd10, [main_kernel_param_1];
	ld.param.u64 	%rd11, [main_kernel_param_2];
	cvta.to.global.u64 	%rd12, %rd10;
	cvta.to.global.u64 	%rd20, %rd9;
	cvta.to.global.u64 	%rd2, %rd11;
	mov.u32 	%r1, %tid.x;
	shr.s32 	%r13, %r1, 3;
	shl.b32 	%r14, %r13, 5;
	and.b32  	%r2, %r1, 32;
	shr.u32 	%r15, %r2, 5;
	and.b32  	%r16, %r1, 4;
	shr.u32 	%r17, %r16, 2;
	add.s32 	%r18, %r15, %r17;
	shl.b32 	%r19, %r18, 4;
	and.b32  	%r20, %r19, 16;
	shr.u32 	%r21, %r1, 4;
	and.b32  	%r22, %r1, 2;
	shr.u32 	%r23, %r22, 1;
	add.s32 	%r24, %r23, %r21;
	shl.b32 	%r25, %r24, 3;
	and.b32  	%r26, %r25, 8;
	shl.b32 	%r27, %r1, 2;
	and.b32  	%r28, %r27, 4;
	or.b32  	%r29, %r28, %r14;
	or.b32  	%r30, %r29, %r20;
	or.b32  	%r31, %r30, %r26;
	shl.b32 	%r32, %r31, 1;
	mov.u32 	%r33, buf_dyn_shmem;
	add.s32 	%r3, %r33, %r32;
	mov.u32 	%r34, %ctaid.y;
	shl.b32 	%r4, %r34, 16;
	and.b32  	%r35, %r27, 28;
	shr.s32 	%r36, %r1, 31;
	shr.u32 	%r37, %r36, 28;
	add.s32 	%r38, %r1, %r37;
	and.b32  	%r39, %r38, -16;
	sub.s32 	%r40, %r1, %r39;
	shr.u32 	%r41, %r40, 31;
	add.s32 	%r42, %r40, %r41;
	shr.s32 	%r43, %r42, 1;
	shr.s32 	%r44, %r42, 31;
	shr.u32 	%r45, %r44, 30;
	add.s32 	%r46, %r43, %r45;
	and.b32  	%r47, %r46, -4;
	sub.s32 	%r48, %r43, %r47;
	shr.s32 	%r49, %r38, 4;
	shl.b32 	%r50, %r48, 6;
	and.b32  	%r51, %r50, 192;
	shl.b32 	%r52, %r49, 3;
	and.b32  	%r53, %r52, 8;
	or.b32  	%r54, %r51, %r53;
	and.b32  	%r55, %r42, 134217726;
	sub.s32 	%r56, %r40, %r55;
	shl.b32 	%r57, %r56, 5;
	shr.s32 	%r58, %r40, 31;
	shr.u32 	%r59, %r58, 29;
	add.s32 	%r60, %r40, %r59;
	shl.b32 	%r61, %r60, 5;
	and.b32  	%r62, %r61, 2147483392;
	add.s32 	%r63, %r57, %r62;
	shr.u32 	%r64, %r36, 27;
	add.s32 	%r65, %r1, %r64;
	shr.u32 	%r66, %r65, 31;
	shr.s32 	%r67, %r65, 5;
	add.s32 	%r68, %r67, %r66;
	and.b32  	%r69, %r68, 4194302;
	sub.s32 	%r70, %r67, %r69;
	shl.b32 	%r71, %r70, 9;
	add.s32 	%r72, %r63, %r71;
	and.b32  	%r73, %r48, 2;
	setp.eq.s32 	%p1, %r73, 0;
	shr.u32 	%r74, %r51, 3;
	xor.b32  	%r75, %r54, %r74;
	add.s32 	%r76, %r72, %r75;
	shr.u32 	%r77, %r36, 29;
	add.s32 	%r78, %r1, %r77;
	and.b32  	%r79, %r78, -8;
	sub.s32 	%r80, %r1, %r79;
	shr.u32 	%r81, %r80, 31;
	add.s32 	%r82, %r80, %r81;
	shr.s32 	%r83, %r82, 1;
	mov.u32 	%r269, 0;
	shl.b32 	%r84, %r83, 6;
	and.b32  	%r85, %r84, 192;
	and.b32  	%r86, %r78, 8;
	or.b32  	%r87, %r85, %r86;
	and.b32  	%r88, %r82, 67108862;
	sub.s32 	%r89, %r80, %r88;
	shl.b32 	%r90, %r89, 5;
	shr.u32 	%r91, %r36, 26;
	add.s32 	%r92, %r1, %r91;
	shl.b32 	%r93, %r92, 2;
	and.b32  	%r94, %r93, 2147483392;
	add.s32 	%r95, %r90, %r94;
	and.b32  	%r96, %r83, 2;
	setp.eq.s32 	%p2, %r96, 0;
	shr.u32 	%r97, %r85, 3;
	xor.b32  	%r98, %r87, %r97;
	add.s32 	%r99, %r95, %r98;
	shl.b32 	%r100, %r76, 1;
	add.s32 	%r5, %r33, %r100;
	shl.b32 	%r101, %r99, 1;
	add.s32 	%r102, %r33, %r101;
	add.s32 	%r6, %r102, 2048;
	selp.b32 	%r103, 32, -32, %p1;
	add.s32 	%r7, %r5, %r103;
	selp.b32 	%r104, 32, -32, %p2;
	add.s32 	%r8, %r6, %r104;
	or.b32  	%r105, %r35, %r4;
	shl.b32 	%r106, %r13, 11;
	add.s32 	%r107, %r105, %r106;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r108, %r9, 16;
	or.b32  	%r109, %r35, %r108;
	add.s32 	%r110, %r109, %r106;
	mul.wide.s32 	%rd13, %r110, 2;
	add.s64 	%rd19, %rd12, %rd13;
	mul.wide.s32 	%rd4, %r107, 2;
	mov.f32 	%f89, 0f00000000;
	mov.f32 	%f90, %f89;
	mov.f32 	%f91, %f89;
	mov.f32 	%f92, %f89;

$L__BB0_1:
	bar.sync 	0;
	add.s64 	%rd14, %rd20, %rd4;
	ld.global.nc.v2.u32 	{%r223, %r224}, [%rd14];
	st.shared.v2.u32 	[%r3], {%r223, %r224};
	ld.global.nc.v2.u32 	{%r227, %r228}, [%rd19];
	st.shared.v2.u32 	[%r3+2048], {%r227, %r228};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r111, %r112, %r113, %r114}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r116, %r117}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f13,  %f14,  %f15,  %f16},{%r111,  %r112,  %r113,  %r114},{%r116,  %r117},{%f92, %f91, %f90, %f89};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r125, %r126, %r127, %r128}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r130, %r131}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f21,  %f22,  %f23,  %f24},{%r125,  %r126,  %r127,  %r128},{%r130,  %r131},{%f13, %f14, %f15, %f16};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r231, %r232}, [%rd14+64];
	st.shared.v2.u32 	[%r3], {%r231, %r232};
	ld.global.nc.v2.u32 	{%r235, %r236}, [%rd19+64];
	st.shared.v2.u32 	[%r3+2048], {%r235, %r236};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r139, %r140, %r141, %r142}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r144, %r145}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f29,  %f30,  %f31,  %f32},{%r139,  %r140,  %r141,  %r142},{%r144,  %r145},{%f21, %f22, %f23, %f24};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r153, %r154, %r155, %r156}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r158, %r159}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f37,  %f38,  %f39,  %f40},{%r153,  %r154,  %r155,  %r156},{%r158,  %r159},{%f29, %f30, %f31, %f32};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r239, %r240}, [%rd14+128];
	st.shared.v2.u32 	[%r3], {%r239, %r240};
	ld.global.nc.v2.u32 	{%r243, %r244}, [%rd19+128];
	st.shared.v2.u32 	[%r3+2048], {%r243, %r244};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r167, %r168, %r169, %r170}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r172, %r173}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f45,  %f46,  %f47,  %f48},{%r167,  %r168,  %r169,  %r170},{%r172,  %r173},{%f37, %f38, %f39, %f40};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r181, %r182, %r183, %r184}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r186, %r187}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f53,  %f54,  %f55,  %f56},{%r181,  %r182,  %r183,  %r184},{%r186,  %r187},{%f45, %f46, %f47, %f48};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r247, %r248}, [%rd14+192];
	st.shared.v2.u32 	[%r3], {%r247, %r248};
	ld.global.nc.v2.u32 	{%r251, %r252}, [%rd19+192];
	st.shared.v2.u32 	[%r3+2048], {%r251, %r252};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r195, %r196, %r197, %r198}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r200, %r201}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f61,  %f62,  %f63,  %f64},{%r195,  %r196,  %r197,  %r198},{%r200,  %r201},{%f53, %f54, %f55, %f56};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r209, %r210, %r211, %r212}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r214, %r215}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f92,  %f91,  %f90,  %f89},{%r209,  %r210,  %r211,  %r212},{%r214,  %r215},{%f61, %f62, %f63, %f64};

	// end inline asm
	add.s64 	%rd20, %rd20, 256;
	add.s64 	%rd19, %rd19, 256;
	add.s32 	%r269, %r269, 4;
	setp.ne.s32 	%p3, %r269, 64;
	@%p3 bra 	$L__BB0_1;

	shl.b32 	%r255, %r2, 10;
	shl.b32 	%r256, %r1, 9;
	and.b32  	%r257, %r256, 14336;
	shr.s32 	%r258, %r1, 6;
	shl.b32 	%r259, %r258, 3;
	shl.b32 	%r260, %r1, 1;
	and.b32  	%r261, %r260, 6;
	shl.b32 	%r262, %r9, 5;
	add.s32 	%r263, %r262, %r4;
	add.s32 	%r264, %r263, %r255;
	add.s32 	%r265, %r264, %r259;
	add.s32 	%r266, %r265, %r261;
	add.s32 	%r267, %r266, %r257;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f92;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f78, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f91;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f81, %rs4;}

	// end inline asm
	mul.wide.s32 	%rd15, %r267, 2;
	add.s64 	%rd16, %rd2, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f81;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f78;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f90;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f84, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f89;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f87, %rs10;}

	// end inline asm
	add.s32 	%r268, %r267, 16384;
	mul.wide.s32 	%rd17, %r268, 2;
	add.s64 	%rd18, %rd2, %rd17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f87;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f84;}

	// end inline asm
	st.global.v2.u16 	[%rd18], {%rs9, %rs12};
	ret;

}

