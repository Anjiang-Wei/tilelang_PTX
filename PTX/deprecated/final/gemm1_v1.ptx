//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN65_INTERNAL_f4ee390c_34_gemm_BM64_BN32_BK32_S0_T256_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 256, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<185>;
	.reg .b32 	%r<399>;
	.reg .b64 	%rd<21>;


	ld.param.u64 	%rd9, [main_kernel_param_0];
	ld.param.u64 	%rd10, [main_kernel_param_1];
	ld.param.u64 	%rd11, [main_kernel_param_2];
	cvta.to.global.u64 	%rd20, %rd9;
	cvta.to.global.u64 	%rd2, %rd11;
	mov.u32 	%r1, %tid.x;
	shr.s32 	%r17, %r1, 2;
	shl.b32 	%r18, %r17, 5;
	shr.u32 	%r19, %r1, 4;
	and.b32  	%r20, %r1, 2;
	shr.u32 	%r21, %r20, 1;
	add.s32 	%r22, %r21, %r19;
	shl.b32 	%r23, %r22, 4;
	and.b32  	%r24, %r23, 16;
	or.b32  	%r25, %r24, %r18;
	and.b32  	%r26, %r1, 8;
	shr.u32 	%r27, %r26, 3;
	add.s32 	%r28, %r27, %r1;
	shl.b32 	%r29, %r28, 3;
	and.b32  	%r30, %r29, 8;
	or.b32  	%r31, %r25, %r30;
	shl.b32 	%r32, %r31, 1;
	mov.u32 	%r33, buf_dyn_shmem;
	add.s32 	%r2, %r33, %r32;
	mov.u32 	%r34, %ctaid.y;
	shl.b32 	%r3, %r34, 17;
	and.b32  	%r4, %r1, 3;
	shl.b32 	%r35, %r1, 3;
	and.b32  	%r36, %r35, 24;
	shr.s32 	%r37, %r1, 3;
	shl.b32 	%r38, %r37, 5;
	and.b32  	%r5, %r1, 32;
	shr.u32 	%r39, %r5, 5;
	and.b32  	%r40, %r1, 4;
	shr.u32 	%r41, %r40, 2;
	add.s32 	%r42, %r39, %r41;
	shl.b32 	%r43, %r42, 4;
	and.b32  	%r44, %r43, 16;
	shl.b32 	%r45, %r22, 3;
	and.b32  	%r46, %r45, 8;
	shl.b32 	%r47, %r1, 2;
	and.b32  	%r48, %r47, 4;
	or.b32  	%r49, %r46, %r44;
	or.b32  	%r50, %r49, %r48;
	or.b32  	%r51, %r50, %r38;
	shl.b32 	%r52, %r51, 1;
	add.s32 	%r53, %r33, %r52;
	and.b32  	%r54, %r47, 28;
	shr.s32 	%r55, %r1, 31;
	shr.u32 	%r56, %r55, 28;
	add.s32 	%r57, %r1, %r56;
	and.b32  	%r58, %r57, -16;
	sub.s32 	%r59, %r1, %r58;
	shr.u32 	%r60, %r59, 31;
	add.s32 	%r61, %r59, %r60;
	shr.s32 	%r62, %r61, 1;
	shr.s32 	%r63, %r61, 31;
	shr.u32 	%r64, %r63, 30;
	add.s32 	%r65, %r62, %r64;
	and.b32  	%r66, %r65, -4;
	sub.s32 	%r67, %r62, %r66;
	shr.s32 	%r68, %r57, 4;
	shl.b32 	%r69, %r67, 6;
	and.b32  	%r70, %r69, 192;
	shl.b32 	%r71, %r68, 3;
	and.b32  	%r72, %r71, 8;
	or.b32  	%r73, %r70, %r72;
	and.b32  	%r74, %r61, 134217726;
	sub.s32 	%r75, %r59, %r74;
	shl.b32 	%r76, %r75, 5;
	shr.s32 	%r77, %r59, 31;
	shr.u32 	%r78, %r77, 29;
	add.s32 	%r79, %r59, %r78;
	shl.b32 	%r80, %r79, 5;
	and.b32  	%r81, %r80, 2147483392;
	add.s32 	%r82, %r76, %r81;
	shr.u32 	%r83, %r55, 27;
	add.s32 	%r84, %r1, %r83;
	shr.u32 	%r85, %r84, 31;
	shr.s32 	%r86, %r84, 5;
	add.s32 	%r87, %r86, %r85;
	and.b32  	%r88, %r87, 4194302;
	sub.s32 	%r89, %r86, %r88;
	shl.b32 	%r90, %r89, 9;
	add.s32 	%r91, %r82, %r90;
	and.b32  	%r92, %r67, 2;
	setp.eq.s32 	%p1, %r92, 0;
	shr.u32 	%r93, %r70, 3;
	xor.b32  	%r94, %r73, %r93;
	add.s32 	%r95, %r91, %r94;
	shr.u32 	%r96, %r55, 29;
	add.s32 	%r97, %r1, %r96;
	and.b32  	%r98, %r97, -8;
	sub.s32 	%r99, %r1, %r98;
	shr.u32 	%r100, %r99, 31;
	add.s32 	%r101, %r99, %r100;
	shr.s32 	%r102, %r101, 1;
	mov.u32 	%r398, 0;
	shl.b32 	%r103, %r102, 6;
	and.b32  	%r104, %r103, 192;
	and.b32  	%r105, %r97, 8;
	or.b32  	%r106, %r104, %r105;
	and.b32  	%r107, %r101, 67108862;
	sub.s32 	%r108, %r99, %r107;
	shl.b32 	%r109, %r108, 5;
	shr.u32 	%r110, %r55, 26;
	add.s32 	%r111, %r1, %r110;
	shl.b32 	%r112, %r111, 2;
	and.b32  	%r113, %r112, 2147483392;
	add.s32 	%r114, %r109, %r113;
	and.b32  	%r115, %r102, 2;
	setp.eq.s32 	%p2, %r115, 0;
	shr.u32 	%r116, %r104, 3;
	xor.b32  	%r117, %r106, %r116;
	add.s32 	%r118, %r114, %r117;
	shl.b32 	%r119, %r95, 1;
	add.s32 	%r7, %r33, %r119;
	add.s32 	%r8, %r7, 2048;
	shl.b32 	%r120, %r118, 1;
	add.s32 	%r121, %r33, %r120;
	add.s32 	%r9, %r121, 4096;
	selp.b32 	%r122, 32, -32, %p1;
	add.s32 	%r10, %r7, %r122;
	add.s32 	%r11, %r10, 2048;
	selp.b32 	%r123, 32, -32, %p2;
	add.s32 	%r12, %r9, %r123;
	or.b32  	%r124, %r36, %r3;
	shl.b32 	%r125, %r17, 11;
	add.s32 	%r126, %r124, %r125;
	mov.u32 	%r13, %ctaid.x;
	shl.b32 	%r127, %r13, 16;
	or.b32  	%r128, %r54, %r127;
	shl.b32 	%r129, %r37, 11;
	add.s32 	%r130, %r128, %r129;
	cvta.to.global.u64 	%rd12, %rd10;
	mul.wide.s32 	%rd13, %r130, 2;
	add.s64 	%rd19, %rd12, %rd13;
	mul.wide.s32 	%rd4, %r126, 2;
	mov.f32 	%f177, 0f00000000;
	mov.f32 	%f178, %f177;
	mov.f32 	%f179, %f177;
	mov.f32 	%f180, %f177;
	mov.f32 	%f181, %f177;
	mov.f32 	%f182, %f177;
	mov.f32 	%f183, %f177;
	mov.f32 	%f184, %f177;

$L__BB0_1:
	bar.sync 	0;
	add.s64 	%rd14, %rd20, %rd4;
	ld.global.nc.v4.u32 	{%r331, %r332, %r333, %r334}, [%rd14];
	st.shared.v4.u32 	[%r2], {%r331, %r332, %r333, %r334};
	ld.global.nc.v2.u32 	{%r339, %r340}, [%rd19];
	add.s32 	%r392, %r53, 4096;
	st.shared.v2.u32 	[%r392], {%r339, %r340};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r131, %r132, %r133, %r134}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r136, %r137, %r138, %r139}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r141, %r142}, [%r9];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f25,  %f26,  %f27,  %f28},{%r131,  %r132,  %r133,  %r134},{%r141,  %r142},{%f184, %f183, %f182, %f181};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f33,  %f34,  %f35,  %f36},{%r136,  %r137,  %r138,  %r139},{%r141,  %r142},{%f180, %f179, %f178, %f177};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r156, %r157, %r158, %r159}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r161, %r162, %r163, %r164}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r166, %r167}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f41,  %f42,  %f43,  %f44},{%r156,  %r157,  %r158,  %r159},{%r166,  %r167},{%f25, %f26, %f27, %f28};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r161,  %r162,  %r163,  %r164},{%r166,  %r167},{%f33, %f34, %f35, %f36};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r343, %r344, %r345, %r346}, [%rd14+64];
	st.shared.v4.u32 	[%r2], {%r343, %r344, %r345, %r346};
	ld.global.nc.v2.u32 	{%r351, %r352}, [%rd19+64];
	add.s32 	%r393, %r53, 4096;
	st.shared.v2.u32 	[%r393], {%r351, %r352};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r181, %r182, %r183, %r184}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r186, %r187, %r188, %r189}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r191, %r192}, [%r9];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r181,  %r182,  %r183,  %r184},{%r191,  %r192},{%f41, %f42, %f43, %f44};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r186,  %r187,  %r188,  %r189},{%r191,  %r192},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r206, %r207, %r208, %r209}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r211, %r212, %r213, %r214}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r216, %r217}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r206,  %r207,  %r208,  %r209},{%r216,  %r217},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r211,  %r212,  %r213,  %r214},{%r216,  %r217},{%f65, %f66, %f67, %f68};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r355, %r356, %r357, %r358}, [%rd14+128];
	st.shared.v4.u32 	[%r2], {%r355, %r356, %r357, %r358};
	ld.global.nc.v2.u32 	{%r363, %r364}, [%rd19+128];
	add.s32 	%r394, %r53, 4096;
	st.shared.v2.u32 	[%r394], {%r363, %r364};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r231, %r232, %r233, %r234}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r236, %r237, %r238, %r239}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r241, %r242}, [%r9];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r231,  %r232,  %r233,  %r234},{%r241,  %r242},{%f73, %f74, %f75, %f76};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r236,  %r237,  %r238,  %r239},{%r241,  %r242},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r256, %r257, %r258, %r259}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r261, %r262, %r263, %r264}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r266, %r267}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r256,  %r257,  %r258,  %r259},{%r266,  %r267},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r261,  %r262,  %r263,  %r264},{%r266,  %r267},{%f97, %f98, %f99, %f100};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r367, %r368, %r369, %r370}, [%rd14+192];
	st.shared.v4.u32 	[%r2], {%r367, %r368, %r369, %r370};
	ld.global.nc.v2.u32 	{%r375, %r376}, [%rd19+192];
	add.s32 	%r395, %r53, 4096;
	st.shared.v2.u32 	[%r395], {%r375, %r376};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r281, %r282, %r283, %r284}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r286, %r287, %r288, %r289}, [%r8];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r291, %r292}, [%r9];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r281,  %r282,  %r283,  %r284},{%r291,  %r292},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r286,  %r287,  %r288,  %r289},{%r291,  %r292},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r306, %r307, %r308, %r309}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r311, %r312, %r313, %r314}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%r316, %r317}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f184,  %f183,  %f182,  %f181},{%r306,  %r307,  %r308,  %r309},{%r316,  %r317},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f180,  %f179,  %f178,  %f177},{%r311,  %r312,  %r313,  %r314},{%r316,  %r317},{%f129, %f130, %f131, %f132};

	// end inline asm
	add.s64 	%rd20, %rd20, 256;
	add.s64 	%rd19, %rd19, 256;
	add.s32 	%r398, %r398, 4;
	setp.ne.s32 	%p3, %r398, 64;
	@%p3 bra 	$L__BB0_1;

	mov.u32 	%r397, %ctaid.x;
	mov.u32 	%r396, %tid.x;
	shl.b32 	%r379, %r5, 10;
	shl.b32 	%r380, %r396, 9;
	and.b32  	%r381, %r380, 14336;
	shr.s32 	%r382, %r396, 6;
	shl.b32 	%r383, %r382, 3;
	shl.b32 	%r384, %r397, 5;
	add.s32 	%r385, %r384, %r3;
	add.s32 	%r386, %r385, %r379;
	add.s32 	%r387, %r386, %r383;
	shl.b32 	%r388, %r4, 1;
	add.s32 	%r389, %r387, %r388;
	add.s32 	%r390, %r389, %r381;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f184;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f154, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f183;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f157, %rs4;}

	// end inline asm
	mul.wide.s32 	%rd15, %r390, 2;
	add.s64 	%rd16, %rd2, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f154;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f182;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f160, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f163, %rs10;}

	// end inline asm
	add.s32 	%r391, %r390, 16384;
	mul.wide.s32 	%rd17, %r391, 2;
	add.s64 	%rd18, %rd2, %rd17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f160;}

	// end inline asm
	st.global.v2.u16 	[%rd18], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f180;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f166, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f179;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f169, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f166;}

	// end inline asm
	st.global.v2.u16 	[%rd18+98304], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f178;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f172, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f175, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f172;}

	// end inline asm
	st.global.v2.u16 	[%rd18+131072], {%rs21, %rs24};
	ret;

}

