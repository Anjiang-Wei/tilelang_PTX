//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN64_INTERNAL_56f8adae_33_gemm_tc_BM32_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<185>;
	.reg .b32 	%r<382>;
	.reg .b64 	%rd<20>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_1];
	cvta.to.global.u64 	%rd19, %rd8;
	mov.u32 	%r8, %tid.x;
	shl.b32 	%r9, %r8, 3;
	and.b32  	%r10, %r9, 24;
	shr.s32 	%r11, %r8, 31;
	shr.u32 	%r12, %r11, 28;
	add.s32 	%r13, %r8, %r12;
	and.b32  	%r14, %r13, -16;
	sub.s32 	%r15, %r8, %r14;
	shr.u32 	%r16, %r15, 31;
	add.s32 	%r17, %r15, %r16;
	shr.s32 	%r18, %r17, 1;
	shr.s32 	%r19, %r17, 31;
	shr.u32 	%r20, %r19, 30;
	add.s32 	%r21, %r18, %r20;
	and.b32  	%r22, %r21, -4;
	sub.s32 	%r23, %r18, %r22;
	shr.u32 	%r24, %r13, 31;
	shr.s32 	%r25, %r13, 4;
	add.s32 	%r26, %r25, %r24;
	and.b32  	%r27, %r26, -2;
	sub.s32 	%r28, %r25, %r27;
	shl.b32 	%r29, %r23, 6;
	and.b32  	%r30, %r29, 192;
	shl.b32 	%r31, %r28, 3;
	and.b32  	%r32, %r31, 8;
	or.b32  	%r33, %r30, %r32;
	and.b32  	%r34, %r17, 134217726;
	sub.s32 	%r35, %r15, %r34;
	shl.b32 	%r36, %r35, 5;
	shr.s32 	%r37, %r15, 31;
	shr.u32 	%r38, %r37, 29;
	add.s32 	%r39, %r15, %r38;
	shl.b32 	%r40, %r39, 5;
	and.b32  	%r41, %r40, 2147483392;
	add.s32 	%r42, %r36, %r41;
	shr.u32 	%r43, %r11, 27;
	add.s32 	%r44, %r8, %r43;
	shr.u32 	%r45, %r44, 31;
	shr.s32 	%r46, %r44, 5;
	add.s32 	%r47, %r46, %r45;
	and.b32  	%r48, %r47, 4194302;
	sub.s32 	%r49, %r46, %r48;
	shl.b32 	%r50, %r49, 9;
	add.s32 	%r51, %r42, %r50;
	and.b32  	%r52, %r23, 2;
	setp.eq.s32 	%p1, %r52, 0;
	shr.u32 	%r53, %r30, 3;
	xor.b32  	%r54, %r33, %r53;
	add.s32 	%r55, %r51, %r54;
	shr.u32 	%r56, %r11, 29;
	add.s32 	%r57, %r8, %r56;
	and.b32  	%r58, %r57, -8;
	sub.s32 	%r59, %r8, %r58;
	shr.u32 	%r60, %r59, 31;
	add.s32 	%r61, %r59, %r60;
	shr.s32 	%r62, %r61, 1;
	mov.u32 	%r381, 0;
	shl.b32 	%r63, %r62, 6;
	and.b32  	%r64, %r63, 192;
	and.b32  	%r65, %r57, 8;
	or.b32  	%r66, %r64, %r65;
	and.b32  	%r67, %r61, 67108862;
	sub.s32 	%r68, %r59, %r67;
	shl.b32 	%r69, %r68, 5;
	shl.b32 	%r70, %r28, 9;
	shr.u32 	%r71, %r11, 26;
	add.s32 	%r72, %r8, %r71;
	shl.b32 	%r73, %r72, 2;
	and.b32  	%r74, %r73, 2147483392;
	add.s32 	%r75, %r70, %r74;
	add.s32 	%r76, %r75, %r69;
	and.b32  	%r77, %r62, 2;
	setp.eq.s32 	%p2, %r77, 0;
	shr.u32 	%r78, %r64, 3;
	xor.b32  	%r79, %r66, %r78;
	add.s32 	%r80, %r76, %r79;
	shl.b32 	%r81, %r55, 1;
	mov.u32 	%r82, buf_dyn_shmem;
	add.s32 	%r1, %r82, %r81;
	shl.b32 	%r83, %r80, 1;
	add.s32 	%r84, %r82, %r83;
	add.s32 	%r2, %r84, 2048;
	selp.b32 	%r85, 32, -32, %p1;
	add.s32 	%r3, %r1, %r85;
	selp.b32 	%r86, 32, -32, %p2;
	add.s32 	%r4, %r2, %r86;
	shl.b32 	%r87, %r8, 8;
	and.b32  	%r88, %r87, -1024;
	mov.u32 	%r89, %ctaid.x;
	shl.b32 	%r90, %r89, 15;
	or.b32  	%r91, %r10, %r90;
	add.s32 	%r92, %r91, %r88;
	cvta.to.global.u64 	%rd10, %rd9;
	mul.wide.s32 	%rd11, %r92, 2;
	add.s64 	%rd18, %rd10, %rd11;
	mov.f32 	%f177, 0f00000000;
	mov.f32 	%f178, %f177;
	mov.f32 	%f179, %f177;
	mov.f32 	%f180, %f177;
	mov.f32 	%f181, %f177;
	mov.f32 	%f182, %f177;
	mov.f32 	%f183, %f177;
	mov.f32 	%f184, %f177;

$L__BB0_1:
	mov.u32 	%r272, %ctaid.y;
	shl.b32 	%r273, %r272, 15;
	or.b32  	%r274, %r10, %r273;
	shr.s32 	%r275, %r8, 2;
	shl.b32 	%r276, %r275, 10;
	add.s32 	%r277, %r274, %r276;
	mul.wide.s32 	%rd12, %r277, 2;
	add.s64 	%rd13, %rd19, %rd12;
	and.b32  	%r278, %r8, 2;
	shr.u32 	%r279, %r278, 1;
	shr.u32 	%r280, %r8, 4;
	add.s32 	%r281, %r279, %r280;
	shl.b32 	%r282, %r281, 4;
	and.b32  	%r283, %r282, 16;
	shl.b32 	%r284, %r275, 5;
	or.b32  	%r285, %r283, %r284;
	and.b32  	%r286, %r8, 8;
	shr.u32 	%r287, %r286, 3;
	add.s32 	%r288, %r287, %r8;
	shl.b32 	%r289, %r288, 3;
	and.b32  	%r290, %r289, 8;
	or.b32  	%r291, %r285, %r290;
	shl.b32 	%r292, %r291, 1;
	add.s32 	%r294, %r82, %r292;
	ld.global.nc.v4.u32 	{%r295, %r296, %r297, %r298}, [%rd13];
	st.shared.v4.u32 	[%r294], {%r295, %r296, %r297, %r298};
	ld.global.nc.v4.u32 	{%r303, %r304, %r305, %r306}, [%rd18];
	st.shared.v4.u32 	[%r294+2048], {%r303, %r304, %r305, %r306};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r93, %r94, %r95, %r96}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r98, %r99, %r100, %r101}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f25,  %f26,  %f27,  %f28},{%r93,  %r94,  %r95,  %r96},{%r98,  %r99},{%f184, %f183, %f182, %f181};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f33,  %f34,  %f35,  %f36},{%r93,  %r94,  %r95,  %r96},{%r100,  %r101},{%f180, %f179, %f178, %f177};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r115, %r116, %r117, %r118}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r120, %r121, %r122, %r123}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f41,  %f42,  %f43,  %f44},{%r115,  %r116,  %r117,  %r118},{%r120,  %r121},{%f25, %f26, %f27, %f28};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r115,  %r116,  %r117,  %r118},{%r122,  %r123},{%f33, %f34, %f35, %f36};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r311, %r312, %r313, %r314}, [%rd13+64];
	st.shared.v4.u32 	[%r294], {%r311, %r312, %r313, %r314};
	ld.global.nc.v4.u32 	{%r319, %r320, %r321, %r322}, [%rd18+64];
	st.shared.v4.u32 	[%r294+2048], {%r319, %r320, %r321, %r322};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r137, %r138, %r139, %r140}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r142, %r143, %r144, %r145}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r137,  %r138,  %r139,  %r140},{%r142,  %r143},{%f41, %f42, %f43, %f44};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r137,  %r138,  %r139,  %r140},{%r144,  %r145},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r159, %r160, %r161, %r162}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r164, %r165, %r166, %r167}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r159,  %r160,  %r161,  %r162},{%r164,  %r165},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r159,  %r160,  %r161,  %r162},{%r166,  %r167},{%f65, %f66, %f67, %f68};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r327, %r328, %r329, %r330}, [%rd13+128];
	st.shared.v4.u32 	[%r294], {%r327, %r328, %r329, %r330};
	ld.global.nc.v4.u32 	{%r335, %r336, %r337, %r338}, [%rd18+128];
	st.shared.v4.u32 	[%r294+2048], {%r335, %r336, %r337, %r338};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r181, %r182, %r183, %r184}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r186, %r187, %r188, %r189}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r181,  %r182,  %r183,  %r184},{%r186,  %r187},{%f73, %f74, %f75, %f76};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r181,  %r182,  %r183,  %r184},{%r188,  %r189},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r203, %r204, %r205, %r206}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r208, %r209, %r210, %r211}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r203,  %r204,  %r205,  %r206},{%r208,  %r209},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r203,  %r204,  %r205,  %r206},{%r210,  %r211},{%f97, %f98, %f99, %f100};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r343, %r344, %r345, %r346}, [%rd13+192];
	st.shared.v4.u32 	[%r294], {%r343, %r344, %r345, %r346};
	ld.global.nc.v4.u32 	{%r351, %r352, %r353, %r354}, [%rd18+192];
	st.shared.v4.u32 	[%r294+2048], {%r351, %r352, %r353, %r354};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r225, %r226, %r227, %r228}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r230, %r231, %r232, %r233}, [%r2];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r225,  %r226,  %r227,  %r228},{%r230,  %r231},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r225,  %r226,  %r227,  %r228},{%r232,  %r233},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r247, %r248, %r249, %r250}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r252, %r253, %r254, %r255}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f184,  %f183,  %f182,  %f181},{%r247,  %r248,  %r249,  %r250},{%r252,  %r253},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f180,  %f179,  %f178,  %f177},{%r247,  %r248,  %r249,  %r250},{%r254,  %r255},{%f129, %f130, %f131, %f132};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd19, %rd19, 256;
	add.s64 	%rd18, %rd18, 256;
	add.s32 	%r381, %r381, 4;
	setp.ne.s32 	%p3, %r381, 32;
	@%p3 bra 	$L__BB0_1;

	ld.param.u64 	%rd17, [main_kernel_param_2];
	mov.u32 	%r380, %ctaid.y;
	shl.b32 	%r379, %r380, 15;
	mov.u32 	%r378, %ctaid.x;
	shl.b32 	%r377, %r8, 8;
	shl.b32 	%r360, %r8, 9;
	and.b32  	%r361, %r360, 16384;
	and.b32  	%r363, %r377, 7168;
	shl.b32 	%r365, %r378, 5;
	shr.s32 	%r366, %r8, 6;
	shl.b32 	%r367, %r366, 3;
	shl.b32 	%r368, %r8, 1;
	and.b32  	%r369, %r368, 6;
	add.s32 	%r372, %r365, %r379;
	add.s32 	%r373, %r372, %r361;
	add.s32 	%r374, %r373, %r367;
	or.b32  	%r375, %r374, %r369;
	add.s32 	%r376, %r375, %r363;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f184;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f154, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f183;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f157, %rs4;}

	// end inline asm
	cvta.to.global.u64 	%rd14, %rd17;
	mul.wide.s32 	%rd15, %r376, 2;
	add.s64 	%rd16, %rd14, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f154;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f182;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f160, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f163, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f160;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16384], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f180;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f166, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f179;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f169, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f166;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f178;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f172, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f175, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f172;}

	// end inline asm
	st.global.v2.u16 	[%rd16+16416], {%rs21, %rs24};
	ret;

}

