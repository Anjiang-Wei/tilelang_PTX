//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN70_INTERNAL_ce5efd78_39_conv2d_tc_BM64_BN64_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<9>;
	.reg .b16 	%rs<129>;
	.reg .f32 	%f<385>;
	.reg .b32 	%r<432>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd5, [main_kernel_param_0];
	ld.param.u64 	%rd7, [main_kernel_param_2];
	cvta.to.global.u64 	%rd8, %rd7;
	mov.u32 	%r40, %tid.x;
	shl.b32 	%r41, %r40, 4;
	and.b32  	%r42, %r41, -128;
	mov.u32 	%r43, %ctaid.x;
	shl.b32 	%r44, %r43, 6;
	shl.b32 	%r45, %r40, 3;
	and.b32  	%r46, %r45, 56;
	shr.s32 	%r47, %r40, 31;
	shr.u32 	%r48, %r47, 28;
	add.s32 	%r49, %r40, %r48;
	and.b32  	%r50, %r49, -16;
	sub.s32 	%r51, %r40, %r50;
	shr.u32 	%r52, %r51, 31;
	add.s32 	%r53, %r51, %r52;
	shr.s32 	%r54, %r53, 1;
	shr.s32 	%r55, %r53, 31;
	shr.u32 	%r56, %r55, 30;
	add.s32 	%r57, %r54, %r56;
	and.b32  	%r58, %r57, -4;
	sub.s32 	%r59, %r54, %r58;
	shr.u32 	%r60, %r49, 31;
	shr.s32 	%r61, %r49, 4;
	add.s32 	%r62, %r61, %r60;
	and.b32  	%r63, %r62, -2;
	sub.s32 	%r64, %r61, %r63;
	shl.b32 	%r65, %r59, 6;
	and.b32  	%r66, %r65, 192;
	shl.b32 	%r67, %r64, 3;
	and.b32  	%r68, %r67, 8;
	or.b32  	%r69, %r66, %r68;
	and.b32  	%r70, %r53, 134217726;
	sub.s32 	%r71, %r51, %r70;
	shl.b32 	%r72, %r71, 5;
	shr.s32 	%r73, %r51, 31;
	shr.u32 	%r74, %r73, 29;
	add.s32 	%r75, %r51, %r74;
	shr.s32 	%r76, %r75, 3;
	shl.b32 	%r77, %r76, 8;
	add.s32 	%r78, %r72, %r77;
	shr.u32 	%r79, %r47, 27;
	add.s32 	%r80, %r40, %r79;
	shr.u32 	%r81, %r80, 31;
	shr.s32 	%r82, %r80, 5;
	add.s32 	%r83, %r82, %r81;
	and.b32  	%r84, %r83, 4194302;
	sub.s32 	%r85, %r82, %r84;
	shl.b32 	%r86, %r85, 9;
	add.s32 	%r87, %r78, %r86;
	and.b32  	%r88, %r59, 2;
	setp.eq.s32 	%p1, %r88, 0;
	mov.u32 	%r422, 0;
	shr.u32 	%r89, %r66, 3;
	xor.b32  	%r90, %r69, %r89;
	add.s32 	%r91, %r87, %r90;
	and.b32  	%r92, %r75, -8;
	sub.s32 	%r93, %r51, %r92;
	shr.u32 	%r94, %r47, 26;
	add.s32 	%r95, %r40, %r94;
	shl.b32 	%r96, %r93, 6;
	and.b32  	%r97, %r96, 448;
	shl.b32 	%r98, %r64, 4;
	and.b32  	%r99, %r98, 16;
	shr.u32 	%r100, %r95, 3;
	and.b32  	%r101, %r100, 8;
	or.b32  	%r102, %r99, %r101;
	or.b32  	%r103, %r102, %r97;
	shl.b32 	%r104, %r76, 9;
	and.b32  	%r105, %r93, 4;
	setp.eq.s32 	%p2, %r105, 0;
	shr.u32 	%r106, %r97, 3;
	xor.b32  	%r107, %r103, %r106;
	or.b32  	%r108, %r107, %r104;
	shl.b32 	%r109, %r91, 1;
	mov.u32 	%r110, buf_dyn_shmem;
	add.s32 	%r111, %r110, %r109;
	add.s32 	%r1, %r111, 4096;
	shl.b32 	%r112, %r108, 1;
	add.s32 	%r2, %r110, %r112;
	selp.b32 	%r113, 64, -64, %p2;
	add.s32 	%r3, %r2, %r113;
	selp.b32 	%r114, 32, -32, %p1;
	add.s32 	%r4, %r1, %r114;
	add.s32 	%r5, %r4, 2048;
	add.s32 	%r6, %r3, 2048;
	or.b32  	%r115, %r46, %r42;
	add.s32 	%r116, %r115, %r44;
	mul.wide.s32 	%rd9, %r116, 2;
	add.s64 	%rd18, %rd8, %rd9;
	mov.f32 	%f353, 0f00000000;
	cvta.to.global.u64 	%rd12, %rd5;
	and.b32  	%r308, %r40, 32;
	shr.u32 	%r309, %r308, 5;
	and.b32  	%r310, %r40, 4;
	shr.u32 	%r311, %r310, 2;
	add.s32 	%r312, %r309, %r311;
	shl.b32 	%r313, %r312, 5;
	and.b32  	%r314, %r313, 32;
	and.b32  	%r316, %r45, 2147483584;
	or.b32  	%r317, %r314, %r316;
	and.b32  	%r318, %r40, 2;
	shr.u32 	%r319, %r318, 1;
	shr.u32 	%r320, %r40, 4;
	add.s32 	%r321, %r319, %r320;
	shl.b32 	%r322, %r321, 4;
	and.b32  	%r323, %r322, 16;
	or.b32  	%r324, %r317, %r323;
	and.b32  	%r325, %r40, 8;
	shr.u32 	%r326, %r325, 3;
	add.s32 	%r327, %r326, %r40;
	shl.b32 	%r328, %r327, 3;
	and.b32  	%r329, %r328, 8;
	or.b32  	%r330, %r324, %r329;
	shl.b32 	%r331, %r330, 1;
	add.s32 	%r333, %r110, %r331;
	mov.f32 	%f354, %f353;
	mov.f32 	%f355, %f353;
	mov.f32 	%f356, %f353;
	mov.f32 	%f357, %f353;
	mov.f32 	%f358, %f353;
	mov.f32 	%f359, %f353;
	mov.f32 	%f360, %f353;
	mov.f32 	%f361, %f353;
	mov.f32 	%f362, %f353;
	mov.f32 	%f363, %f353;
	mov.f32 	%f364, %f353;
	mov.f32 	%f365, %f353;
	mov.f32 	%f366, %f353;
	mov.f32 	%f367, %f353;
	mov.f32 	%f368, %f353;
	mov.f32 	%f369, %f353;
	mov.f32 	%f370, %f353;
	mov.f32 	%f371, %f353;
	mov.f32 	%f372, %f353;
	mov.f32 	%f373, %f353;
	mov.f32 	%f374, %f353;
	mov.f32 	%f375, %f353;
	mov.f32 	%f376, %f353;
	mov.f32 	%f377, %f353;
	mov.f32 	%f378, %f353;
	mov.f32 	%f379, %f353;
	mov.f32 	%f380, %f353;
	mov.f32 	%f381, %f353;
	mov.f32 	%f382, %f353;
	mov.f32 	%f383, %f353;
	mov.f32 	%f384, %f353;
	mov.u32 	%r423, %r422;

$L__BB0_1:
	mov.u32 	%r413, %tid.x;
	shl.b32 	%r412, %r413, 3;
	mul.wide.u32 	%rd10, %r423, -1431655765;
	shr.u64 	%rd11, %rd10, 35;
	cvt.u32.u64 	%r117, %rd11;
	and.b32  	%r120, %r412, 24;
	mov.u32 	%r121, %ctaid.y;
	shl.b32 	%r122, %r121, 13;
	or.b32  	%r123, %r120, %r122;
	shr.s32 	%r124, %r413, 2;
	shl.b32 	%r125, %r124, 7;
	add.s32 	%r126, %r123, %r125;
	mad.lo.s32 	%r127, %r117, 7808, %r126;
	add.s32 	%r9, %r127, -8320;
	and.b32  	%r128, %r121, 63;
	neg.s32 	%r129, %r128;
	setp.ne.s32 	%p3, %r117, %r129;
	add.s32 	%r130, %r117, %r128;
	setp.lt.u32 	%p4, %r130, 65;
	and.pred  	%p5, %p3, %p4;
	mad.lo.s32 	%r131, %r117, -12, %r423;
	shr.u32 	%r132, %r131, 2;
	add.s32 	%r133, %r124, %r132;
	add.s32 	%r10, %r133, -1;
	shl.b32 	%r134, %r124, 5;
	or.b32  	%r146, %r329, %r323;
	or.b32  	%r147, %r146, %r134;
	shl.b32 	%r148, %r147, 1;
	add.s32 	%r150, %r110, %r148;
	@%p5 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_2;

$L__BB0_3:
	and.b32  	%r159, %r10, -64;
	setp.eq.s32 	%p6, %r159, 0;
	add.s32 	%r160, %r422, %r9;
	mul.wide.s32 	%rd13, %r160, 2;
	add.s64 	%rd3, %rd12, %rd13;
	@%p6 bra 	$L__BB0_5;

	mov.f32 	%f120, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs17, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs20, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs23, %f120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f120;}

	// end inline asm
	mov.b32 	%r427, {%rs23, %rs24};
	mov.b32 	%r426, {%rs21, %rs22};
	mov.b32 	%r425, {%rs19, %rs20};
	mov.b32 	%r424, {%rs17, %rs18};
	bra.uni 	$L__BB0_6;

$L__BB0_2:
	mov.f32 	%f112, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs2, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs5, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs8, %f112;}

	// end inline asm
	mov.b32 	%r151, {%rs7, %rs8};
	mov.b32 	%r152, {%rs5, %rs6};
	mov.b32 	%r153, {%rs3, %rs4};
	mov.b32 	%r154, {%rs1, %rs2};
	add.s32 	%r408, %r150, 4096;
	st.shared.v4.u32 	[%r408], {%r154, %r153, %r152, %r151};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs11, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs14, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f112;}

	// end inline asm
	mov.b32 	%r155, {%rs15, %rs16};
	mov.b32 	%r156, {%rs13, %rs14};
	mov.b32 	%r157, {%rs11, %rs12};
	mov.b32 	%r158, {%rs9, %rs10};
	add.s32 	%r409, %r150, 4096;
	st.shared.v4.u32 	[%r409+2048], {%r158, %r157, %r156, %r155};
	bra.uni 	$L__BB0_10;

$L__BB0_5:
	ld.global.nc.v4.u32 	{%r424, %r425, %r426, %r427}, [%rd3];

$L__BB0_6:
	add.s32 	%r410, %r150, 4096;
	st.shared.v4.u32 	[%r410], {%r424, %r425, %r426, %r427};
	add.s32 	%r165, %r10, 32;
	and.b32  	%r166, %r165, -64;
	setp.eq.s32 	%p7, %r166, 0;
	@%p7 bra 	$L__BB0_8;

	mov.f32 	%f128, 0f00000000;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs26, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs29, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f128;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs32, %f128;}

	// end inline asm
	mov.b32 	%r431, {%rs31, %rs32};
	mov.b32 	%r430, {%rs29, %rs30};
	mov.b32 	%r429, {%rs27, %rs28};
	mov.b32 	%r428, {%rs25, %rs26};
	bra.uni 	$L__BB0_9;

$L__BB0_8:
	ld.global.nc.v4.u32 	{%r428, %r429, %r430, %r431}, [%rd3+8192];

$L__BB0_9:
	add.s32 	%r411, %r150, 4096;
	st.shared.v4.u32 	[%r411+2048], {%r428, %r429, %r430, %r431};

$L__BB0_10:
	add.s32 	%r253, %r2, 2048;
	add.s32 	%r180, %r1, 2048;
	ld.global.nc.v4.u32 	{%r334, %r335, %r336, %r337}, [%rd18];
	st.shared.v4.u32 	[%r333], {%r334, %r335, %r336, %r337};
	ld.global.nc.v4.u32 	{%r342, %r343, %r344, %r345}, [%rd18+4096];
	st.shared.v4.u32 	[%r333+2048], {%r342, %r343, %r344, %r345};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r171, %r172, %r173, %r174}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r176, %r177, %r178, %r179}, [%r180];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r181, %r182, %r183, %r184}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r186, %r187, %r188, %r189}, [%r3];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r171,  %r172,  %r173,  %r174},{%r181,  %r182},{%f384, %f383, %f382, %f381};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r176,  %r177,  %r178,  %r179},{%r181,  %r182},{%f380, %f379, %f378, %f377};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f145,  %f146,  %f147,  %f148},{%r176,  %r177,  %r178,  %r179},{%r183,  %r184},{%f372, %f371, %f370, %f369};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f153,  %f154,  %f155,  %f156},{%r171,  %r172,  %r173,  %r174},{%r183,  %r184},{%f376, %f375, %f374, %f373};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f161,  %f162,  %f163,  %f164},{%r171,  %r172,  %r173,  %r174},{%r186,  %r187},{%f368, %f367, %f366, %f365};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f169,  %f170,  %f171,  %f172},{%r176,  %r177,  %r178,  %r179},{%r186,  %r187},{%f364, %f363, %f362, %f361};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f177,  %f178,  %f179,  %f180},{%r176,  %r177,  %r178,  %r179},{%r188,  %r189},{%f356, %f355, %f354, %f353};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f185,  %f186,  %f187,  %f188},{%r171,  %r172,  %r173,  %r174},{%r188,  %r189},{%f360, %f359, %f358, %f357};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r239, %r240, %r241, %r242}, [%r4];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r244, %r245, %r246, %r247}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r249, %r250, %r251, %r252}, [%r253];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r254, %r255, %r256, %r257}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f384,  %f383,  %f382,  %f381},{%r239,  %r240,  %r241,  %r242},{%r249,  %r250},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f380,  %f379,  %f378,  %f377},{%r244,  %r245,  %r246,  %r247},{%r249,  %r250},{%f137, %f138, %f139, %f140};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f372,  %f371,  %f370,  %f369},{%r244,  %r245,  %r246,  %r247},{%r251,  %r252},{%f145, %f146, %f147, %f148};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f376,  %f375,  %f374,  %f373},{%r239,  %r240,  %r241,  %r242},{%r251,  %r252},{%f153, %f154, %f155, %f156};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f368,  %f367,  %f366,  %f365},{%r239,  %r240,  %r241,  %r242},{%r254,  %r255},{%f161, %f162, %f163, %f164};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f364,  %f363,  %f362,  %f361},{%r244,  %r245,  %r246,  %r247},{%r254,  %r255},{%f169, %f170, %f171, %f172};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f356,  %f355,  %f354,  %f353},{%r244,  %r245,  %r246,  %r247},{%r256,  %r257},{%f177, %f178, %f179, %f180};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f360,  %f359,  %f358,  %f357},{%r239,  %r240,  %r241,  %r242},{%r256,  %r257},{%f185, %f186, %f187, %f188};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd18, %rd18, 8192;
	add.s32 	%r423, %r423, 1;
	add.s32 	%r422, %r422, 32;
	setp.ne.s32 	%p8, %r422, 1152;
	@%p8 bra 	$L__BB0_1;

	shl.b32 	%r421, %r40, 4;
	ld.param.u64 	%rd17, [main_kernel_param_1];
	mov.u32 	%r420, %ctaid.y;
	shl.b32 	%r419, %r420, 13;
	shl.b32 	%r418, %r40, 3;
	and.b32  	%r417, %r418, 56;
	mov.u32 	%r416, %ctaid.x;
	shl.b32 	%r415, %r416, 6;
	and.b32  	%r414, %r421, -128;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f384;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f258, %rs33;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f383;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f261, %rs36;}

	// end inline asm
	shl.b32 	%r351, %r40, 5;
	and.b32  	%r352, %r351, 1024;
	and.b32  	%r354, %r421, 448;
	shr.s32 	%r355, %r40, 6;
	shl.b32 	%r356, %r355, 3;
	shl.b32 	%r357, %r40, 1;
	and.b32  	%r358, %r357, 6;
	add.s32 	%r359, %r356, %r352;
	or.b32  	%r360, %r359, %r358;
	add.s32 	%r361, %r360, %r354;
	shl.b32 	%r362, %r361, 1;
	add.s32 	%r364, %r110, %r362;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs38, %f261;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs35, %f258;}

	// end inline asm
	st.shared.v2.u16 	[%r364], {%rs35, %rs38};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f382;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f264, %rs39;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f381;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f267, %rs42;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs44, %f267;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f264;}

	// end inline asm
	st.shared.v2.u16 	[%r364+1024], {%rs41, %rs44};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f380;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f270, %rs45;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f379;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f273, %rs48;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs50, %f273;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f270;}

	// end inline asm
	st.shared.v2.u16 	[%r364+4096], {%rs47, %rs50};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs51, %f378;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f276, %rs51;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs54, %f377;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f279, %rs54;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs56, %f279;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f276;}

	// end inline asm
	st.shared.v2.u16 	[%r364+5120], {%rs53, %rs56};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs57, %f376;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f282, %rs57;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f375;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f285, %rs60;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs62, %f285;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f282;}

	// end inline asm
	st.shared.v2.u16 	[%r364+32], {%rs59, %rs62};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f374;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f288, %rs63;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f373;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f291, %rs66;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs68, %f291;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f288;}

	// end inline asm
	st.shared.v2.u16 	[%r364+1056], {%rs65, %rs68};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f372;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f294, %rs69;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f371;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f297, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs74, %f297;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f294;}

	// end inline asm
	st.shared.v2.u16 	[%r364+4128], {%rs71, %rs74};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f370;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f300, %rs75;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f369;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f303, %rs78;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs80, %f303;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs77, %f300;}

	// end inline asm
	st.shared.v2.u16 	[%r364+5152], {%rs77, %rs80};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f368;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f306, %rs81;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f367;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f309, %rs84;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs86, %f309;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs83, %f306;}

	// end inline asm
	st.shared.v2.u16 	[%r364+64], {%rs83, %rs86};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f366;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f312, %rs87;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f365;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f315, %rs90;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs92, %f315;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs89, %f312;}

	// end inline asm
	st.shared.v2.u16 	[%r364+1088], {%rs89, %rs92};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f364;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f318, %rs93;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs96, %f363;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f321, %rs96;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs98, %f321;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs95, %f318;}

	// end inline asm
	st.shared.v2.u16 	[%r364+4160], {%rs95, %rs98};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs99, %f362;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f324, %rs99;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f361;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f327, %rs102;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs104, %f327;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs101, %f324;}

	// end inline asm
	st.shared.v2.u16 	[%r364+5184], {%rs101, %rs104};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs105, %f360;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f330, %rs105;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f359;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f333, %rs108;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs110, %f333;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs107, %f330;}

	// end inline asm
	st.shared.v2.u16 	[%r364+96], {%rs107, %rs110};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs111, %f358;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f336, %rs111;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f357;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f339, %rs114;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs116, %f339;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs113, %f336;}

	// end inline asm
	st.shared.v2.u16 	[%r364+1120], {%rs113, %rs116};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f356;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f342, %rs117;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f355;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f345, %rs120;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs122, %f345;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs119, %f342;}

	// end inline asm
	st.shared.v2.u16 	[%r364+4192], {%rs119, %rs122};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs123, %f354;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f348, %rs123;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f353;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f351, %rs126;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs128, %f351;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs125, %f348;}

	// end inline asm
	st.shared.v2.u16 	[%r364+5216], {%rs125, %rs128};
	bar.sync 	0;
	add.s32 	%r368, %r414, %r415;
	or.b32  	%r371, %r368, %r417;
	add.s32 	%r374, %r371, %r419;
	cvta.to.global.u64 	%rd14, %rd17;
	mul.wide.s32 	%rd15, %r374, 2;
	add.s64 	%rd16, %rd14, %rd15;
	add.s32 	%r375, %r110, %r421;
	ld.shared.v4.u32 	{%r376, %r377, %r378, %r379}, [%r375];
	st.global.v4.u32 	[%rd16], {%r376, %r377, %r378, %r379};
	ld.shared.v4.u32 	{%r384, %r385, %r386, %r387}, [%r375+2048];
	st.global.v4.u32 	[%rd16+4096], {%r384, %r385, %r386, %r387};
	ld.shared.v4.u32 	{%r392, %r393, %r394, %r395}, [%r375+4096];
	st.global.v4.u32 	[%rd16+8192], {%r392, %r393, %r394, %r395};
	ld.shared.v4.u32 	{%r400, %r401, %r402, %r403}, [%r375+6144];
	st.global.v4.u32 	[%rd16+12288], {%r400, %r401, %r402, %r403};
	ret;

}

