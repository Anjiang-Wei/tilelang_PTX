//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN68_INTERNAL_6dab69d9_37_conv2d_BM64_BN128_BK32_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<35>;
	.reg .b16 	%rs<193>;
	.reg .f32 	%f<961>;
	.reg .b32 	%r<984>;
	.reg .b64 	%rd<34>;


	ld.param.u64 	%rd3, [main_kernel_param_0];
	ld.param.u64 	%rd33, [main_kernel_param_1];
	mov.u32 	%r24, %ctaid.y;
	and.b32  	%r25, %r24, 63;
	setp.ne.s32 	%p5, %r25, 0;
	mov.u32 	%r982, 0;
	mov.u32 	%r26, %tid.x;
	shr.s32 	%r27, %r26, 2;
	shl.b32 	%r28, %r27, 6;
	and.b32  	%r29, %r26, 2;
	shr.u32 	%r30, %r29, 1;
	mov.u32 	%r983, 1;
	shr.u32 	%r31, %r26, 4;
	add.s32 	%r32, %r30, %r31;
	shl.b32 	%r33, %r32, 5;
	and.b32  	%r34, %r33, 32;
	and.b32  	%r35, %r26, 8;
	shr.u32 	%r36, %r35, 3;
	add.s32 	%r37, %r36, %r26;
	shl.b32 	%r38, %r37, 4;
	and.b32  	%r39, %r38, 16;
	or.b32  	%r40, %r28, %r34;
	add.s32 	%r41, %r40, 8192;
	or.b32  	%r42, %r41, %r39;
	shl.b32 	%r43, %r24, 13;
	shl.b32 	%r44, %r27, 7;
	shl.b32 	%r45, %r26, 3;
	and.b32  	%r46, %r45, 24;
	add.s32 	%r47, %r43, %r44;
	add.s32 	%r48, %r47, -8320;
	or.b32  	%r49, %r48, %r46;
	setp.gt.s32 	%p6, %r26, 3;
	and.pred  	%p7, %p5, %p6;
	mul.wide.s32 	%rd12, %r49, 2;
	add.s64 	%rd6, %rd3, %rd12;
	selp.b32 	%r15, 16, 0, %p7;
	mov.u32 	%r50, buf_dyn_shmem;
	add.s32 	%r14, %r50, %r42;
	// begin inline asm
	cp.async.cg.shared.global [%r14], [%rd6], 16, %r15;
	// end inline asm
	setp.gt.s32 	%p8, %r26, -125;
	and.pred  	%p9, %p5, %p8;
	add.s32 	%r51, %r49, 4096;
	mul.wide.s32 	%rd13, %r51, 2;
	add.s64 	%rd7, %rd3, %rd13;
	selp.b32 	%r17, 16, 0, %p9;
	add.s32 	%r16, %r14, 2048;
	// begin inline asm
	cp.async.cg.shared.global [%r16], [%rd7], 16, %r17;
	// end inline asm
	shl.b32 	%r52, %r26, 9;
	and.b32  	%r53, %r52, 4096;
	shl.b32 	%r54, %r31, 7;
	shr.u32 	%r55, %r26, 6;
	and.b32  	%r56, %r26, 4;
	shr.u32 	%r57, %r56, 2;
	add.s32 	%r58, %r57, %r55;
	shl.b32 	%r59, %r58, 6;
	and.b32  	%r60, %r59, 64;
	and.b32  	%r61, %r26, 32;
	shr.u32 	%r62, %r61, 5;
	add.s32 	%r63, %r62, %r30;
	shl.b32 	%r64, %r63, 5;
	and.b32  	%r65, %r64, 32;
	add.s32 	%r66, %r31, %r26;
	shl.b32 	%r67, %r66, 4;
	and.b32  	%r68, %r67, 16;
	add.s32 	%r69, %r54, %r53;
	or.b32  	%r70, %r69, %r68;
	or.b32  	%r71, %r70, %r60;
	or.b32  	%r72, %r71, %r65;
	mul.wide.s32 	%rd14, %r45, 2;
	add.s64 	%rd8, %rd33, %rd14;
	add.s32 	%r18, %r50, %r72;
	// begin inline asm
	cp.async.cg.shared.global [%r18], [%rd8], 16;
	// end inline asm
	add.s64 	%rd9, %rd8, 2048;
	add.s32 	%r19, %r18, 1024;
	// begin inline asm
	cp.async.cg.shared.global [%r19], [%rd9], 16;
	// end inline asm
	add.s64 	%rd10, %rd8, 4096;
	add.s32 	%r20, %r18, 2048;
	// begin inline asm
	cp.async.cg.shared.global [%r20], [%rd10], 16;
	// end inline asm
	add.s64 	%rd11, %rd8, 6144;
	add.s32 	%r21, %r18, 3072;
	// begin inline asm
	cp.async.cg.shared.global [%r21], [%rd11], 16;
	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	shr.s32 	%r73, %r26, 31;
	shr.u32 	%r74, %r73, 28;
	add.s32 	%r75, %r26, %r74;
	and.b32  	%r76, %r75, -16;
	sub.s32 	%r77, %r26, %r76;
	shr.u32 	%r78, %r77, 31;
	add.s32 	%r79, %r77, %r78;
	shr.s32 	%r80, %r79, 1;
	shr.s32 	%r81, %r79, 31;
	shr.u32 	%r82, %r81, 30;
	add.s32 	%r83, %r80, %r82;
	and.b32  	%r84, %r83, -4;
	sub.s32 	%r85, %r80, %r84;
	shr.u32 	%r86, %r75, 31;
	shr.s32 	%r87, %r75, 4;
	add.s32 	%r88, %r87, %r86;
	and.b32  	%r89, %r88, -2;
	sub.s32 	%r90, %r87, %r89;
	shl.b32 	%r91, %r85, 6;
	and.b32  	%r92, %r91, 192;
	shl.b32 	%r93, %r90, 3;
	and.b32  	%r94, %r93, 8;
	or.b32  	%r95, %r92, %r94;
	and.b32  	%r96, %r79, 134217726;
	sub.s32 	%r97, %r77, %r96;
	shl.b32 	%r98, %r97, 5;
	shr.s32 	%r99, %r77, 31;
	shr.u32 	%r100, %r99, 29;
	add.s32 	%r101, %r77, %r100;
	shr.s32 	%r102, %r101, 3;
	shl.b32 	%r103, %r102, 8;
	add.s32 	%r104, %r98, %r103;
	shr.u32 	%r105, %r73, 27;
	add.s32 	%r106, %r26, %r105;
	shr.u32 	%r107, %r106, 31;
	shr.s32 	%r108, %r106, 5;
	add.s32 	%r109, %r108, %r107;
	and.b32  	%r110, %r109, 4194302;
	sub.s32 	%r111, %r108, %r110;
	shl.b32 	%r112, %r111, 9;
	add.s32 	%r113, %r104, %r112;
	and.b32  	%r114, %r85, 2;
	setp.eq.s32 	%p10, %r114, 0;
	shr.u32 	%r115, %r92, 3;
	xor.b32  	%r116, %r95, %r115;
	add.s32 	%r117, %r113, %r116;
	and.b32  	%r118, %r101, -8;
	sub.s32 	%r119, %r77, %r118;
	shr.u32 	%r120, %r73, 26;
	add.s32 	%r121, %r26, %r120;
	shl.b32 	%r122, %r119, 6;
	and.b32  	%r123, %r122, 448;
	shl.b32 	%r124, %r90, 4;
	and.b32  	%r125, %r124, 16;
	shr.u32 	%r126, %r121, 3;
	and.b32  	%r127, %r126, 8;
	or.b32  	%r128, %r125, %r127;
	or.b32  	%r129, %r128, %r123;
	shl.b32 	%r130, %r102, 9;
	and.b32  	%r131, %r119, 4;
	setp.eq.s32 	%p11, %r131, 0;
	shr.u32 	%r132, %r123, 3;
	xor.b32  	%r133, %r129, %r132;
	or.b32  	%r134, %r133, %r130;
	shl.b32 	%r135, %r117, 1;
	add.s32 	%r136, %r50, %r135;
	add.s32 	%r1, %r136, 8192;
	shl.b32 	%r137, %r134, 1;
	add.s32 	%r2, %r50, %r137;
	selp.b32 	%r138, 64, -64, %p11;
	add.s32 	%r3, %r2, %r138;
	add.s32 	%r4, %r3, 4096;
	selp.b32 	%r139, 32, -32, %p10;
	add.s32 	%r5, %r1, %r139;
	add.s32 	%r6, %r5, 2048;
	add.s32 	%r7, %r3, 2048;
	add.s32 	%r8, %r3, 6144;
	mov.f32 	%f897, 0f00000000;
	mov.f32 	%f898, %f897;
	mov.f32 	%f899, %f897;
	mov.f32 	%f900, %f897;
	mov.f32 	%f901, %f897;
	mov.f32 	%f902, %f897;
	mov.f32 	%f903, %f897;
	mov.f32 	%f904, %f897;
	mov.f32 	%f905, %f897;
	mov.f32 	%f906, %f897;
	mov.f32 	%f907, %f897;
	mov.f32 	%f908, %f897;
	mov.f32 	%f909, %f897;
	mov.f32 	%f910, %f897;
	mov.f32 	%f911, %f897;
	mov.f32 	%f912, %f897;
	mov.f32 	%f913, %f897;
	mov.f32 	%f914, %f897;
	mov.f32 	%f915, %f897;
	mov.f32 	%f916, %f897;
	mov.f32 	%f917, %f897;
	mov.f32 	%f918, %f897;
	mov.f32 	%f919, %f897;
	mov.f32 	%f920, %f897;
	mov.f32 	%f921, %f897;
	mov.f32 	%f922, %f897;
	mov.f32 	%f923, %f897;
	mov.f32 	%f924, %f897;
	mov.f32 	%f925, %f897;
	mov.f32 	%f926, %f897;
	mov.f32 	%f927, %f897;
	mov.f32 	%f928, %f897;
	mov.f32 	%f929, %f897;
	mov.f32 	%f930, %f897;
	mov.f32 	%f931, %f897;
	mov.f32 	%f932, %f897;
	mov.f32 	%f933, %f897;
	mov.f32 	%f934, %f897;
	mov.f32 	%f935, %f897;
	mov.f32 	%f936, %f897;
	mov.f32 	%f937, %f897;
	mov.f32 	%f938, %f897;
	mov.f32 	%f939, %f897;
	mov.f32 	%f940, %f897;
	mov.f32 	%f941, %f897;
	mov.f32 	%f942, %f897;
	mov.f32 	%f943, %f897;
	mov.f32 	%f944, %f897;
	mov.f32 	%f945, %f897;
	mov.f32 	%f946, %f897;
	mov.f32 	%f947, %f897;
	mov.f32 	%f948, %f897;
	mov.f32 	%f949, %f897;
	mov.f32 	%f950, %f897;
	mov.f32 	%f951, %f897;
	mov.f32 	%f952, %f897;
	mov.f32 	%f953, %f897;
	mov.f32 	%f954, %f897;
	mov.f32 	%f955, %f897;
	mov.f32 	%f956, %f897;
	mov.f32 	%f957, %f897;
	mov.f32 	%f958, %f897;
	mov.f32 	%f959, %f897;
	mov.f32 	%f960, %f897;

$L__BB0_1:
	add.s32 	%r290, %r2, 6144;
	add.s32 	%r280, %r2, 2048;
	add.s32 	%r164, %r2, 4096;
	add.s32 	%r149, %r1, 2048;
	mul.wide.u32 	%rd15, %r983, -1431655765;
	shr.u64 	%rd16, %rd15, 35;
	cvt.u32.u64 	%r11, %rd16;
	// begin inline asm
	cp.async.wait_all;

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r140, %r141, %r142, %r143}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r145, %r146, %r147, %r148}, [%r149];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r150, %r151, %r152, %r153}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r155, %r156, %r157, %r158}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r160, %r161, %r162, %r163}, [%r164];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r165, %r166, %r167, %r168}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f193,  %f194,  %f195,  %f196},{%r140,  %r141,  %r142,  %r143},{%r150,  %r151},{%f960, %f959, %f958, %f957};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f201,  %f202,  %f203,  %f204},{%r145,  %r146,  %r147,  %r148},{%r150,  %r151},{%f956, %f955, %f954, %f953};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f209,  %f210,  %f211,  %f212},{%r145,  %r146,  %r147,  %r148},{%r152,  %r153},{%f948, %f947, %f946, %f945};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f217,  %f218,  %f219,  %f220},{%r140,  %r141,  %r142,  %r143},{%r152,  %r153},{%f952, %f951, %f950, %f949};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f225,  %f226,  %f227,  %f228},{%r140,  %r141,  %r142,  %r143},{%r155,  %r156},{%f944, %f943, %f942, %f941};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f233,  %f234,  %f235,  %f236},{%r145,  %r146,  %r147,  %r148},{%r155,  %r156},{%f940, %f939, %f938, %f937};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f241,  %f242,  %f243,  %f244},{%r145,  %r146,  %r147,  %r148},{%r157,  %r158},{%f932, %f931, %f930, %f929};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f249,  %f250,  %f251,  %f252},{%r140,  %r141,  %r142,  %r143},{%r157,  %r158},{%f936, %f935, %f934, %f933};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f257,  %f258,  %f259,  %f260},{%r140,  %r141,  %r142,  %r143},{%r160,  %r161},{%f928, %f927, %f926, %f925};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f265,  %f266,  %f267,  %f268},{%r145,  %r146,  %r147,  %r148},{%r160,  %r161},{%f924, %f923, %f922, %f921};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f273,  %f274,  %f275,  %f276},{%r145,  %r146,  %r147,  %r148},{%r162,  %r163},{%f916, %f915, %f914, %f913};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f281,  %f282,  %f283,  %f284},{%r140,  %r141,  %r142,  %r143},{%r162,  %r163},{%f920, %f919, %f918, %f917};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f289,  %f290,  %f291,  %f292},{%r140,  %r141,  %r142,  %r143},{%r165,  %r166},{%f912, %f911, %f910, %f909};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f297,  %f298,  %f299,  %f300},{%r145,  %r146,  %r147,  %r148},{%r165,  %r166},{%f908, %f907, %f906, %f905};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f305,  %f306,  %f307,  %f308},{%r145,  %r146,  %r147,  %r148},{%r167,  %r168},{%f900, %f899, %f898, %f897};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f313,  %f314,  %f315,  %f316},{%r140,  %r141,  %r142,  %r143},{%r167,  %r168},{%f904, %f903, %f902, %f901};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r266, %r267, %r268, %r269}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r271, %r272, %r273, %r274}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r276, %r277, %r278, %r279}, [%r280];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r281, %r282, %r283, %r284}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r286, %r287, %r288, %r289}, [%r290];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r291, %r292, %r293, %r294}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f960,  %f959,  %f958,  %f957},{%r266,  %r267,  %r268,  %r269},{%r276,  %r277},{%f193, %f194, %f195, %f196};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f956,  %f955,  %f954,  %f953},{%r271,  %r272,  %r273,  %r274},{%r276,  %r277},{%f201, %f202, %f203, %f204};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f948,  %f947,  %f946,  %f945},{%r271,  %r272,  %r273,  %r274},{%r278,  %r279},{%f209, %f210, %f211, %f212};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f952,  %f951,  %f950,  %f949},{%r266,  %r267,  %r268,  %r269},{%r278,  %r279},{%f217, %f218, %f219, %f220};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f944,  %f943,  %f942,  %f941},{%r266,  %r267,  %r268,  %r269},{%r281,  %r282},{%f225, %f226, %f227, %f228};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f940,  %f939,  %f938,  %f937},{%r271,  %r272,  %r273,  %r274},{%r281,  %r282},{%f233, %f234, %f235, %f236};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f932,  %f931,  %f930,  %f929},{%r271,  %r272,  %r273,  %r274},{%r283,  %r284},{%f241, %f242, %f243, %f244};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f936,  %f935,  %f934,  %f933},{%r266,  %r267,  %r268,  %r269},{%r283,  %r284},{%f249, %f250, %f251, %f252};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f928,  %f927,  %f926,  %f925},{%r266,  %r267,  %r268,  %r269},{%r286,  %r287},{%f257, %f258, %f259, %f260};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f924,  %f923,  %f922,  %f921},{%r271,  %r272,  %r273,  %r274},{%r286,  %r287},{%f265, %f266, %f267, %f268};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f916,  %f915,  %f914,  %f913},{%r271,  %r272,  %r273,  %r274},{%r288,  %r289},{%f273, %f274, %f275, %f276};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f920,  %f919,  %f918,  %f917},{%r266,  %r267,  %r268,  %r269},{%r288,  %r289},{%f281, %f282, %f283, %f284};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f912,  %f911,  %f910,  %f909},{%r266,  %r267,  %r268,  %r269},{%r291,  %r292},{%f289, %f290, %f291, %f292};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f908,  %f907,  %f906,  %f905},{%r271,  %r272,  %r273,  %r274},{%r291,  %r292},{%f297, %f298, %f299, %f300};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f900,  %f899,  %f898,  %f897},{%r271,  %r272,  %r273,  %r274},{%r293,  %r294},{%f305, %f306, %f307, %f308};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f904,  %f903,  %f902,  %f901},{%r266,  %r267,  %r268,  %r269},{%r293,  %r294},{%f313, %f314, %f315, %f316};

	// end inline asm
	bar.sync 	0;
	neg.s32 	%r394, %r25;
	setp.eq.s32 	%p13, %r11, %r394;
	mov.pred 	%p33, 0;
	@%p13 bra 	$L__BB0_3;

	add.s32 	%r397, %r11, %r25;
	setp.lt.u32 	%p14, %r397, 65;
	mad.lo.s32 	%r400, %r11, -12, %r983;
	shr.u32 	%r401, %r400, 2;
	add.s32 	%r402, %r27, %r401;
	add.s32 	%r403, %r402, -1;
	setp.lt.u32 	%p15, %r403, 64;
	and.pred  	%p33, %p14, %p15;

$L__BB0_3:
	shl.b32 	%r962, %r27, 7;
	add.s32 	%r408, %r11, %r25;
	setp.lt.u32 	%p16, %r408, 65;
	setp.ne.s32 	%p18, %r11, %r394;
	or.b32  	%r431, %r46, %r43;
	add.s32 	%r433, %r431, %r962;
	mad.lo.s32 	%r434, %r11, 7808, %r433;
	add.s32 	%r435, %r982, %r434;
	add.s32 	%r436, %r435, -8288;
	mul.wide.s32 	%rd18, %r436, 2;
	add.s64 	%rd17, %rd3, %rd18;
	mad.lo.s32 	%r437, %r11, -12, %r983;
	shr.u32 	%r438, %r437, 2;
	add.s32 	%r439, %r27, %r438;
	add.s32 	%r440, %r439, -1;
	setp.lt.u32 	%p19, %r440, 64;
	and.pred  	%p20, %p18, %p19;
	and.pred  	%p21, %p16, %p20;
	and.pred  	%p22, %p33, %p21;
	selp.b32 	%r405, 16, 0, %p22;
	// begin inline asm
	cp.async.cg.shared.global [%r14], [%rd17], 16, %r405;
	// end inline asm
	@%p13 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_4;

$L__BB0_5:
	mov.pred 	%p34, 0;
	bra.uni 	$L__BB0_6;

$L__BB0_4:
	add.s32 	%r449, %r439, 31;
	setp.lt.u32 	%p24, %r449, 64;
	and.pred  	%p34, %p16, %p24;

$L__BB0_6:
	add.s32 	%r503, %r439, 31;
	add.s32 	%r512, %r435, -4192;
	mul.wide.s32 	%rd24, %r512, 2;
	add.s64 	%rd19, %rd3, %rd24;
	setp.lt.u32 	%p28, %r503, 64;
	and.pred  	%p29, %p18, %p28;
	and.pred  	%p30, %p16, %p29;
	and.pred  	%p31, %p34, %p30;
	selp.b32 	%r451, 16, 0, %p31;
	// begin inline asm
	cp.async.cg.shared.global [%r16], [%rd19], 16, %r451;
	// end inline asm
	add.s32 	%r513, %r45, 4096;
	mul.wide.s32 	%rd25, %r513, 2;
	add.s64 	%rd20, %rd33, %rd25;
	// begin inline asm
	cp.async.cg.shared.global [%r18], [%rd20], 16;
	// end inline asm
	add.s32 	%r514, %r45, 5120;
	mul.wide.s32 	%rd26, %r514, 2;
	add.s64 	%rd21, %rd33, %rd26;
	// begin inline asm
	cp.async.cg.shared.global [%r19], [%rd21], 16;
	// end inline asm
	add.s32 	%r515, %r45, 6144;
	mul.wide.s32 	%rd27, %r515, 2;
	add.s64 	%rd22, %rd33, %rd27;
	// begin inline asm
	cp.async.cg.shared.global [%r20], [%rd22], 16;
	// end inline asm
	add.s32 	%r516, %r45, 7168;
	mul.wide.s32 	%rd28, %r516, 2;
	add.s64 	%rd23, %rd33, %rd28;
	// begin inline asm
	cp.async.cg.shared.global [%r21], [%rd23], 16;
	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	add.s64 	%rd33, %rd33, 8192;
	add.s32 	%r982, %r982, 32;
	add.s32 	%r983, %r983, 1;
	setp.ne.s32 	%p32, %r983, 36;
	@%p32 bra 	$L__BB0_1;

	mov.u32 	%r981, %tid.x;
	and.b32  	%r980, %r981, 32;
	shr.u32 	%r979, %r981, 4;
	shl.b32 	%r978, %r981, 9;
	and.b32  	%r977, %r978, 4096;
	ld.param.u64 	%rd32, [main_kernel_param_2];
	add.s32 	%r976, %r979, %r981;
	and.b32  	%r975, %r981, 2;
	shr.u32 	%r974, %r975, 1;
	shr.u32 	%r973, %r980, 5;
	add.s32 	%r972, %r973, %r974;
	mov.u32 	%r971, buf_dyn_shmem;
	and.b32  	%r970, %r981, 4;
	shr.u32 	%r969, %r981, 6;
	shr.u32 	%r968, %r970, 2;
	add.s32 	%r967, %r968, %r969;
	add.s32 	%r966, %r2, 6144;
	add.s32 	%r965, %r2, 2048;
	add.s32 	%r964, %r2, 4096;
	add.s32 	%r963, %r1, 2048;
	// begin inline asm
	cp.async.wait_all;

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r517, %r518, %r519, %r520}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r522, %r523, %r524, %r525}, [%r963];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r527, %r528, %r529, %r530}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r532, %r533, %r534, %r535}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r537, %r538, %r539, %r540}, [%r964];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r542, %r543, %r544, %r545}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f449,  %f450,  %f451,  %f452},{%r517,  %r518,  %r519,  %r520},{%r527,  %r528},{%f960, %f959, %f958, %f957};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f457,  %f458,  %f459,  %f460},{%r522,  %r523,  %r524,  %r525},{%r527,  %r528},{%f956, %f955, %f954, %f953};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f465,  %f466,  %f467,  %f468},{%r522,  %r523,  %r524,  %r525},{%r529,  %r530},{%f948, %f947, %f946, %f945};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f473,  %f474,  %f475,  %f476},{%r517,  %r518,  %r519,  %r520},{%r529,  %r530},{%f952, %f951, %f950, %f949};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f481,  %f482,  %f483,  %f484},{%r517,  %r518,  %r519,  %r520},{%r532,  %r533},{%f944, %f943, %f942, %f941};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f489,  %f490,  %f491,  %f492},{%r522,  %r523,  %r524,  %r525},{%r532,  %r533},{%f940, %f939, %f938, %f937};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f497,  %f498,  %f499,  %f500},{%r522,  %r523,  %r524,  %r525},{%r534,  %r535},{%f932, %f931, %f930, %f929};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f505,  %f506,  %f507,  %f508},{%r517,  %r518,  %r519,  %r520},{%r534,  %r535},{%f936, %f935, %f934, %f933};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f513,  %f514,  %f515,  %f516},{%r517,  %r518,  %r519,  %r520},{%r537,  %r538},{%f928, %f927, %f926, %f925};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f521,  %f522,  %f523,  %f524},{%r522,  %r523,  %r524,  %r525},{%r537,  %r538},{%f924, %f923, %f922, %f921};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f529,  %f530,  %f531,  %f532},{%r522,  %r523,  %r524,  %r525},{%r539,  %r540},{%f916, %f915, %f914, %f913};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f537,  %f538,  %f539,  %f540},{%r517,  %r518,  %r519,  %r520},{%r539,  %r540},{%f920, %f919, %f918, %f917};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f545,  %f546,  %f547,  %f548},{%r517,  %r518,  %r519,  %r520},{%r542,  %r543},{%f912, %f911, %f910, %f909};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f553,  %f554,  %f555,  %f556},{%r522,  %r523,  %r524,  %r525},{%r542,  %r543},{%f908, %f907, %f906, %f905};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f561,  %f562,  %f563,  %f564},{%r522,  %r523,  %r524,  %r525},{%r544,  %r545},{%f900, %f899, %f898, %f897};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f569,  %f570,  %f571,  %f572},{%r517,  %r518,  %r519,  %r520},{%r544,  %r545},{%f904, %f903, %f902, %f901};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r643, %r644, %r645, %r646}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r648, %r649, %r650, %r651}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r653, %r654, %r655, %r656}, [%r965];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r658, %r659, %r660, %r661}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r663, %r664, %r665, %r666}, [%r966];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r668, %r669, %r670, %r671}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f577,  %f578,  %f579,  %f580},{%r643,  %r644,  %r645,  %r646},{%r653,  %r654},{%f449, %f450, %f451, %f452};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f585,  %f586,  %f587,  %f588},{%r648,  %r649,  %r650,  %r651},{%r653,  %r654},{%f457, %f458, %f459, %f460};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f593,  %f594,  %f595,  %f596},{%r648,  %r649,  %r650,  %r651},{%r655,  %r656},{%f465, %f466, %f467, %f468};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f601,  %f602,  %f603,  %f604},{%r643,  %r644,  %r645,  %r646},{%r655,  %r656},{%f473, %f474, %f475, %f476};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f609,  %f610,  %f611,  %f612},{%r643,  %r644,  %r645,  %r646},{%r658,  %r659},{%f481, %f482, %f483, %f484};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f617,  %f618,  %f619,  %f620},{%r648,  %r649,  %r650,  %r651},{%r658,  %r659},{%f489, %f490, %f491, %f492};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f625,  %f626,  %f627,  %f628},{%r648,  %r649,  %r650,  %r651},{%r660,  %r661},{%f497, %f498, %f499, %f500};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f633,  %f634,  %f635,  %f636},{%r643,  %r644,  %r645,  %r646},{%r660,  %r661},{%f505, %f506, %f507, %f508};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f641,  %f642,  %f643,  %f644},{%r643,  %r644,  %r645,  %r646},{%r663,  %r664},{%f513, %f514, %f515, %f516};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f649,  %f650,  %f651,  %f652},{%r648,  %r649,  %r650,  %r651},{%r663,  %r664},{%f521, %f522, %f523, %f524};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f657,  %f658,  %f659,  %f660},{%r648,  %r649,  %r650,  %r651},{%r665,  %r666},{%f529, %f530, %f531, %f532};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f665,  %f666,  %f667,  %f668},{%r643,  %r644,  %r645,  %r646},{%r665,  %r666},{%f537, %f538, %f539, %f540};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f673,  %f674,  %f675,  %f676},{%r643,  %r644,  %r645,  %r646},{%r668,  %r669},{%f545, %f546, %f547, %f548};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f681,  %f682,  %f683,  %f684},{%r648,  %r649,  %r650,  %r651},{%r668,  %r669},{%f553, %f554, %f555, %f556};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f689,  %f690,  %f691,  %f692},{%r648,  %r649,  %r650,  %r651},{%r670,  %r671},{%f561, %f562, %f563, %f564};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f697,  %f698,  %f699,  %f700},{%r643,  %r644,  %r645,  %r646},{%r670,  %r671},{%f569, %f570, %f571, %f572};

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f577;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f706, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f578;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f709, %rs4;}

	// end inline asm
	shr.u32 	%r770, %r981, 8;
	and.b32  	%r772, %r45, 2147479552;
	shl.b32 	%r773, %r981, 4;
	and.b32  	%r774, %r773, 448;
	add.s32 	%r776, %r770, %r979;
	and.b32  	%r777, %r776, 1;
	shl.b32 	%r782, %r967, 3;
	and.b32  	%r783, %r782, 8;
	shl.b32 	%r784, %r981, 1;
	and.b32  	%r785, %r784, 6;
	or.b32  	%r787, %r777, %r980;
	shl.b32 	%r788, %r787, 5;
	or.b32  	%r789, %r774, %r785;
	or.b32  	%r790, %r789, %r783;
	or.b32  	%r791, %r790, %r772;
	and.b32  	%r792, %r784, 16;
	or.b32  	%r793, %r791, %r792;
	or.b32  	%r794, %r793, %r788;
	shl.b32 	%r795, %r794, 1;
	add.s32 	%r797, %r971, %r795;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f709;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f706;}

	// end inline asm
	st.shared.v2.u16 	[%r797], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f579;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f712, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f580;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f715, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f715;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f712;}

	// end inline asm
	st.shared.v2.u16 	[%r797+1024], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f585;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f718, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f586;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f721, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f721;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f718;}

	// end inline asm
	st.shared.v2.u16 	[%r797+4096], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f587;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f724, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f588;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f727, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f727;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f724;}

	// end inline asm
	st.shared.v2.u16 	[%r797+5120], {%rs21, %rs24};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f601;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f730, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f602;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f733, %rs28;}

	// end inline asm
	shr.s32 	%r798, %r981, 6;
	shl.b32 	%r799, %r798, 3;
	add.s32 	%r800, %r799, 16;
	shl.b32 	%r801, %r800, 6;
	and.b32  	%r802, %r801, 2147479552;
	shr.u32 	%r803, %r800, 5;
	add.s32 	%r804, %r803, %r979;
	and.b32  	%r805, %r804, 1;
	xor.b32  	%r806, %r792, 16;
	or.b32  	%r807, %r805, %r980;
	shl.b32 	%r808, %r807, 5;
	or.b32  	%r809, %r790, %r802;
	or.b32  	%r810, %r809, %r806;
	or.b32  	%r811, %r810, %r808;
	shl.b32 	%r812, %r811, 1;
	add.s32 	%r813, %r971, %r812;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f733;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f730;}

	// end inline asm
	st.shared.v2.u16 	[%r813], {%rs27, %rs30};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f603;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f736, %rs31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f604;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f739, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f739;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f736;}

	// end inline asm
	st.shared.v2.u16 	[%r813+1024], {%rs33, %rs36};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f593;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f742, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f594;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f745, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f745;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f742;}

	// end inline asm
	st.shared.v2.u16 	[%r813+4096], {%rs39, %rs42};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f595;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f748, %rs43;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f596;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f751, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f751;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f748;}

	// end inline asm
	st.shared.v2.u16 	[%r813+5120], {%rs45, %rs48};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs49, %f609;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f754, %rs49;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs52, %f610;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f757, %rs52;}

	// end inline asm
	add.s32 	%r814, %r799, 32;
	shl.b32 	%r815, %r814, 6;
	and.b32  	%r816, %r815, 2147479552;
	shr.u32 	%r817, %r814, 5;
	add.s32 	%r818, %r817, %r979;
	and.b32  	%r819, %r818, 1;
	or.b32  	%r820, %r819, %r980;
	shl.b32 	%r821, %r820, 5;
	or.b32  	%r822, %r790, %r816;
	or.b32  	%r823, %r822, %r792;
	or.b32  	%r824, %r823, %r821;
	shl.b32 	%r825, %r824, 1;
	add.s32 	%r826, %r971, %r825;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs54, %f757;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs51, %f754;}

	// end inline asm
	st.shared.v2.u16 	[%r826], {%rs51, %rs54};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs55, %f611;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f760, %rs55;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs58, %f612;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f763, %rs58;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f763;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs57, %f760;}

	// end inline asm
	st.shared.v2.u16 	[%r826+1024], {%rs57, %rs60};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs61, %f617;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f766, %rs61;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs64, %f618;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f769, %rs64;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f769;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f766;}

	// end inline asm
	st.shared.v2.u16 	[%r826+4096], {%rs63, %rs66};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs67, %f619;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f772, %rs67;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f620;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f775, %rs70;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f775;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f772;}

	// end inline asm
	st.shared.v2.u16 	[%r826+5120], {%rs69, %rs72};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f633;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f778, %rs73;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs76, %f634;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f781, %rs76;}

	// end inline asm
	add.s32 	%r827, %r799, 48;
	shl.b32 	%r828, %r827, 6;
	and.b32  	%r829, %r828, 2147479552;
	shr.u32 	%r830, %r827, 5;
	add.s32 	%r831, %r830, %r979;
	and.b32  	%r832, %r831, 1;
	or.b32  	%r833, %r832, %r980;
	shl.b32 	%r834, %r833, 5;
	or.b32  	%r835, %r790, %r829;
	or.b32  	%r836, %r835, %r806;
	or.b32  	%r837, %r836, %r834;
	shl.b32 	%r838, %r837, 1;
	add.s32 	%r839, %r971, %r838;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f781;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f778;}

	// end inline asm
	st.shared.v2.u16 	[%r839], {%rs75, %rs78};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f635;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f784, %rs79;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs82, %f636;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f787, %rs82;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f787;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f784;}

	// end inline asm
	st.shared.v2.u16 	[%r839+1024], {%rs81, %rs84};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs85, %f625;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f790, %rs85;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs88, %f626;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f793, %rs88;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f793;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f790;}

	// end inline asm
	st.shared.v2.u16 	[%r839+4096], {%rs87, %rs90};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs91, %f627;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f796, %rs91;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs94, %f628;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f799, %rs94;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs96, %f799;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f796;}

	// end inline asm
	st.shared.v2.u16 	[%r839+5120], {%rs93, %rs96};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs97, %f641;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f802, %rs97;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs100, %f642;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f805, %rs100;}

	// end inline asm
	shl.b32 	%r840, %r798, 9;
	add.s32 	%r841, %r840, 4096;
	and.b32  	%r842, %r841, 2147479552;
	shr.u32 	%r843, %r798, 2;
	add.s32 	%r844, %r843, %r979;
	and.b32  	%r845, %r844, 1;
	or.b32  	%r846, %r845, %r980;
	shl.b32 	%r847, %r846, 5;
	or.b32  	%r848, %r790, %r842;
	or.b32  	%r849, %r848, %r792;
	or.b32  	%r850, %r849, %r847;
	shl.b32 	%r851, %r850, 1;
	add.s32 	%r852, %r971, %r851;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f805;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs99, %f802;}

	// end inline asm
	st.shared.v2.u16 	[%r852], {%rs99, %rs102};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs103, %f643;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f808, %rs103;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs106, %f644;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f811, %rs106;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f811;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs105, %f808;}

	// end inline asm
	st.shared.v2.u16 	[%r852+1024], {%rs105, %rs108};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs109, %f649;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f814, %rs109;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs112, %f650;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f817, %rs112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f817;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs111, %f814;}

	// end inline asm
	st.shared.v2.u16 	[%r852+4096], {%rs111, %rs114};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs115, %f651;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f820, %rs115;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs118, %f652;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f823, %rs118;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f823;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f820;}

	// end inline asm
	st.shared.v2.u16 	[%r852+5120], {%rs117, %rs120};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs121, %f665;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f826, %rs121;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs124, %f666;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f829, %rs124;}

	// end inline asm
	add.s32 	%r853, %r840, 5120;
	and.b32  	%r854, %r853, 2147479552;
	or.b32  	%r855, %r790, %r854;
	or.b32  	%r856, %r855, %r806;
	or.b32  	%r857, %r856, %r808;
	shl.b32 	%r858, %r857, 1;
	add.s32 	%r859, %r971, %r858;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f829;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs123, %f826;}

	// end inline asm
	st.shared.v2.u16 	[%r859], {%rs123, %rs126};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs127, %f667;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f832, %rs127;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs130, %f668;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f835, %rs130;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs132, %f835;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs129, %f832;}

	// end inline asm
	st.shared.v2.u16 	[%r859+1024], {%rs129, %rs132};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs133, %f657;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f838, %rs133;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs136, %f658;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f841, %rs136;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs138, %f841;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs135, %f838;}

	// end inline asm
	st.shared.v2.u16 	[%r859+4096], {%rs135, %rs138};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs139, %f659;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f844, %rs139;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs142, %f660;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f847, %rs142;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs144, %f847;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs141, %f844;}

	// end inline asm
	st.shared.v2.u16 	[%r859+5120], {%rs141, %rs144};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs145, %f673;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f850, %rs145;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs148, %f674;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f853, %rs148;}

	// end inline asm
	add.s32 	%r860, %r840, 6144;
	and.b32  	%r861, %r860, 2147479552;
	or.b32  	%r862, %r790, %r861;
	or.b32  	%r863, %r862, %r792;
	or.b32  	%r864, %r863, %r821;
	shl.b32 	%r865, %r864, 1;
	add.s32 	%r866, %r971, %r865;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs150, %f853;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs147, %f850;}

	// end inline asm
	st.shared.v2.u16 	[%r866], {%rs147, %rs150};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs151, %f675;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f856, %rs151;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs154, %f676;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f859, %rs154;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs156, %f859;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs153, %f856;}

	// end inline asm
	st.shared.v2.u16 	[%r866+1024], {%rs153, %rs156};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs157, %f681;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f862, %rs157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs160, %f682;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f865, %rs160;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs162, %f865;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs159, %f862;}

	// end inline asm
	st.shared.v2.u16 	[%r866+4096], {%rs159, %rs162};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs163, %f683;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f868, %rs163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs166, %f684;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f871, %rs166;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs168, %f871;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs165, %f868;}

	// end inline asm
	st.shared.v2.u16 	[%r866+5120], {%rs165, %rs168};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs169, %f697;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f874, %rs169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs172, %f698;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f877, %rs172;}

	// end inline asm
	add.s32 	%r867, %r840, 7168;
	and.b32  	%r868, %r867, 2147479552;
	or.b32  	%r869, %r790, %r868;
	or.b32  	%r870, %r869, %r806;
	or.b32  	%r871, %r870, %r834;
	shl.b32 	%r872, %r871, 1;
	add.s32 	%r873, %r971, %r872;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs174, %f877;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs171, %f874;}

	// end inline asm
	st.shared.v2.u16 	[%r873], {%rs171, %rs174};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs175, %f699;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f880, %rs175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f700;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f883, %rs178;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f883;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f880;}

	// end inline asm
	st.shared.v2.u16 	[%r873+1024], {%rs177, %rs180};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f689;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f886, %rs181;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs184, %f690;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f889, %rs184;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs186, %f889;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs183, %f886;}

	// end inline asm
	st.shared.v2.u16 	[%r873+4096], {%rs183, %rs186};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs187, %f691;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f892, %rs187;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs190, %f692;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f895, %rs190;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs192, %f895;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs189, %f892;}

	// end inline asm
	st.shared.v2.u16 	[%r873+5120], {%rs189, %rs192};
	bar.sync 	0;
	add.s32 	%r876, %r45, %r43;
	shl.b32 	%r877, %r979, 6;
	shl.b32 	%r878, %r967, 5;
	and.b32  	%r879, %r878, 32;
	shl.b32 	%r884, %r972, 4;
	and.b32  	%r885, %r884, 16;
	shl.b32 	%r887, %r976, 3;
	and.b32  	%r888, %r887, 8;
	or.b32  	%r889, %r888, %r877;
	or.b32  	%r890, %r889, %r879;
	or.b32  	%r891, %r890, %r885;
	cvta.to.global.u64 	%rd29, %rd32;
	mul.wide.s32 	%rd30, %r876, 2;
	add.s64 	%rd31, %rd29, %rd30;
	add.s32 	%r894, %r891, %r977;
	shl.b32 	%r895, %r894, 1;
	add.s32 	%r896, %r971, %r895;
	ld.shared.v4.u32 	{%r897, %r898, %r899, %r900}, [%r896];
	st.global.v4.u32 	[%rd31], {%r897, %r898, %r899, %r900};
	ld.shared.v4.u32 	{%r905, %r906, %r907, %r908}, [%r896+1024];
	st.global.v4.u32 	[%rd31+2048], {%r905, %r906, %r907, %r908};
	ld.shared.v4.u32 	{%r913, %r914, %r915, %r916}, [%r896+2048];
	st.global.v4.u32 	[%rd31+4096], {%r913, %r914, %r915, %r916};
	ld.shared.v4.u32 	{%r921, %r922, %r923, %r924}, [%r896+3072];
	st.global.v4.u32 	[%rd31+6144], {%r921, %r922, %r923, %r924};
	ld.shared.v4.u32 	{%r929, %r930, %r931, %r932}, [%r896+4096];
	st.global.v4.u32 	[%rd31+8192], {%r929, %r930, %r931, %r932};
	ld.shared.v4.u32 	{%r937, %r938, %r939, %r940}, [%r896+5120];
	st.global.v4.u32 	[%rd31+10240], {%r937, %r938, %r939, %r940};
	ld.shared.v4.u32 	{%r945, %r946, %r947, %r948}, [%r896+6144];
	st.global.v4.u32 	[%rd31+12288], {%r945, %r946, %r947, %r948};
	ld.shared.v4.u32 	{%r953, %r954, %r955, %r956}, [%r896+7168];
	st.global.v4.u32 	[%rd31+14336], {%r953, %r954, %r955, %r956};
	ret;

}

