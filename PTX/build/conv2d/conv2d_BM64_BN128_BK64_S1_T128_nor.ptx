//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN68_INTERNAL_ca94ecb1_37_conv2d_BM64_BN128_BK64_S1_T128_nor_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<65>;
	.reg .b16 	%rs<193>;
	.reg .f32 	%f<1473>;
	.reg .b32 	%r<1593>;
	.reg .b64 	%rd<53>;


	ld.param.u64 	%rd4, [main_kernel_param_0];
	ld.param.u64 	%rd52, [main_kernel_param_1];
	mov.u32 	%r39, %ctaid.y;
	and.b32  	%r40, %r39, 63;
	setp.ne.s32 	%p9, %r40, 0;
	mov.u32 	%r41, %tid.x;
	shl.b32 	%r42, %r41, 4;
	and.b32  	%r43, %r42, -128;
	and.b32  	%r44, %r41, 32;
	shr.u32 	%r45, %r44, 5;
	and.b32  	%r46, %r41, 4;
	shr.u32 	%r47, %r46, 2;
	add.s32 	%r48, %r45, %r47;
	mov.u32 	%r1592, 1;
	shl.b32 	%r49, %r48, 6;
	and.b32  	%r50, %r49, 64;
	and.b32  	%r51, %r41, 2;
	shr.u32 	%r52, %r51, 1;
	shr.u32 	%r53, %r41, 4;
	add.s32 	%r54, %r52, %r53;
	shl.b32 	%r55, %r54, 5;
	and.b32  	%r56, %r55, 32;
	and.b32  	%r57, %r41, 8;
	shr.u32 	%r58, %r57, 3;
	add.s32 	%r59, %r58, %r41;
	shl.b32 	%r60, %r59, 4;
	and.b32  	%r61, %r60, 16;
	or.b32  	%r62, %r43, %r50;
	add.s32 	%r63, %r62, 16384;
	or.b32  	%r64, %r63, %r56;
	or.b32  	%r65, %r64, %r61;
	shl.b32 	%r66, %r39, 13;
	shl.b32 	%r67, %r41, 3;
	and.b32  	%r68, %r67, 56;
	add.s32 	%r69, %r66, %r43;
	add.s32 	%r70, %r69, -8320;
	or.b32  	%r71, %r70, %r68;
	setp.gt.s32 	%p10, %r41, 7;
	and.pred  	%p11, %p9, %p10;
	mul.wide.s32 	%rd19, %r71, 2;
	add.s64 	%rd7, %rd4, %rd19;
	selp.b32 	%r23, 16, 0, %p11;
	mov.u32 	%r72, buf_dyn_shmem;
	add.s32 	%r22, %r72, %r65;
	// begin inline asm
	cp.async.cg.shared.global [%r22], [%rd7], 16, %r23;
	// end inline asm
	setp.gt.s32 	%p12, %r41, -121;
	and.pred  	%p13, %p9, %p12;
	add.s32 	%r73, %r71, 2048;
	mul.wide.s32 	%rd20, %r73, 2;
	add.s64 	%rd8, %rd4, %rd20;
	selp.b32 	%r25, 16, 0, %p13;
	add.s32 	%r24, %r22, 2048;
	// begin inline asm
	cp.async.cg.shared.global [%r24], [%rd8], 16, %r25;
	// end inline asm
	setp.gt.s32 	%p14, %r41, -249;
	and.pred  	%p15, %p9, %p14;
	add.s32 	%r74, %r71, 4096;
	mul.wide.s32 	%rd21, %r74, 2;
	add.s64 	%rd9, %rd4, %rd21;
	selp.b32 	%r27, 16, 0, %p15;
	add.s32 	%r26, %r22, 4096;
	// begin inline asm
	cp.async.cg.shared.global [%r26], [%rd9], 16, %r27;
	// end inline asm
	setp.gt.s32 	%p16, %r41, -377;
	and.pred  	%p17, %p9, %p16;
	add.s32 	%r75, %r71, 6144;
	mul.wide.s32 	%rd22, %r75, 2;
	add.s64 	%rd10, %rd4, %rd22;
	selp.b32 	%r29, 16, 0, %p17;
	add.s32 	%r28, %r22, 6144;
	// begin inline asm
	cp.async.cg.shared.global [%r28], [%rd10], 16, %r29;
	// end inline asm
	shl.b32 	%r76, %r53, 7;
	shr.u32 	%r77, %r41, 6;
	add.s32 	%r78, %r47, %r77;
	shl.b32 	%r79, %r78, 6;
	and.b32  	%r80, %r79, 64;
	add.s32 	%r81, %r45, %r52;
	shl.b32 	%r82, %r81, 5;
	and.b32  	%r83, %r82, 32;
	add.s32 	%r84, %r53, %r41;
	shl.b32 	%r85, %r84, 4;
	and.b32  	%r86, %r85, 16;
	or.b32  	%r87, %r86, %r76;
	or.b32  	%r88, %r87, %r80;
	or.b32  	%r89, %r88, %r83;
	shl.b32 	%r90, %r41, 10;
	and.b32  	%r91, %r90, 8192;
	add.s32 	%r92, %r89, %r91;
	mul.wide.s32 	%rd23, %r67, 2;
	add.s64 	%rd11, %rd52, %rd23;
	add.s32 	%r30, %r72, %r92;
	// begin inline asm
	cp.async.cg.shared.global [%r30], [%rd11], 16;
	// end inline asm
	add.s64 	%rd12, %rd11, 2048;
	add.s32 	%r31, %r30, 1024;
	// begin inline asm
	cp.async.cg.shared.global [%r31], [%rd12], 16;
	// end inline asm
	add.s64 	%rd13, %rd11, 4096;
	add.s32 	%r32, %r30, 2048;
	// begin inline asm
	cp.async.cg.shared.global [%r32], [%rd13], 16;
	// end inline asm
	add.s64 	%rd14, %rd11, 6144;
	add.s32 	%r33, %r30, 3072;
	// begin inline asm
	cp.async.cg.shared.global [%r33], [%rd14], 16;
	// end inline asm
	add.s64 	%rd15, %rd11, 8192;
	add.s32 	%r34, %r30, 4096;
	// begin inline asm
	cp.async.cg.shared.global [%r34], [%rd15], 16;
	// end inline asm
	add.s64 	%rd16, %rd11, 10240;
	add.s32 	%r35, %r30, 5120;
	// begin inline asm
	cp.async.cg.shared.global [%r35], [%rd16], 16;
	// end inline asm
	add.s64 	%rd17, %rd11, 12288;
	add.s32 	%r36, %r30, 6144;
	// begin inline asm
	cp.async.cg.shared.global [%r36], [%rd17], 16;
	// end inline asm
	add.s64 	%rd18, %rd11, 14336;
	add.s32 	%r37, %r30, 7168;
	// begin inline asm
	cp.async.cg.shared.global [%r37], [%rd18], 16;
	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	shr.s32 	%r93, %r41, 31;
	shr.u32 	%r94, %r93, 28;
	add.s32 	%r95, %r41, %r94;
	and.b32  	%r96, %r95, -16;
	sub.s32 	%r97, %r41, %r96;
	shr.s32 	%r98, %r97, 31;
	shr.u32 	%r99, %r98, 29;
	add.s32 	%r100, %r97, %r99;
	and.b32  	%r101, %r100, -8;
	sub.s32 	%r102, %r97, %r101;
	shr.u32 	%r103, %r95, 31;
	shr.s32 	%r104, %r95, 4;
	add.s32 	%r105, %r104, %r103;
	and.b32  	%r106, %r105, -2;
	sub.s32 	%r107, %r104, %r106;
	shl.b32 	%r108, %r102, 6;
	and.b32  	%r109, %r108, 448;
	shl.b32 	%r110, %r107, 3;
	and.b32  	%r111, %r110, 8;
	or.b32  	%r112, %r109, %r111;
	shl.b32 	%r113, %r100, 6;
	and.b32  	%r114, %r113, -512;
	shr.u32 	%r115, %r93, 27;
	add.s32 	%r116, %r41, %r115;
	shr.u32 	%r117, %r116, 31;
	shr.s32 	%r118, %r116, 5;
	add.s32 	%r119, %r118, %r117;
	and.b32  	%r120, %r119, 2097150;
	sub.s32 	%r121, %r118, %r120;
	shl.b32 	%r122, %r121, 10;
	and.b32  	%r123, %r102, 2;
	setp.eq.s32 	%p18, %r123, 0;
	and.b32  	%r124, %r102, 4;
	setp.eq.s32 	%p19, %r124, 0;
	shr.u32 	%r125, %r109, 3;
	xor.b32  	%r126, %r112, %r125;
	shr.u32 	%r127, %r93, 26;
	add.s32 	%r128, %r41, %r127;
	shl.b32 	%r129, %r107, 4;
	and.b32  	%r130, %r129, 16;
	shr.u32 	%r131, %r128, 3;
	and.b32  	%r132, %r131, 8;
	or.b32  	%r133, %r130, %r132;
	or.b32  	%r134, %r133, %r109;
	xor.b32  	%r135, %r134, %r125;
	add.s32 	%r136, %r114, %r122;
	or.b32  	%r137, %r136, %r126;
	shl.b32 	%r138, %r137, 1;
	add.s32 	%r139, %r72, %r138;
	add.s32 	%r1, %r139, 16384;
	or.b32  	%r140, %r135, %r114;
	shl.b32 	%r141, %r140, 1;
	add.s32 	%r2, %r72, %r141;
	selp.b32 	%r142, 64, -64, %p19;
	add.s32 	%r3, %r2, %r142;
	add.s32 	%r4, %r3, 8192;
	selp.b32 	%r143, 32, -32, %p18;
	add.s32 	%r5, %r1, %r143;
	add.s32 	%r6, %r5, 4096;
	add.s32 	%r7, %r3, 2048;
	add.s32 	%r8, %r3, 10240;
	add.s32 	%r9, %r1, %r142;
	add.s32 	%r10, %r9, 4096;
	add.s32 	%r11, %r3, 4096;
	add.s32 	%r12, %r3, 12288;
	add.s32 	%r13, %r9, %r143;
	add.s32 	%r14, %r3, 6144;
	add.s32 	%r15, %r3, 14336;
	or.b32  	%r144, %r68, %r66;
	add.s32 	%r1591, %r144, %r43;
	mov.f32 	%f1409, 0f00000000;
	mov.f32 	%f1410, %f1409;
	mov.f32 	%f1411, %f1409;
	mov.f32 	%f1412, %f1409;
	mov.f32 	%f1413, %f1409;
	mov.f32 	%f1414, %f1409;
	mov.f32 	%f1415, %f1409;
	mov.f32 	%f1416, %f1409;
	mov.f32 	%f1417, %f1409;
	mov.f32 	%f1418, %f1409;
	mov.f32 	%f1419, %f1409;
	mov.f32 	%f1420, %f1409;
	mov.f32 	%f1421, %f1409;
	mov.f32 	%f1422, %f1409;
	mov.f32 	%f1423, %f1409;
	mov.f32 	%f1424, %f1409;
	mov.f32 	%f1425, %f1409;
	mov.f32 	%f1426, %f1409;
	mov.f32 	%f1427, %f1409;
	mov.f32 	%f1428, %f1409;
	mov.f32 	%f1429, %f1409;
	mov.f32 	%f1430, %f1409;
	mov.f32 	%f1431, %f1409;
	mov.f32 	%f1432, %f1409;
	mov.f32 	%f1433, %f1409;
	mov.f32 	%f1434, %f1409;
	mov.f32 	%f1435, %f1409;
	mov.f32 	%f1436, %f1409;
	mov.f32 	%f1437, %f1409;
	mov.f32 	%f1438, %f1409;
	mov.f32 	%f1439, %f1409;
	mov.f32 	%f1440, %f1409;
	mov.f32 	%f1441, %f1409;
	mov.f32 	%f1442, %f1409;
	mov.f32 	%f1443, %f1409;
	mov.f32 	%f1444, %f1409;
	mov.f32 	%f1445, %f1409;
	mov.f32 	%f1446, %f1409;
	mov.f32 	%f1447, %f1409;
	mov.f32 	%f1448, %f1409;
	mov.f32 	%f1449, %f1409;
	mov.f32 	%f1450, %f1409;
	mov.f32 	%f1451, %f1409;
	mov.f32 	%f1452, %f1409;
	mov.f32 	%f1453, %f1409;
	mov.f32 	%f1454, %f1409;
	mov.f32 	%f1455, %f1409;
	mov.f32 	%f1456, %f1409;
	mov.f32 	%f1457, %f1409;
	mov.f32 	%f1458, %f1409;
	mov.f32 	%f1459, %f1409;
	mov.f32 	%f1460, %f1409;
	mov.f32 	%f1461, %f1409;
	mov.f32 	%f1462, %f1409;
	mov.f32 	%f1463, %f1409;
	mov.f32 	%f1464, %f1409;
	mov.f32 	%f1465, %f1409;
	mov.f32 	%f1466, %f1409;
	mov.f32 	%f1467, %f1409;
	mov.f32 	%f1468, %f1409;
	mov.f32 	%f1469, %f1409;
	mov.f32 	%f1470, %f1409;
	mov.f32 	%f1471, %f1409;
	mov.f32 	%f1472, %f1409;

$L__BB0_1:
	add.s32 	%r547, %r2, 14336;
	add.s32 	%r537, %r2, 6144;
	add.s32 	%r532, %r13, 4096;
	add.s32 	%r421, %r2, 12288;
	add.s32 	%r411, %r2, 4096;
	add.s32 	%r295, %r2, 10240;
	add.s32 	%r285, %r2, 2048;
	add.s32 	%r169, %r2, 8192;
	add.s32 	%r154, %r1, 4096;
	mul.wide.u32 	%rd24, %r1592, -1431655765;
	shr.u64 	%rd25, %rd24, 34;
	cvt.u32.u64 	%r19, %rd25;
	// begin inline asm
	cp.async.wait_all;

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r145, %r146, %r147, %r148}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r150, %r151, %r152, %r153}, [%r154];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r155, %r156, %r157, %r158}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r160, %r161, %r162, %r163}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r165, %r166, %r167, %r168}, [%r169];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r170, %r171, %r172, %r173}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f193,  %f194,  %f195,  %f196},{%r145,  %r146,  %r147,  %r148},{%r155,  %r156},{%f1472, %f1471, %f1470, %f1469};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f201,  %f202,  %f203,  %f204},{%r150,  %r151,  %r152,  %r153},{%r155,  %r156},{%f1468, %f1467, %f1466, %f1465};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f209,  %f210,  %f211,  %f212},{%r150,  %r151,  %r152,  %r153},{%r157,  %r158},{%f1460, %f1459, %f1458, %f1457};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f217,  %f218,  %f219,  %f220},{%r145,  %r146,  %r147,  %r148},{%r157,  %r158},{%f1464, %f1463, %f1462, %f1461};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f225,  %f226,  %f227,  %f228},{%r145,  %r146,  %r147,  %r148},{%r160,  %r161},{%f1456, %f1455, %f1454, %f1453};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f233,  %f234,  %f235,  %f236},{%r150,  %r151,  %r152,  %r153},{%r160,  %r161},{%f1452, %f1451, %f1450, %f1449};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f241,  %f242,  %f243,  %f244},{%r150,  %r151,  %r152,  %r153},{%r162,  %r163},{%f1444, %f1443, %f1442, %f1441};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f249,  %f250,  %f251,  %f252},{%r145,  %r146,  %r147,  %r148},{%r162,  %r163},{%f1448, %f1447, %f1446, %f1445};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f257,  %f258,  %f259,  %f260},{%r145,  %r146,  %r147,  %r148},{%r165,  %r166},{%f1440, %f1439, %f1438, %f1437};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f265,  %f266,  %f267,  %f268},{%r150,  %r151,  %r152,  %r153},{%r165,  %r166},{%f1436, %f1435, %f1434, %f1433};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f273,  %f274,  %f275,  %f276},{%r150,  %r151,  %r152,  %r153},{%r167,  %r168},{%f1428, %f1427, %f1426, %f1425};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f281,  %f282,  %f283,  %f284},{%r145,  %r146,  %r147,  %r148},{%r167,  %r168},{%f1432, %f1431, %f1430, %f1429};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f289,  %f290,  %f291,  %f292},{%r145,  %r146,  %r147,  %r148},{%r170,  %r171},{%f1424, %f1423, %f1422, %f1421};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f297,  %f298,  %f299,  %f300},{%r150,  %r151,  %r152,  %r153},{%r170,  %r171},{%f1420, %f1419, %f1418, %f1417};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f305,  %f306,  %f307,  %f308},{%r150,  %r151,  %r152,  %r153},{%r172,  %r173},{%f1412, %f1411, %f1410, %f1409};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f313,  %f314,  %f315,  %f316},{%r145,  %r146,  %r147,  %r148},{%r172,  %r173},{%f1416, %f1415, %f1414, %f1413};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r271, %r272, %r273, %r274}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r276, %r277, %r278, %r279}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r281, %r282, %r283, %r284}, [%r285];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r286, %r287, %r288, %r289}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r291, %r292, %r293, %r294}, [%r295];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r296, %r297, %r298, %r299}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f321,  %f322,  %f323,  %f324},{%r271,  %r272,  %r273,  %r274},{%r281,  %r282},{%f193, %f194, %f195, %f196};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f329,  %f330,  %f331,  %f332},{%r276,  %r277,  %r278,  %r279},{%r281,  %r282},{%f201, %f202, %f203, %f204};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f337,  %f338,  %f339,  %f340},{%r276,  %r277,  %r278,  %r279},{%r283,  %r284},{%f209, %f210, %f211, %f212};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f345,  %f346,  %f347,  %f348},{%r271,  %r272,  %r273,  %r274},{%r283,  %r284},{%f217, %f218, %f219, %f220};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f353,  %f354,  %f355,  %f356},{%r271,  %r272,  %r273,  %r274},{%r286,  %r287},{%f225, %f226, %f227, %f228};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f361,  %f362,  %f363,  %f364},{%r276,  %r277,  %r278,  %r279},{%r286,  %r287},{%f233, %f234, %f235, %f236};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f369,  %f370,  %f371,  %f372},{%r276,  %r277,  %r278,  %r279},{%r288,  %r289},{%f241, %f242, %f243, %f244};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f377,  %f378,  %f379,  %f380},{%r271,  %r272,  %r273,  %r274},{%r288,  %r289},{%f249, %f250, %f251, %f252};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f385,  %f386,  %f387,  %f388},{%r271,  %r272,  %r273,  %r274},{%r291,  %r292},{%f257, %f258, %f259, %f260};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f393,  %f394,  %f395,  %f396},{%r276,  %r277,  %r278,  %r279},{%r291,  %r292},{%f265, %f266, %f267, %f268};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f401,  %f402,  %f403,  %f404},{%r276,  %r277,  %r278,  %r279},{%r293,  %r294},{%f273, %f274, %f275, %f276};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f409,  %f410,  %f411,  %f412},{%r271,  %r272,  %r273,  %r274},{%r293,  %r294},{%f281, %f282, %f283, %f284};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f417,  %f418,  %f419,  %f420},{%r271,  %r272,  %r273,  %r274},{%r296,  %r297},{%f289, %f290, %f291, %f292};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f425,  %f426,  %f427,  %f428},{%r276,  %r277,  %r278,  %r279},{%r296,  %r297},{%f297, %f298, %f299, %f300};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f433,  %f434,  %f435,  %f436},{%r276,  %r277,  %r278,  %r279},{%r298,  %r299},{%f305, %f306, %f307, %f308};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f441,  %f442,  %f443,  %f444},{%r271,  %r272,  %r273,  %r274},{%r298,  %r299},{%f313, %f314, %f315, %f316};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r397, %r398, %r399, %r400}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r402, %r403, %r404, %r405}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r407, %r408, %r409, %r410}, [%r411];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r412, %r413, %r414, %r415}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r417, %r418, %r419, %r420}, [%r421];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r422, %r423, %r424, %r425}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f449,  %f450,  %f451,  %f452},{%r397,  %r398,  %r399,  %r400},{%r407,  %r408},{%f321, %f322, %f323, %f324};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f457,  %f458,  %f459,  %f460},{%r402,  %r403,  %r404,  %r405},{%r407,  %r408},{%f329, %f330, %f331, %f332};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f465,  %f466,  %f467,  %f468},{%r402,  %r403,  %r404,  %r405},{%r409,  %r410},{%f337, %f338, %f339, %f340};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f473,  %f474,  %f475,  %f476},{%r397,  %r398,  %r399,  %r400},{%r409,  %r410},{%f345, %f346, %f347, %f348};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f481,  %f482,  %f483,  %f484},{%r397,  %r398,  %r399,  %r400},{%r412,  %r413},{%f353, %f354, %f355, %f356};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f489,  %f490,  %f491,  %f492},{%r402,  %r403,  %r404,  %r405},{%r412,  %r413},{%f361, %f362, %f363, %f364};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f497,  %f498,  %f499,  %f500},{%r402,  %r403,  %r404,  %r405},{%r414,  %r415},{%f369, %f370, %f371, %f372};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f505,  %f506,  %f507,  %f508},{%r397,  %r398,  %r399,  %r400},{%r414,  %r415},{%f377, %f378, %f379, %f380};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f513,  %f514,  %f515,  %f516},{%r397,  %r398,  %r399,  %r400},{%r417,  %r418},{%f385, %f386, %f387, %f388};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f521,  %f522,  %f523,  %f524},{%r402,  %r403,  %r404,  %r405},{%r417,  %r418},{%f393, %f394, %f395, %f396};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f529,  %f530,  %f531,  %f532},{%r402,  %r403,  %r404,  %r405},{%r419,  %r420},{%f401, %f402, %f403, %f404};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f537,  %f538,  %f539,  %f540},{%r397,  %r398,  %r399,  %r400},{%r419,  %r420},{%f409, %f410, %f411, %f412};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f545,  %f546,  %f547,  %f548},{%r397,  %r398,  %r399,  %r400},{%r422,  %r423},{%f417, %f418, %f419, %f420};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f553,  %f554,  %f555,  %f556},{%r402,  %r403,  %r404,  %r405},{%r422,  %r423},{%f425, %f426, %f427, %f428};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f561,  %f562,  %f563,  %f564},{%r402,  %r403,  %r404,  %r405},{%r424,  %r425},{%f433, %f434, %f435, %f436};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f569,  %f570,  %f571,  %f572},{%r397,  %r398,  %r399,  %r400},{%r424,  %r425},{%f441, %f442, %f443, %f444};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r523, %r524, %r525, %r526}, [%r13];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r528, %r529, %r530, %r531}, [%r532];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r533, %r534, %r535, %r536}, [%r537];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r538, %r539, %r540, %r541}, [%r14];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r543, %r544, %r545, %r546}, [%r547];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r548, %r549, %r550, %r551}, [%r15];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1472,  %f1471,  %f1470,  %f1469},{%r523,  %r524,  %r525,  %r526},{%r533,  %r534},{%f449, %f450, %f451, %f452};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1468,  %f1467,  %f1466,  %f1465},{%r528,  %r529,  %r530,  %r531},{%r533,  %r534},{%f457, %f458, %f459, %f460};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1460,  %f1459,  %f1458,  %f1457},{%r528,  %r529,  %r530,  %r531},{%r535,  %r536},{%f465, %f466, %f467, %f468};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1464,  %f1463,  %f1462,  %f1461},{%r523,  %r524,  %r525,  %r526},{%r535,  %r536},{%f473, %f474, %f475, %f476};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1456,  %f1455,  %f1454,  %f1453},{%r523,  %r524,  %r525,  %r526},{%r538,  %r539},{%f481, %f482, %f483, %f484};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1452,  %f1451,  %f1450,  %f1449},{%r528,  %r529,  %r530,  %r531},{%r538,  %r539},{%f489, %f490, %f491, %f492};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1444,  %f1443,  %f1442,  %f1441},{%r528,  %r529,  %r530,  %r531},{%r540,  %r541},{%f497, %f498, %f499, %f500};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1448,  %f1447,  %f1446,  %f1445},{%r523,  %r524,  %r525,  %r526},{%r540,  %r541},{%f505, %f506, %f507, %f508};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1440,  %f1439,  %f1438,  %f1437},{%r523,  %r524,  %r525,  %r526},{%r543,  %r544},{%f513, %f514, %f515, %f516};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1436,  %f1435,  %f1434,  %f1433},{%r528,  %r529,  %r530,  %r531},{%r543,  %r544},{%f521, %f522, %f523, %f524};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1428,  %f1427,  %f1426,  %f1425},{%r528,  %r529,  %r530,  %r531},{%r545,  %r546},{%f529, %f530, %f531, %f532};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1432,  %f1431,  %f1430,  %f1429},{%r523,  %r524,  %r525,  %r526},{%r545,  %r546},{%f537, %f538, %f539, %f540};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1424,  %f1423,  %f1422,  %f1421},{%r523,  %r524,  %r525,  %r526},{%r548,  %r549},{%f545, %f546, %f547, %f548};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1420,  %f1419,  %f1418,  %f1417},{%r528,  %r529,  %r530,  %r531},{%r548,  %r549},{%f553, %f554, %f555, %f556};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1412,  %f1411,  %f1410,  %f1409},{%r528,  %r529,  %r530,  %r531},{%r550,  %r551},{%f561, %f562, %f563, %f564};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1416,  %f1415,  %f1414,  %f1413},{%r523,  %r524,  %r525,  %r526},{%r550,  %r551},{%f569, %f570, %f571, %f572};

	// end inline asm
	bar.sync 	0;
	neg.s32 	%r651, %r40;
	setp.eq.s32 	%p21, %r19, %r651;
	mov.pred 	%p61, 0;
	@%p21 bra 	$L__BB0_3;

	add.s32 	%r654, %r19, %r40;
	setp.lt.u32 	%p22, %r654, 65;
	shr.s32 	%r656, %r41, 3;
	mad.lo.s32 	%r657, %r19, -6, %r1592;
	shr.u32 	%r658, %r657, 1;
	add.s32 	%r659, %r656, %r658;
	add.s32 	%r660, %r659, -1;
	setp.lt.u32 	%p23, %r660, 64;
	and.pred  	%p61, %p22, %p23;

$L__BB0_3:
	ld.param.u64 	%rd50, [main_kernel_param_0];
	add.s32 	%r665, %r19, %r40;
	setp.lt.u32 	%p24, %r665, 65;
	setp.ne.s32 	%p26, %r19, %r651;
	shr.s32 	%r668, %r41, 3;
	shl.b32 	%r669, %r668, 7;
	or.b32  	%r677, %r669, %r50;
	add.s32 	%r678, %r677, 16384;
	or.b32  	%r685, %r678, %r56;
	or.b32  	%r691, %r685, %r61;
	add.s32 	%r661, %r72, %r691;
	mad.lo.s32 	%r693, %r19, 7808, %r1591;
	add.s32 	%r694, %r693, -8256;
	mul.wide.s32 	%rd27, %r694, 2;
	add.s64 	%rd26, %rd50, %rd27;
	mad.lo.s32 	%r695, %r19, -6, %r1592;
	shr.u32 	%r696, %r695, 1;
	add.s32 	%r697, %r668, %r696;
	add.s32 	%r698, %r697, -1;
	setp.lt.u32 	%p27, %r698, 64;
	and.pred  	%p28, %p26, %p27;
	and.pred  	%p29, %p24, %p28;
	and.pred  	%p30, %p61, %p29;
	selp.b32 	%r662, 16, 0, %p30;
	// begin inline asm
	cp.async.cg.shared.global [%r661], [%rd26], 16, %r662;
	// end inline asm
	@%p21 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_4;

$L__BB0_5:
	mov.pred 	%p62, 0;
	bra.uni 	$L__BB0_6;

$L__BB0_4:
	add.s32 	%r707, %r697, 15;
	setp.lt.u32 	%p32, %r707, 64;
	and.pred  	%p62, %p24, %p32;

$L__BB0_6:
	add.s32 	%r708, %r661, 2048;
	add.s32 	%r744, %r697, 15;
	setp.lt.u32 	%p37, %r744, 64;
	and.pred  	%p38, %p26, %p37;
	and.pred  	%p39, %p24, %p38;
	and.pred  	%p40, %p62, %p39;
	selp.b32 	%r709, 16, 0, %p40;
	add.s64 	%rd28, %rd26, 4096;
	// begin inline asm
	cp.async.cg.shared.global [%r708], [%rd28], 16, %r709;
	// end inline asm
	@%p21 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_7;

$L__BB0_8:
	mov.pred 	%p63, 0;
	bra.uni 	$L__BB0_9;

$L__BB0_7:
	add.s32 	%r753, %r697, 31;
	setp.lt.u32 	%p42, %r753, 64;
	and.pred  	%p63, %p24, %p42;

$L__BB0_9:
	add.s32 	%r754, %r661, 4096;
	add.s32 	%r790, %r697, 31;
	setp.lt.u32 	%p47, %r790, 64;
	and.pred  	%p48, %p26, %p47;
	and.pred  	%p49, %p24, %p48;
	and.pred  	%p50, %p63, %p49;
	selp.b32 	%r755, 16, 0, %p50;
	add.s64 	%rd29, %rd26, 8192;
	// begin inline asm
	cp.async.cg.shared.global [%r754], [%rd29], 16, %r755;
	// end inline asm
	@%p21 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_10;

$L__BB0_11:
	mov.pred 	%p64, 0;
	bra.uni 	$L__BB0_12;

$L__BB0_10:
	add.s32 	%r799, %r697, 47;
	setp.lt.u32 	%p52, %r799, 64;
	and.pred  	%p64, %p24, %p52;

$L__BB0_12:
	add.s32 	%r800, %r661, 6144;
	add.s32 	%r861, %r697, 47;
	setp.lt.u32 	%p56, %r861, 64;
	and.pred  	%p57, %p26, %p56;
	and.pred  	%p58, %p24, %p57;
	and.pred  	%p59, %p64, %p58;
	selp.b32 	%r801, 16, 0, %p59;
	add.s64 	%rd30, %rd26, 12288;
	// begin inline asm
	cp.async.cg.shared.global [%r800], [%rd30], 16, %r801;
	// end inline asm
	add.s32 	%r863, %r67, 8192;
	mul.wide.s32 	%rd39, %r863, 2;
	add.s64 	%rd31, %rd52, %rd39;
	// begin inline asm
	cp.async.cg.shared.global [%r30], [%rd31], 16;
	// end inline asm
	add.s32 	%r864, %r67, 9216;
	mul.wide.s32 	%rd40, %r864, 2;
	add.s64 	%rd32, %rd52, %rd40;
	// begin inline asm
	cp.async.cg.shared.global [%r31], [%rd32], 16;
	// end inline asm
	add.s32 	%r865, %r67, 10240;
	mul.wide.s32 	%rd41, %r865, 2;
	add.s64 	%rd33, %rd52, %rd41;
	// begin inline asm
	cp.async.cg.shared.global [%r32], [%rd33], 16;
	// end inline asm
	add.s32 	%r866, %r67, 11264;
	mul.wide.s32 	%rd42, %r866, 2;
	add.s64 	%rd34, %rd52, %rd42;
	// begin inline asm
	cp.async.cg.shared.global [%r33], [%rd34], 16;
	// end inline asm
	add.s32 	%r867, %r67, 12288;
	mul.wide.s32 	%rd43, %r867, 2;
	add.s64 	%rd35, %rd52, %rd43;
	// begin inline asm
	cp.async.cg.shared.global [%r34], [%rd35], 16;
	// end inline asm
	add.s32 	%r868, %r67, 13312;
	mul.wide.s32 	%rd44, %r868, 2;
	add.s64 	%rd36, %rd52, %rd44;
	// begin inline asm
	cp.async.cg.shared.global [%r35], [%rd36], 16;
	// end inline asm
	add.s32 	%r869, %r67, 14336;
	mul.wide.s32 	%rd45, %r869, 2;
	add.s64 	%rd37, %rd52, %rd45;
	// begin inline asm
	cp.async.cg.shared.global [%r36], [%rd37], 16;
	// end inline asm
	add.s32 	%r870, %r67, 15360;
	mul.wide.s32 	%rd46, %r870, 2;
	add.s64 	%rd38, %rd52, %rd46;
	// begin inline asm
	cp.async.cg.shared.global [%r37], [%rd38], 16;
	// end inline asm
	// begin inline asm
	cp.async.commit_group;

	// end inline asm
	add.s64 	%rd52, %rd52, 16384;
	add.s32 	%r1591, %r1591, 64;
	add.s32 	%r1592, %r1592, 1;
	setp.ne.s32 	%p60, %r1592, 18;
	@%p60 bra 	$L__BB0_1;

	and.b32  	%r1590, %r41, 32;
	shr.u32 	%r1589, %r41, 4;
	ld.param.u64 	%rd51, [main_kernel_param_2];
	add.s32 	%r1588, %r1589, %r41;
	and.b32  	%r1587, %r41, 2;
	shr.u32 	%r1586, %r1587, 1;
	shr.u32 	%r1585, %r1590, 5;
	add.s32 	%r1584, %r1585, %r1586;
	mov.u32 	%r1583, %ctaid.y;
	shl.b32 	%r1582, %r1583, 13;
	and.b32  	%r1581, %r41, 4;
	shr.u32 	%r1580, %r41, 6;
	shr.u32 	%r1579, %r1581, 2;
	add.s32 	%r1578, %r1579, %r1580;
	shl.b32 	%r1577, %r41, 4;
	add.s32 	%r1576, %r2, 14336;
	add.s32 	%r1575, %r2, 6144;
	add.s32 	%r1574, %r13, 4096;
	add.s32 	%r1573, %r2, 12288;
	add.s32 	%r1572, %r2, 4096;
	add.s32 	%r1571, %r2, 10240;
	add.s32 	%r1570, %r2, 2048;
	add.s32 	%r1569, %r2, 8192;
	add.s32 	%r1568, %r1, 4096;
	// begin inline asm
	cp.async.wait_all;

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r871, %r872, %r873, %r874}, [%r1];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r876, %r877, %r878, %r879}, [%r1568];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r881, %r882, %r883, %r884}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r886, %r887, %r888, %r889}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r891, %r892, %r893, %r894}, [%r1569];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r896, %r897, %r898, %r899}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f705,  %f706,  %f707,  %f708},{%r871,  %r872,  %r873,  %r874},{%r881,  %r882},{%f1472, %f1471, %f1470, %f1469};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f713,  %f714,  %f715,  %f716},{%r876,  %r877,  %r878,  %r879},{%r881,  %r882},{%f1468, %f1467, %f1466, %f1465};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f721,  %f722,  %f723,  %f724},{%r876,  %r877,  %r878,  %r879},{%r883,  %r884},{%f1460, %f1459, %f1458, %f1457};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f729,  %f730,  %f731,  %f732},{%r871,  %r872,  %r873,  %r874},{%r883,  %r884},{%f1464, %f1463, %f1462, %f1461};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f737,  %f738,  %f739,  %f740},{%r871,  %r872,  %r873,  %r874},{%r886,  %r887},{%f1456, %f1455, %f1454, %f1453};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f745,  %f746,  %f747,  %f748},{%r876,  %r877,  %r878,  %r879},{%r886,  %r887},{%f1452, %f1451, %f1450, %f1449};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f753,  %f754,  %f755,  %f756},{%r876,  %r877,  %r878,  %r879},{%r888,  %r889},{%f1444, %f1443, %f1442, %f1441};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f761,  %f762,  %f763,  %f764},{%r871,  %r872,  %r873,  %r874},{%r888,  %r889},{%f1448, %f1447, %f1446, %f1445};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f769,  %f770,  %f771,  %f772},{%r871,  %r872,  %r873,  %r874},{%r891,  %r892},{%f1440, %f1439, %f1438, %f1437};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f777,  %f778,  %f779,  %f780},{%r876,  %r877,  %r878,  %r879},{%r891,  %r892},{%f1436, %f1435, %f1434, %f1433};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f785,  %f786,  %f787,  %f788},{%r876,  %r877,  %r878,  %r879},{%r893,  %r894},{%f1428, %f1427, %f1426, %f1425};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f793,  %f794,  %f795,  %f796},{%r871,  %r872,  %r873,  %r874},{%r893,  %r894},{%f1432, %f1431, %f1430, %f1429};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f801,  %f802,  %f803,  %f804},{%r871,  %r872,  %r873,  %r874},{%r896,  %r897},{%f1424, %f1423, %f1422, %f1421};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f809,  %f810,  %f811,  %f812},{%r876,  %r877,  %r878,  %r879},{%r896,  %r897},{%f1420, %f1419, %f1418, %f1417};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f817,  %f818,  %f819,  %f820},{%r876,  %r877,  %r878,  %r879},{%r898,  %r899},{%f1412, %f1411, %f1410, %f1409};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f825,  %f826,  %f827,  %f828},{%r871,  %r872,  %r873,  %r874},{%r898,  %r899},{%f1416, %f1415, %f1414, %f1413};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r997, %r998, %r999, %r1000}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1002, %r1003, %r1004, %r1005}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1007, %r1008, %r1009, %r1010}, [%r1570];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1012, %r1013, %r1014, %r1015}, [%r7];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1017, %r1018, %r1019, %r1020}, [%r1571];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1022, %r1023, %r1024, %r1025}, [%r8];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f833,  %f834,  %f835,  %f836},{%r997,  %r998,  %r999,  %r1000},{%r1007,  %r1008},{%f705, %f706, %f707, %f708};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f841,  %f842,  %f843,  %f844},{%r1002,  %r1003,  %r1004,  %r1005},{%r1007,  %r1008},{%f713, %f714, %f715, %f716};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f849,  %f850,  %f851,  %f852},{%r1002,  %r1003,  %r1004,  %r1005},{%r1009,  %r1010},{%f721, %f722, %f723, %f724};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f857,  %f858,  %f859,  %f860},{%r997,  %r998,  %r999,  %r1000},{%r1009,  %r1010},{%f729, %f730, %f731, %f732};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f865,  %f866,  %f867,  %f868},{%r997,  %r998,  %r999,  %r1000},{%r1012,  %r1013},{%f737, %f738, %f739, %f740};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f873,  %f874,  %f875,  %f876},{%r1002,  %r1003,  %r1004,  %r1005},{%r1012,  %r1013},{%f745, %f746, %f747, %f748};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f881,  %f882,  %f883,  %f884},{%r1002,  %r1003,  %r1004,  %r1005},{%r1014,  %r1015},{%f753, %f754, %f755, %f756};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f889,  %f890,  %f891,  %f892},{%r997,  %r998,  %r999,  %r1000},{%r1014,  %r1015},{%f761, %f762, %f763, %f764};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f897,  %f898,  %f899,  %f900},{%r997,  %r998,  %r999,  %r1000},{%r1017,  %r1018},{%f769, %f770, %f771, %f772};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f905,  %f906,  %f907,  %f908},{%r1002,  %r1003,  %r1004,  %r1005},{%r1017,  %r1018},{%f777, %f778, %f779, %f780};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f913,  %f914,  %f915,  %f916},{%r1002,  %r1003,  %r1004,  %r1005},{%r1019,  %r1020},{%f785, %f786, %f787, %f788};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f921,  %f922,  %f923,  %f924},{%r997,  %r998,  %r999,  %r1000},{%r1019,  %r1020},{%f793, %f794, %f795, %f796};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f929,  %f930,  %f931,  %f932},{%r997,  %r998,  %r999,  %r1000},{%r1022,  %r1023},{%f801, %f802, %f803, %f804};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f937,  %f938,  %f939,  %f940},{%r1002,  %r1003,  %r1004,  %r1005},{%r1022,  %r1023},{%f809, %f810, %f811, %f812};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f945,  %f946,  %f947,  %f948},{%r1002,  %r1003,  %r1004,  %r1005},{%r1024,  %r1025},{%f817, %f818, %f819, %f820};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f953,  %f954,  %f955,  %f956},{%r997,  %r998,  %r999,  %r1000},{%r1024,  %r1025},{%f825, %f826, %f827, %f828};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1123, %r1124, %r1125, %r1126}, [%r9];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1128, %r1129, %r1130, %r1131}, [%r10];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1133, %r1134, %r1135, %r1136}, [%r1572];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1138, %r1139, %r1140, %r1141}, [%r11];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1143, %r1144, %r1145, %r1146}, [%r1573];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1148, %r1149, %r1150, %r1151}, [%r12];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f961,  %f962,  %f963,  %f964},{%r1123,  %r1124,  %r1125,  %r1126},{%r1133,  %r1134},{%f833, %f834, %f835, %f836};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f969,  %f970,  %f971,  %f972},{%r1128,  %r1129,  %r1130,  %r1131},{%r1133,  %r1134},{%f841, %f842, %f843, %f844};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f977,  %f978,  %f979,  %f980},{%r1128,  %r1129,  %r1130,  %r1131},{%r1135,  %r1136},{%f849, %f850, %f851, %f852};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f985,  %f986,  %f987,  %f988},{%r1123,  %r1124,  %r1125,  %r1126},{%r1135,  %r1136},{%f857, %f858, %f859, %f860};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f993,  %f994,  %f995,  %f996},{%r1123,  %r1124,  %r1125,  %r1126},{%r1138,  %r1139},{%f865, %f866, %f867, %f868};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1001,  %f1002,  %f1003,  %f1004},{%r1128,  %r1129,  %r1130,  %r1131},{%r1138,  %r1139},{%f873, %f874, %f875, %f876};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1009,  %f1010,  %f1011,  %f1012},{%r1128,  %r1129,  %r1130,  %r1131},{%r1140,  %r1141},{%f881, %f882, %f883, %f884};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1017,  %f1018,  %f1019,  %f1020},{%r1123,  %r1124,  %r1125,  %r1126},{%r1140,  %r1141},{%f889, %f890, %f891, %f892};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1025,  %f1026,  %f1027,  %f1028},{%r1123,  %r1124,  %r1125,  %r1126},{%r1143,  %r1144},{%f897, %f898, %f899, %f900};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1033,  %f1034,  %f1035,  %f1036},{%r1128,  %r1129,  %r1130,  %r1131},{%r1143,  %r1144},{%f905, %f906, %f907, %f908};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1041,  %f1042,  %f1043,  %f1044},{%r1128,  %r1129,  %r1130,  %r1131},{%r1145,  %r1146},{%f913, %f914, %f915, %f916};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1049,  %f1050,  %f1051,  %f1052},{%r1123,  %r1124,  %r1125,  %r1126},{%r1145,  %r1146},{%f921, %f922, %f923, %f924};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1057,  %f1058,  %f1059,  %f1060},{%r1123,  %r1124,  %r1125,  %r1126},{%r1148,  %r1149},{%f929, %f930, %f931, %f932};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1065,  %f1066,  %f1067,  %f1068},{%r1128,  %r1129,  %r1130,  %r1131},{%r1148,  %r1149},{%f937, %f938, %f939, %f940};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1073,  %f1074,  %f1075,  %f1076},{%r1128,  %r1129,  %r1130,  %r1131},{%r1150,  %r1151},{%f945, %f946, %f947, %f948};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1081,  %f1082,  %f1083,  %f1084},{%r1123,  %r1124,  %r1125,  %r1126},{%r1150,  %r1151},{%f953, %f954, %f955, %f956};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1249, %r1250, %r1251, %r1252}, [%r13];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r1254, %r1255, %r1256, %r1257}, [%r1574];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1259, %r1260, %r1261, %r1262}, [%r1575];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1264, %r1265, %r1266, %r1267}, [%r14];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1269, %r1270, %r1271, %r1272}, [%r1576];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%r1274, %r1275, %r1276, %r1277}, [%r15];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1089,  %f1090,  %f1091,  %f1092},{%r1249,  %r1250,  %r1251,  %r1252},{%r1259,  %r1260},{%f961, %f962, %f963, %f964};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1097,  %f1098,  %f1099,  %f1100},{%r1254,  %r1255,  %r1256,  %r1257},{%r1259,  %r1260},{%f969, %f970, %f971, %f972};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1105,  %f1106,  %f1107,  %f1108},{%r1254,  %r1255,  %r1256,  %r1257},{%r1261,  %r1262},{%f977, %f978, %f979, %f980};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1113,  %f1114,  %f1115,  %f1116},{%r1249,  %r1250,  %r1251,  %r1252},{%r1261,  %r1262},{%f985, %f986, %f987, %f988};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1121,  %f1122,  %f1123,  %f1124},{%r1249,  %r1250,  %r1251,  %r1252},{%r1264,  %r1265},{%f993, %f994, %f995, %f996};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1129,  %f1130,  %f1131,  %f1132},{%r1254,  %r1255,  %r1256,  %r1257},{%r1264,  %r1265},{%f1001, %f1002, %f1003, %f1004};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1137,  %f1138,  %f1139,  %f1140},{%r1254,  %r1255,  %r1256,  %r1257},{%r1266,  %r1267},{%f1009, %f1010, %f1011, %f1012};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1145,  %f1146,  %f1147,  %f1148},{%r1249,  %r1250,  %r1251,  %r1252},{%r1266,  %r1267},{%f1017, %f1018, %f1019, %f1020};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1153,  %f1154,  %f1155,  %f1156},{%r1249,  %r1250,  %r1251,  %r1252},{%r1269,  %r1270},{%f1025, %f1026, %f1027, %f1028};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1161,  %f1162,  %f1163,  %f1164},{%r1254,  %r1255,  %r1256,  %r1257},{%r1269,  %r1270},{%f1033, %f1034, %f1035, %f1036};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1169,  %f1170,  %f1171,  %f1172},{%r1254,  %r1255,  %r1256,  %r1257},{%r1271,  %r1272},{%f1041, %f1042, %f1043, %f1044};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1177,  %f1178,  %f1179,  %f1180},{%r1249,  %r1250,  %r1251,  %r1252},{%r1271,  %r1272},{%f1049, %f1050, %f1051, %f1052};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1185,  %f1186,  %f1187,  %f1188},{%r1249,  %r1250,  %r1251,  %r1252},{%r1274,  %r1275},{%f1057, %f1058, %f1059, %f1060};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1193,  %f1194,  %f1195,  %f1196},{%r1254,  %r1255,  %r1256,  %r1257},{%r1274,  %r1275},{%f1065, %f1066, %f1067, %f1068};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1201,  %f1202,  %f1203,  %f1204},{%r1254,  %r1255,  %r1256,  %r1257},{%r1276,  %r1277},{%f1073, %f1074, %f1075, %f1076};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f1209,  %f1210,  %f1211,  %f1212},{%r1249,  %r1250,  %r1251,  %r1252},{%r1276,  %r1277},{%f1081, %f1082, %f1083, %f1084};

	// end inline asm
	bar.sync 	0;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f1089;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1218, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f1090;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1221, %rs4;}

	// end inline asm
	shr.u32 	%r1376, %r41, 8;
	and.b32  	%r1378, %r67, 2147479552;
	and.b32  	%r1380, %r1577, 448;
	add.s32 	%r1382, %r1376, %r1589;
	and.b32  	%r1383, %r1382, 1;
	shl.b32 	%r1388, %r1578, 3;
	and.b32  	%r1389, %r1388, 8;
	shl.b32 	%r1390, %r41, 1;
	and.b32  	%r1391, %r1390, 6;
	or.b32  	%r1393, %r1383, %r1590;
	shl.b32 	%r1394, %r1393, 5;
	or.b32  	%r1395, %r1380, %r1391;
	or.b32  	%r1396, %r1395, %r1389;
	or.b32  	%r1397, %r1396, %r1378;
	and.b32  	%r1398, %r1390, 16;
	or.b32  	%r1399, %r1397, %r1398;
	or.b32  	%r1400, %r1399, %r1394;
	shl.b32 	%r1401, %r1400, 1;
	add.s32 	%r1403, %r72, %r1401;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f1221;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f1218;}

	// end inline asm
	st.shared.v2.u16 	[%r1403], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f1091;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1224, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f1092;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1227, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f1227;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f1224;}

	// end inline asm
	st.shared.v2.u16 	[%r1403+1024], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f1097;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1230, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f1098;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1233, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f1233;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f1230;}

	// end inline asm
	st.shared.v2.u16 	[%r1403+4096], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f1099;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1236, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f1100;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1239, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f1239;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f1236;}

	// end inline asm
	st.shared.v2.u16 	[%r1403+5120], {%rs21, %rs24};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f1113;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1242, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f1114;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1245, %rs28;}

	// end inline asm
	shr.s32 	%r1404, %r41, 6;
	shl.b32 	%r1405, %r1404, 3;
	add.s32 	%r1406, %r1405, 16;
	shl.b32 	%r1407, %r1406, 6;
	and.b32  	%r1408, %r1407, 2147479552;
	shr.u32 	%r1409, %r1406, 5;
	add.s32 	%r1410, %r1409, %r1589;
	and.b32  	%r1411, %r1410, 1;
	xor.b32  	%r1412, %r1398, 16;
	or.b32  	%r1413, %r1411, %r1590;
	shl.b32 	%r1414, %r1413, 5;
	or.b32  	%r1415, %r1396, %r1408;
	or.b32  	%r1416, %r1415, %r1412;
	or.b32  	%r1417, %r1416, %r1414;
	shl.b32 	%r1418, %r1417, 1;
	add.s32 	%r1419, %r72, %r1418;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f1245;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f1242;}

	// end inline asm
	st.shared.v2.u16 	[%r1419], {%rs27, %rs30};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f1115;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1248, %rs31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f1116;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1251, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f1251;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f1248;}

	// end inline asm
	st.shared.v2.u16 	[%r1419+1024], {%rs33, %rs36};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f1105;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1254, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f1106;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1257, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f1257;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f1254;}

	// end inline asm
	st.shared.v2.u16 	[%r1419+4096], {%rs39, %rs42};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f1107;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1260, %rs43;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f1108;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1263, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f1263;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f1260;}

	// end inline asm
	st.shared.v2.u16 	[%r1419+5120], {%rs45, %rs48};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs49, %f1121;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1266, %rs49;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs52, %f1122;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1269, %rs52;}

	// end inline asm
	add.s32 	%r1420, %r1405, 32;
	shl.b32 	%r1421, %r1420, 6;
	and.b32  	%r1422, %r1421, 2147479552;
	shr.u32 	%r1423, %r1420, 5;
	add.s32 	%r1424, %r1423, %r1589;
	and.b32  	%r1425, %r1424, 1;
	or.b32  	%r1426, %r1425, %r1590;
	shl.b32 	%r1427, %r1426, 5;
	or.b32  	%r1428, %r1396, %r1422;
	or.b32  	%r1429, %r1428, %r1398;
	or.b32  	%r1430, %r1429, %r1427;
	shl.b32 	%r1431, %r1430, 1;
	add.s32 	%r1432, %r72, %r1431;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs54, %f1269;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs51, %f1266;}

	// end inline asm
	st.shared.v2.u16 	[%r1432], {%rs51, %rs54};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs55, %f1123;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1272, %rs55;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs58, %f1124;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1275, %rs58;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs60, %f1275;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs57, %f1272;}

	// end inline asm
	st.shared.v2.u16 	[%r1432+1024], {%rs57, %rs60};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs61, %f1129;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1278, %rs61;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs64, %f1130;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1281, %rs64;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs66, %f1281;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs63, %f1278;}

	// end inline asm
	st.shared.v2.u16 	[%r1432+4096], {%rs63, %rs66};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs67, %f1131;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1284, %rs67;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs70, %f1132;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1287, %rs70;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f1287;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs69, %f1284;}

	// end inline asm
	st.shared.v2.u16 	[%r1432+5120], {%rs69, %rs72};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs73, %f1145;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1290, %rs73;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs76, %f1146;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1293, %rs76;}

	// end inline asm
	add.s32 	%r1433, %r1405, 48;
	shl.b32 	%r1434, %r1433, 6;
	and.b32  	%r1435, %r1434, 2147479552;
	shr.u32 	%r1436, %r1433, 5;
	add.s32 	%r1437, %r1436, %r1589;
	and.b32  	%r1438, %r1437, 1;
	or.b32  	%r1439, %r1438, %r1590;
	shl.b32 	%r1440, %r1439, 5;
	or.b32  	%r1441, %r1396, %r1435;
	or.b32  	%r1442, %r1441, %r1412;
	or.b32  	%r1443, %r1442, %r1440;
	shl.b32 	%r1444, %r1443, 1;
	add.s32 	%r1445, %r72, %r1444;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs78, %f1293;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs75, %f1290;}

	// end inline asm
	st.shared.v2.u16 	[%r1445], {%rs75, %rs78};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs79, %f1147;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1296, %rs79;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs82, %f1148;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1299, %rs82;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs84, %f1299;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs81, %f1296;}

	// end inline asm
	st.shared.v2.u16 	[%r1445+1024], {%rs81, %rs84};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs85, %f1137;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1302, %rs85;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs88, %f1138;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1305, %rs88;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs90, %f1305;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs87, %f1302;}

	// end inline asm
	st.shared.v2.u16 	[%r1445+4096], {%rs87, %rs90};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs91, %f1139;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1308, %rs91;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs94, %f1140;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1311, %rs94;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs96, %f1311;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs93, %f1308;}

	// end inline asm
	st.shared.v2.u16 	[%r1445+5120], {%rs93, %rs96};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs97, %f1153;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1314, %rs97;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs100, %f1154;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1317, %rs100;}

	// end inline asm
	shl.b32 	%r1446, %r1404, 9;
	add.s32 	%r1447, %r1446, 4096;
	and.b32  	%r1448, %r1447, 2147479552;
	shr.u32 	%r1449, %r1404, 2;
	add.s32 	%r1450, %r1449, %r1589;
	and.b32  	%r1451, %r1450, 1;
	or.b32  	%r1452, %r1451, %r1590;
	shl.b32 	%r1453, %r1452, 5;
	or.b32  	%r1454, %r1396, %r1448;
	or.b32  	%r1455, %r1454, %r1398;
	or.b32  	%r1456, %r1455, %r1453;
	shl.b32 	%r1457, %r1456, 1;
	add.s32 	%r1458, %r72, %r1457;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs102, %f1317;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs99, %f1314;}

	// end inline asm
	st.shared.v2.u16 	[%r1458], {%rs99, %rs102};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs103, %f1155;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1320, %rs103;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs106, %f1156;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1323, %rs106;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs108, %f1323;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs105, %f1320;}

	// end inline asm
	st.shared.v2.u16 	[%r1458+1024], {%rs105, %rs108};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs109, %f1161;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1326, %rs109;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs112, %f1162;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1329, %rs112;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs114, %f1329;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs111, %f1326;}

	// end inline asm
	st.shared.v2.u16 	[%r1458+4096], {%rs111, %rs114};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs115, %f1163;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1332, %rs115;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs118, %f1164;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1335, %rs118;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs120, %f1335;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs117, %f1332;}

	// end inline asm
	st.shared.v2.u16 	[%r1458+5120], {%rs117, %rs120};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs121, %f1177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1338, %rs121;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs124, %f1178;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1341, %rs124;}

	// end inline asm
	add.s32 	%r1459, %r1446, 5120;
	and.b32  	%r1460, %r1459, 2147479552;
	or.b32  	%r1461, %r1396, %r1460;
	or.b32  	%r1462, %r1461, %r1412;
	or.b32  	%r1463, %r1462, %r1414;
	shl.b32 	%r1464, %r1463, 1;
	add.s32 	%r1465, %r72, %r1464;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs126, %f1341;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs123, %f1338;}

	// end inline asm
	st.shared.v2.u16 	[%r1465], {%rs123, %rs126};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs127, %f1179;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1344, %rs127;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs130, %f1180;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1347, %rs130;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs132, %f1347;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs129, %f1344;}

	// end inline asm
	st.shared.v2.u16 	[%r1465+1024], {%rs129, %rs132};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs133, %f1169;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1350, %rs133;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs136, %f1170;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1353, %rs136;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs138, %f1353;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs135, %f1350;}

	// end inline asm
	st.shared.v2.u16 	[%r1465+4096], {%rs135, %rs138};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs139, %f1171;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1356, %rs139;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs142, %f1172;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1359, %rs142;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs144, %f1359;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs141, %f1356;}

	// end inline asm
	st.shared.v2.u16 	[%r1465+5120], {%rs141, %rs144};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs145, %f1185;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1362, %rs145;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs148, %f1186;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1365, %rs148;}

	// end inline asm
	add.s32 	%r1466, %r1446, 6144;
	and.b32  	%r1467, %r1466, 2147479552;
	or.b32  	%r1468, %r1396, %r1467;
	or.b32  	%r1469, %r1468, %r1398;
	or.b32  	%r1470, %r1469, %r1427;
	shl.b32 	%r1471, %r1470, 1;
	add.s32 	%r1472, %r72, %r1471;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs150, %f1365;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs147, %f1362;}

	// end inline asm
	st.shared.v2.u16 	[%r1472], {%rs147, %rs150};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs151, %f1187;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1368, %rs151;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs154, %f1188;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1371, %rs154;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs156, %f1371;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs153, %f1368;}

	// end inline asm
	st.shared.v2.u16 	[%r1472+1024], {%rs153, %rs156};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs157, %f1193;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1374, %rs157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs160, %f1194;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1377, %rs160;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs162, %f1377;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs159, %f1374;}

	// end inline asm
	st.shared.v2.u16 	[%r1472+4096], {%rs159, %rs162};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs163, %f1195;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1380, %rs163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs166, %f1196;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1383, %rs166;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs168, %f1383;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs165, %f1380;}

	// end inline asm
	st.shared.v2.u16 	[%r1472+5120], {%rs165, %rs168};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs169, %f1209;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1386, %rs169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs172, %f1210;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1389, %rs172;}

	// end inline asm
	add.s32 	%r1473, %r1446, 7168;
	and.b32  	%r1474, %r1473, 2147479552;
	or.b32  	%r1475, %r1396, %r1474;
	or.b32  	%r1476, %r1475, %r1412;
	or.b32  	%r1477, %r1476, %r1440;
	shl.b32 	%r1478, %r1477, 1;
	add.s32 	%r1479, %r72, %r1478;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs174, %f1389;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs171, %f1386;}

	// end inline asm
	st.shared.v2.u16 	[%r1479], {%rs171, %rs174};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs175, %f1211;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1392, %rs175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs178, %f1212;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1395, %rs178;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs180, %f1395;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs177, %f1392;}

	// end inline asm
	st.shared.v2.u16 	[%r1479+1024], {%rs177, %rs180};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs181, %f1201;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1398, %rs181;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs184, %f1202;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1401, %rs184;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs186, %f1401;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs183, %f1398;}

	// end inline asm
	st.shared.v2.u16 	[%r1479+4096], {%rs183, %rs186};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs187, %f1203;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1404, %rs187;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs190, %f1204;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f1407, %rs190;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs192, %f1407;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs189, %f1404;}

	// end inline asm
	st.shared.v2.u16 	[%r1479+5120], {%rs189, %rs192};
	bar.sync 	0;
	add.s32 	%r1482, %r67, %r1582;
	shl.b32 	%r1483, %r1589, 6;
	shl.b32 	%r1484, %r1578, 5;
	and.b32  	%r1485, %r1484, 32;
	shl.b32 	%r1490, %r1584, 4;
	and.b32  	%r1491, %r1490, 16;
	shl.b32 	%r1493, %r1588, 3;
	and.b32  	%r1494, %r1493, 8;
	or.b32  	%r1495, %r1494, %r1483;
	or.b32  	%r1496, %r1495, %r1485;
	or.b32  	%r1497, %r1496, %r1491;
	cvta.to.global.u64 	%rd47, %rd51;
	mul.wide.s32 	%rd48, %r1482, 2;
	add.s64 	%rd49, %rd47, %rd48;
	shl.b32 	%r1498, %r41, 9;
	and.b32  	%r1499, %r1498, 4096;
	add.s32 	%r1500, %r1497, %r1499;
	shl.b32 	%r1501, %r1500, 1;
	add.s32 	%r1502, %r72, %r1501;
	ld.shared.v4.u32 	{%r1503, %r1504, %r1505, %r1506}, [%r1502];
	st.global.v4.u32 	[%rd49], {%r1503, %r1504, %r1505, %r1506};
	ld.shared.v4.u32 	{%r1511, %r1512, %r1513, %r1514}, [%r1502+1024];
	st.global.v4.u32 	[%rd49+2048], {%r1511, %r1512, %r1513, %r1514};
	ld.shared.v4.u32 	{%r1519, %r1520, %r1521, %r1522}, [%r1502+2048];
	st.global.v4.u32 	[%rd49+4096], {%r1519, %r1520, %r1521, %r1522};
	ld.shared.v4.u32 	{%r1527, %r1528, %r1529, %r1530}, [%r1502+3072];
	st.global.v4.u32 	[%rd49+6144], {%r1527, %r1528, %r1529, %r1530};
	ld.shared.v4.u32 	{%r1535, %r1536, %r1537, %r1538}, [%r1502+4096];
	st.global.v4.u32 	[%rd49+8192], {%r1535, %r1536, %r1537, %r1538};
	ld.shared.v4.u32 	{%r1543, %r1544, %r1545, %r1546}, [%r1502+5120];
	st.global.v4.u32 	[%rd49+10240], {%r1543, %r1544, %r1545, %r1546};
	ld.shared.v4.u32 	{%r1551, %r1552, %r1553, %r1554}, [%r1502+6144];
	st.global.v4.u32 	[%rd49+12288], {%r1551, %r1552, %r1553, %r1554};
	ld.shared.v4.u32 	{%r1559, %r1560, %r1561, %r1562}, [%r1502+7168];
	st.global.v4.u32 	[%rd49+14336], {%r1559, %r1560, %r1561, %r1562};
	ret;

}

