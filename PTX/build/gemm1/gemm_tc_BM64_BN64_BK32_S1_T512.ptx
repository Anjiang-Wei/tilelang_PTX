//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN64_INTERNAL_0b299bf5_33_gemm_tc_BM64_BN64_BK32_S1_T512_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 512, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<25>;
	.reg .f32 	%f<185>;
	.reg .b32 	%r<358>;
	.reg .b64 	%rd<14>;


	ld.param.u64 	%rd1, [main_kernel_param_0];
	ld.param.u64 	%rd2, [main_kernel_param_1];
	mov.u32 	%r10, %tid.x;
	shr.s32 	%r11, %r10, 3;
	shl.b32 	%r12, %r11, 5;
	and.b32  	%r13, %r10, 32;
	shr.u32 	%r14, %r13, 5;
	and.b32  	%r15, %r10, 4;
	shr.u32 	%r16, %r15, 2;
	add.s32 	%r17, %r14, %r16;
	shl.b32 	%r18, %r17, 4;
	and.b32  	%r19, %r18, 16;
	shr.u32 	%r20, %r10, 4;
	and.b32  	%r21, %r10, 2;
	shr.u32 	%r22, %r21, 1;
	add.s32 	%r23, %r22, %r20;
	shl.b32 	%r24, %r23, 3;
	and.b32  	%r25, %r24, 8;
	shl.b32 	%r26, %r10, 2;
	and.b32  	%r27, %r26, 4;
	or.b32  	%r28, %r27, %r12;
	or.b32  	%r29, %r28, %r19;
	or.b32  	%r30, %r29, %r25;
	shl.b32 	%r31, %r30, 1;
	mov.u32 	%r32, buf_dyn_shmem;
	add.s32 	%r1, %r32, %r31;
	mov.u32 	%r33, %ctaid.x;
	shl.b32 	%r34, %r33, 12;
	and.b32  	%r35, %r34, -262144;
	shl.b32 	%r36, %r11, 12;
	add.s32 	%r37, %r35, %r36;
	and.b32  	%r38, %r26, 28;
	or.b32  	%r2, %r37, %r38;
	shr.s32 	%r39, %r10, 31;
	shr.u32 	%r40, %r39, 28;
	add.s32 	%r41, %r10, %r40;
	and.b32  	%r42, %r41, -16;
	sub.s32 	%r43, %r10, %r42;
	shr.u32 	%r44, %r43, 31;
	add.s32 	%r45, %r43, %r44;
	shr.s32 	%r46, %r45, 1;
	shr.s32 	%r47, %r45, 31;
	shr.u32 	%r48, %r47, 30;
	add.s32 	%r49, %r46, %r48;
	and.b32  	%r50, %r49, -4;
	sub.s32 	%r51, %r46, %r50;
	shr.u32 	%r52, %r41, 31;
	shr.s32 	%r53, %r41, 4;
	add.s32 	%r54, %r53, %r52;
	and.b32  	%r55, %r54, -2;
	sub.s32 	%r56, %r53, %r55;
	shl.b32 	%r57, %r51, 6;
	and.b32  	%r58, %r57, 192;
	shl.b32 	%r59, %r56, 3;
	and.b32  	%r60, %r59, 8;
	or.b32  	%r61, %r58, %r60;
	and.b32  	%r62, %r45, 134217726;
	sub.s32 	%r63, %r43, %r62;
	shl.b32 	%r64, %r63, 5;
	shr.s32 	%r65, %r43, 31;
	shr.u32 	%r66, %r65, 29;
	add.s32 	%r67, %r43, %r66;
	shl.b32 	%r68, %r67, 5;
	and.b32  	%r69, %r68, 2147483392;
	add.s32 	%r70, %r64, %r69;
	shr.u32 	%r71, %r39, 27;
	add.s32 	%r72, %r10, %r71;
	shr.s32 	%r73, %r72, 5;
	shr.s32 	%r74, %r72, 31;
	shr.u32 	%r75, %r74, 30;
	add.s32 	%r76, %r73, %r75;
	and.b32  	%r77, %r76, 4194300;
	sub.s32 	%r78, %r73, %r77;
	shl.b32 	%r79, %r78, 9;
	add.s32 	%r80, %r70, %r79;
	and.b32  	%r81, %r51, 2;
	setp.eq.s32 	%p1, %r81, 0;
	shr.u32 	%r82, %r58, 3;
	xor.b32  	%r83, %r61, %r82;
	add.s32 	%r84, %r80, %r83;
	shr.u32 	%r85, %r39, 29;
	add.s32 	%r86, %r10, %r85;
	and.b32  	%r87, %r86, -8;
	sub.s32 	%r88, %r10, %r87;
	shr.u32 	%r89, %r88, 31;
	add.s32 	%r90, %r88, %r89;
	shr.s32 	%r91, %r90, 1;
	mov.u32 	%r357, 0;
	shl.b32 	%r92, %r91, 6;
	and.b32  	%r93, %r92, 192;
	and.b32  	%r94, %r86, 8;
	or.b32  	%r95, %r93, %r94;
	and.b32  	%r96, %r90, 67108862;
	sub.s32 	%r97, %r88, %r96;
	shl.b32 	%r98, %r97, 5;
	shl.b32 	%r99, %r56, 10;
	shr.u32 	%r100, %r39, 25;
	add.s32 	%r101, %r10, %r100;
	shl.b32 	%r102, %r101, 1;
	and.b32  	%r103, %r102, 2147483392;
	add.s32 	%r104, %r99, %r103;
	add.s32 	%r105, %r104, %r98;
	and.b32  	%r106, %r91, 2;
	setp.eq.s32 	%p2, %r106, 0;
	shr.u32 	%r107, %r93, 3;
	xor.b32  	%r108, %r95, %r107;
	add.s32 	%r109, %r105, %r108;
	shl.b32 	%r110, %r84, 1;
	add.s32 	%r3, %r32, %r110;
	shl.b32 	%r111, %r109, 1;
	add.s32 	%r112, %r32, %r111;
	add.s32 	%r4, %r112, 4096;
	selp.b32 	%r113, 32, -32, %p1;
	add.s32 	%r5, %r3, %r113;
	selp.b32 	%r114, 32, -32, %p2;
	add.s32 	%r6, %r4, %r114;
	mov.f32 	%f177, 0f00000000;
	cvta.to.global.u64 	%rd4, %rd1;
	cvta.to.global.u64 	%rd7, %rd2;
	mov.f32 	%f178, %f177;
	mov.f32 	%f179, %f177;
	mov.f32 	%f180, %f177;
	mov.f32 	%f181, %f177;
	mov.f32 	%f182, %f177;
	mov.f32 	%f183, %f177;
	mov.f32 	%f184, %f177;

$L__BB0_1:
	shl.b32 	%r291, %r357, 5;
	add.s32 	%r292, %r2, %r291;
	mul.wide.s32 	%rd5, %r292, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v2.u32 	{%r293, %r294}, [%rd6];
	st.shared.v2.u32 	[%r1], {%r293, %r294};
	shl.b32 	%r298, %r33, 18;
	and.b32  	%r299, %r298, 16515072;
	shl.b32 	%r301, %r10, 9;
	and.b32  	%r302, %r301, -4096;
	add.s32 	%r303, %r299, %r302;
	or.b32  	%r306, %r303, %r38;
	add.s32 	%r307, %r306, %r291;
	mul.wide.s32 	%rd8, %r307, 2;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.nc.v2.u32 	{%r308, %r309}, [%rd9];
	st.shared.v2.u32 	[%r1+4096], {%r308, %r309};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r115, %r116, %r117, %r118}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r120, %r121, %r122, %r123}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f25,  %f26,  %f27,  %f28},{%r115,  %r116,  %r117,  %r118},{%r120,  %r121},{%f184, %f183, %f182, %f181};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f33,  %f34,  %f35,  %f36},{%r115,  %r116,  %r117,  %r118},{%r122,  %r123},{%f180, %f179, %f178, %f177};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r137, %r138, %r139, %r140}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r142, %r143, %r144, %r145}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f41,  %f42,  %f43,  %f44},{%r137,  %r138,  %r139,  %r140},{%r142,  %r143},{%f25, %f26, %f27, %f28};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r137,  %r138,  %r139,  %r140},{%r144,  %r145},{%f33, %f34, %f35, %f36};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r312, %r313}, [%rd6+64];
	st.shared.v2.u32 	[%r1], {%r312, %r313};
	ld.global.nc.v2.u32 	{%r316, %r317}, [%rd9+64];
	st.shared.v2.u32 	[%r1+4096], {%r316, %r317};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r159, %r160, %r161, %r162}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r164, %r165, %r166, %r167}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r159,  %r160,  %r161,  %r162},{%r164,  %r165},{%f41, %f42, %f43, %f44};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r159,  %r160,  %r161,  %r162},{%r166,  %r167},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r181, %r182, %r183, %r184}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r186, %r187, %r188, %r189}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r181,  %r182,  %r183,  %r184},{%r186,  %r187},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r181,  %r182,  %r183,  %r184},{%r188,  %r189},{%f65, %f66, %f67, %f68};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r320, %r321}, [%rd6+128];
	st.shared.v2.u32 	[%r1], {%r320, %r321};
	ld.global.nc.v2.u32 	{%r324, %r325}, [%rd9+128];
	st.shared.v2.u32 	[%r1+4096], {%r324, %r325};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r203, %r204, %r205, %r206}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r208, %r209, %r210, %r211}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r203,  %r204,  %r205,  %r206},{%r208,  %r209},{%f73, %f74, %f75, %f76};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r203,  %r204,  %r205,  %r206},{%r210,  %r211},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r225, %r226, %r227, %r228}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r230, %r231, %r232, %r233}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r225,  %r226,  %r227,  %r228},{%r230,  %r231},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r225,  %r226,  %r227,  %r228},{%r232,  %r233},{%f97, %f98, %f99, %f100};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v2.u32 	{%r328, %r329}, [%rd6+192];
	st.shared.v2.u32 	[%r1], {%r328, %r329};
	ld.global.nc.v2.u32 	{%r332, %r333}, [%rd9+192];
	st.shared.v2.u32 	[%r1+4096], {%r332, %r333};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r247, %r248, %r249, %r250}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r252, %r253, %r254, %r255}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r247,  %r248,  %r249,  %r250},{%r252,  %r253},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r247,  %r248,  %r249,  %r250},{%r254,  %r255},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r269, %r270, %r271, %r272}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r274, %r275, %r276, %r277}, [%r6];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f184,  %f183,  %f182,  %f181},{%r269,  %r270,  %r271,  %r272},{%r274,  %r275},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f180,  %f179,  %f178,  %f177},{%r269,  %r270,  %r271,  %r272},{%r276,  %r277},{%f129, %f130, %f131, %f132};

	// end inline asm
	bar.sync 	0;
	add.s32 	%r357, %r357, 4;
	setp.ne.s32 	%p3, %r357, 128;
	@%p3 bra 	$L__BB0_1;

	shl.b32 	%r356, %r33, 12;
	and.b32  	%r355, %r356, -262144;
	ld.param.u64 	%rd13, [main_kernel_param_2];
	shl.b32 	%r337, %r10, 11;
	and.b32  	%r338, %r337, 196608;
	or.b32  	%r342, %r355, %r338;
	shl.b32 	%r343, %r10, 10;
	and.b32  	%r344, %r343, 28672;
	shl.b32 	%r345, %r33, 6;
	and.b32  	%r346, %r345, 4032;
	shr.s32 	%r347, %r10, 7;
	shl.b32 	%r348, %r347, 3;
	shl.b32 	%r349, %r10, 1;
	and.b32  	%r350, %r349, 6;
	or.b32  	%r351, %r342, %r346;
	add.s32 	%r352, %r351, %r348;
	or.b32  	%r353, %r352, %r350;
	add.s32 	%r354, %r353, %r344;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f184;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f154, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f183;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f157, %rs4;}

	// end inline asm
	cvta.to.global.u64 	%rd10, %rd13;
	mul.wide.s32 	%rd11, %r354, 2;
	add.s64 	%rd12, %rd10, %rd11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f157;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f154;}

	// end inline asm
	st.global.v2.u16 	[%rd12], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f182;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f160, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f163, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f163;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f160;}

	// end inline asm
	st.global.v2.u16 	[%rd12+65536], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f180;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f166, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f179;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f169, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f169;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f166;}

	// end inline asm
	st.global.v2.u16 	[%rd12+64], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f178;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f172, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f177;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f175, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f175;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f172;}

	// end inline asm
	st.global.v2.u16 	[%rd12+65600], {%rs21, %rs24};
	ret;

}

