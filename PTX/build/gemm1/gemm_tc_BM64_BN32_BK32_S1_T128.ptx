//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35583870
// Cuda compilation tools, release 12.8, V12.8.93
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_80
.address_size 64

	// .globl	main_kernel
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cute7productE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cute1_E[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo5beginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo3endE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo6cbeginE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__45__cpo4cendE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__419piecewise_constructE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std3__48in_placeE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo4swapE[1];
.global .align 1 .b8 _ZN64_INTERNAL_fb6b95b8_33_gemm_tc_BM64_BN32_BK32_S1_T128_cu_cc9b6e384cuda3std6ranges3__45__cpo9iter_moveE[1];
.extern .shared .align 1024 .b8 buf_dyn_shmem[];

.visible .entry main_kernel(
	.param .u64 main_kernel_param_0,
	.param .u64 main_kernel_param_1,
	.param .u64 main_kernel_param_2
)
.maxntid 128, 1, 1
.minnctapersm 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<49>;
	.reg .f32 	%f<241>;
	.reg .b32 	%r<339>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd8, [main_kernel_param_0];
	ld.param.u64 	%rd9, [main_kernel_param_1];
	ld.param.u64 	%rd7, [main_kernel_param_2];
	cvta.to.global.u64 	%rd10, %rd9;
	mov.u32 	%r11, %tid.x;
	shr.s32 	%r12, %r11, 2;
	shl.b32 	%r13, %r12, 5;
	shr.u32 	%r14, %r11, 4;
	and.b32  	%r15, %r11, 2;
	shr.u32 	%r16, %r15, 1;
	add.s32 	%r17, %r16, %r14;
	shl.b32 	%r18, %r17, 4;
	and.b32  	%r19, %r18, 16;
	and.b32  	%r20, %r11, 8;
	shr.u32 	%r21, %r20, 3;
	add.s32 	%r22, %r21, %r11;
	shl.b32 	%r23, %r22, 3;
	and.b32  	%r24, %r23, 8;
	or.b32  	%r25, %r19, %r13;
	or.b32  	%r26, %r25, %r24;
	mov.u32 	%r27, %ctaid.y;
	shl.b32 	%r28, %r27, 17;
	shl.b32 	%r29, %r11, 3;
	and.b32  	%r30, %r29, 24;
	shr.s32 	%r31, %r11, 31;
	shr.u32 	%r32, %r31, 28;
	add.s32 	%r33, %r11, %r32;
	and.b32  	%r34, %r33, -16;
	sub.s32 	%r35, %r11, %r34;
	shr.u32 	%r36, %r35, 31;
	add.s32 	%r37, %r35, %r36;
	shr.s32 	%r38, %r37, 1;
	shr.s32 	%r39, %r37, 31;
	shr.u32 	%r40, %r39, 30;
	add.s32 	%r41, %r38, %r40;
	and.b32  	%r42, %r41, -4;
	sub.s32 	%r43, %r38, %r42;
	shr.u32 	%r44, %r33, 31;
	shr.s32 	%r45, %r33, 4;
	add.s32 	%r46, %r45, %r44;
	and.b32  	%r47, %r46, -2;
	sub.s32 	%r48, %r45, %r47;
	shl.b32 	%r49, %r43, 6;
	and.b32  	%r50, %r49, 192;
	shl.b32 	%r51, %r48, 3;
	and.b32  	%r52, %r51, 8;
	or.b32  	%r53, %r50, %r52;
	and.b32  	%r54, %r37, 134217726;
	sub.s32 	%r55, %r35, %r54;
	shl.b32 	%r56, %r55, 5;
	shr.s32 	%r57, %r35, 31;
	shr.u32 	%r58, %r57, 29;
	add.s32 	%r59, %r35, %r58;
	shl.b32 	%r60, %r59, 5;
	and.b32  	%r61, %r60, 2147483392;
	add.s32 	%r62, %r56, %r61;
	shr.u32 	%r63, %r31, 27;
	add.s32 	%r64, %r11, %r63;
	shr.u32 	%r65, %r64, 31;
	shr.s32 	%r66, %r64, 5;
	add.s32 	%r67, %r66, %r65;
	and.b32  	%r68, %r67, 4194302;
	sub.s32 	%r69, %r66, %r68;
	shl.b32 	%r70, %r69, 9;
	add.s32 	%r71, %r62, %r70;
	and.b32  	%r72, %r43, 2;
	setp.eq.s32 	%p1, %r72, 0;
	shr.u32 	%r73, %r50, 3;
	xor.b32  	%r74, %r53, %r73;
	add.s32 	%r75, %r71, %r74;
	shr.u32 	%r76, %r31, 29;
	add.s32 	%r77, %r11, %r76;
	and.b32  	%r78, %r77, -8;
	sub.s32 	%r79, %r11, %r78;
	shr.u32 	%r80, %r79, 31;
	add.s32 	%r81, %r79, %r80;
	shr.s32 	%r82, %r81, 1;
	mov.u32 	%r338, 0;
	shl.b32 	%r83, %r82, 6;
	and.b32  	%r84, %r83, 192;
	and.b32  	%r85, %r77, 8;
	or.b32  	%r86, %r84, %r85;
	and.b32  	%r87, %r81, 67108862;
	sub.s32 	%r88, %r79, %r87;
	shl.b32 	%r89, %r88, 5;
	shl.b32 	%r90, %r48, 9;
	shr.u32 	%r91, %r31, 26;
	add.s32 	%r92, %r11, %r91;
	shl.b32 	%r93, %r92, 2;
	and.b32  	%r94, %r93, 2147483392;
	add.s32 	%r95, %r90, %r94;
	add.s32 	%r96, %r95, %r89;
	and.b32  	%r97, %r82, 2;
	setp.eq.s32 	%p2, %r97, 0;
	shr.u32 	%r98, %r84, 3;
	xor.b32  	%r99, %r86, %r98;
	add.s32 	%r100, %r96, %r99;
	shl.b32 	%r101, %r26, 1;
	mov.u32 	%r102, buf_dyn_shmem;
	add.s32 	%r1, %r102, %r101;
	shl.b32 	%r103, %r75, 1;
	add.s32 	%r2, %r102, %r103;
	add.s32 	%r3, %r2, 2048;
	shl.b32 	%r104, %r100, 1;
	add.s32 	%r105, %r102, %r104;
	add.s32 	%r4, %r105, 4096;
	selp.b32 	%r106, 32, -32, %p1;
	add.s32 	%r5, %r2, %r106;
	add.s32 	%r6, %r5, 2048;
	selp.b32 	%r107, 32, -32, %p2;
	add.s32 	%r7, %r4, %r107;
	or.b32  	%r108, %r30, %r28;
	shl.b32 	%r109, %r12, 11;
	add.s32 	%r110, %r108, %r109;
	mov.u32 	%r111, %ctaid.x;
	shl.b32 	%r112, %r111, 16;
	or.b32  	%r113, %r30, %r112;
	add.s32 	%r114, %r113, %r109;
	mul.wide.s32 	%rd11, %r114, 2;
	add.s64 	%rd18, %rd10, %rd11;
	cvta.to.global.u64 	%rd12, %rd8;
	mul.wide.s32 	%rd13, %r110, 2;
	add.s64 	%rd17, %rd12, %rd13;
	mov.f32 	%f225, 0f00000000;
	mov.f32 	%f226, %f225;
	mov.f32 	%f227, %f225;
	mov.f32 	%f228, %f225;
	mov.f32 	%f229, %f225;
	mov.f32 	%f230, %f225;
	mov.f32 	%f231, %f225;
	mov.f32 	%f232, %f225;
	mov.f32 	%f233, %f225;
	mov.f32 	%f234, %f225;
	mov.f32 	%f235, %f225;
	mov.f32 	%f236, %f225;
	mov.f32 	%f237, %f225;
	mov.f32 	%f238, %f225;
	mov.f32 	%f239, %f225;
	mov.f32 	%f240, %f225;

$L__BB0_1:
	ld.global.nc.v4.u32 	{%r271, %r272, %r273, %r274}, [%rd17];
	st.shared.v4.u32 	[%r1], {%r271, %r272, %r273, %r274};
	ld.global.nc.v4.u32 	{%r279, %r280, %r281, %r282}, [%rd17+131072];
	st.shared.v4.u32 	[%r1+2048], {%r279, %r280, %r281, %r282};
	ld.global.nc.v4.u32 	{%r287, %r288, %r289, %r290}, [%rd18];
	st.shared.v4.u32 	[%r1+4096], {%r287, %r288, %r289, %r290};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r115, %r116, %r117, %r118}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r120, %r121, %r122, %r123}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r125, %r126, %r127, %r128}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f49,  %f50,  %f51,  %f52},{%r115,  %r116,  %r117,  %r118},{%r125,  %r126},{%f240, %f239, %f238, %f237};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f57,  %f58,  %f59,  %f60},{%r120,  %r121,  %r122,  %r123},{%r125,  %r126},{%f236, %f235, %f234, %f233};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f65,  %f66,  %f67,  %f68},{%r120,  %r121,  %r122,  %r123},{%r127,  %r128},{%f228, %f227, %f226, %f225};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f73,  %f74,  %f75,  %f76},{%r115,  %r116,  %r117,  %r118},{%r127,  %r128},{%f232, %f231, %f230, %f229};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r154, %r155, %r156, %r157}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r159, %r160, %r161, %r162}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r164, %r165, %r166, %r167}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f81,  %f82,  %f83,  %f84},{%r154,  %r155,  %r156,  %r157},{%r164,  %r165},{%f49, %f50, %f51, %f52};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f89,  %f90,  %f91,  %f92},{%r159,  %r160,  %r161,  %r162},{%r164,  %r165},{%f57, %f58, %f59, %f60};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f97,  %f98,  %f99,  %f100},{%r159,  %r160,  %r161,  %r162},{%r166,  %r167},{%f65, %f66, %f67, %f68};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f105,  %f106,  %f107,  %f108},{%r154,  %r155,  %r156,  %r157},{%r166,  %r167},{%f73, %f74, %f75, %f76};

	// end inline asm
	bar.sync 	0;
	ld.global.nc.v4.u32 	{%r295, %r296, %r297, %r298}, [%rd17+64];
	st.shared.v4.u32 	[%r1], {%r295, %r296, %r297, %r298};
	ld.global.nc.v4.u32 	{%r303, %r304, %r305, %r306}, [%rd17+131136];
	st.shared.v4.u32 	[%r1+2048], {%r303, %r304, %r305, %r306};
	ld.global.nc.v4.u32 	{%r311, %r312, %r313, %r314}, [%rd18+64];
	st.shared.v4.u32 	[%r1+4096], {%r311, %r312, %r313, %r314};
	bar.sync 	0;
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r193, %r194, %r195, %r196}, [%r2];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r198, %r199, %r200, %r201}, [%r3];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r203, %r204, %r205, %r206}, [%r4];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f113,  %f114,  %f115,  %f116},{%r193,  %r194,  %r195,  %r196},{%r203,  %r204},{%f81, %f82, %f83, %f84};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f121,  %f122,  %f123,  %f124},{%r198,  %r199,  %r200,  %r201},{%r203,  %r204},{%f89, %f90, %f91, %f92};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f129,  %f130,  %f131,  %f132},{%r198,  %r199,  %r200,  %r201},{%r205,  %r206},{%f97, %f98, %f99, %f100};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f137,  %f138,  %f139,  %f140},{%r193,  %r194,  %r195,  %r196},{%r205,  %r206},{%f105, %f106, %f107, %f108};

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r232, %r233, %r234, %r235}, [%r5];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r237, %r238, %r239, %r240}, [%r6];

	// end inline asm
	// begin inline asm
	ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%r242, %r243, %r244, %r245}, [%r7];

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f240,  %f239,  %f238,  %f237},{%r232,  %r233,  %r234,  %r235},{%r242,  %r243},{%f113, %f114, %f115, %f116};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f236,  %f235,  %f234,  %f233},{%r237,  %r238,  %r239,  %r240},{%r242,  %r243},{%f121, %f122, %f123, %f124};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f228,  %f227,  %f226,  %f225},{%r237,  %r238,  %r239,  %r240},{%r244,  %r245},{%f129, %f130, %f131, %f132};

	// end inline asm
	// begin inline asm
	mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 {%f232,  %f231,  %f230,  %f229},{%r232,  %r233,  %r234,  %r235},{%r244,  %r245},{%f137, %f138, %f139, %f140};

	// end inline asm
	bar.sync 	0;
	add.s64 	%rd18, %rd18, 128;
	add.s64 	%rd17, %rd17, 128;
	add.s32 	%r338, %r338, 2;
	setp.ne.s32 	%p3, %r338, 64;
	@%p3 bra 	$L__BB0_1;

	mov.u32 	%r337, %tid.x;
	shl.b32 	%r320, %r337, 10;
	and.b32  	%r321, %r320, 32768;
	shl.b32 	%r322, %r337, 9;
	and.b32  	%r323, %r322, 14336;
	shl.b32 	%r325, %r111, 5;
	shr.s32 	%r326, %r337, 6;
	shl.b32 	%r327, %r326, 3;
	shl.b32 	%r328, %r337, 1;
	and.b32  	%r329, %r328, 6;
	add.s32 	%r332, %r325, %r28;
	add.s32 	%r333, %r332, %r321;
	add.s32 	%r334, %r333, %r327;
	or.b32  	%r335, %r334, %r329;
	add.s32 	%r336, %r335, %r323;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs1, %f240;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f178, %rs1;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs4, %f239;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f181, %rs4;}

	// end inline asm
	cvta.to.global.u64 	%rd14, %rd7;
	mul.wide.s32 	%rd15, %r336, 2;
	add.s64 	%rd16, %rd14, %rd15;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs6, %f181;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs3, %f178;}

	// end inline asm
	st.global.v2.u16 	[%rd16], {%rs3, %rs6};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs7, %f238;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f184, %rs7;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs10, %f237;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f187, %rs10;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs12, %f187;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs9, %f184;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32768], {%rs9, %rs12};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs13, %f236;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f190, %rs13;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs16, %f235;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f193, %rs16;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs18, %f193;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs15, %f190;}

	// end inline asm
	st.global.v2.u16 	[%rd16+131072], {%rs15, %rs18};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs19, %f234;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f196, %rs19;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs22, %f233;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f199, %rs22;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs24, %f199;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs21, %f196;}

	// end inline asm
	st.global.v2.u16 	[%rd16+163840], {%rs21, %rs24};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs25, %f232;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f202, %rs25;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs28, %f231;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f205, %rs28;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs30, %f205;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs27, %f202;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32], {%rs27, %rs30};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs31, %f230;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f208, %rs31;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs34, %f229;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f211, %rs34;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs36, %f211;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs33, %f208;}

	// end inline asm
	st.global.v2.u16 	[%rd16+32800], {%rs33, %rs36};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs37, %f228;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f214, %rs37;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs40, %f227;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f217, %rs40;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs42, %f217;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs39, %f214;}

	// end inline asm
	st.global.v2.u16 	[%rd16+131104], {%rs39, %rs42};
	// begin inline asm
	{  cvt.rn.f16.f32 %rs43, %f226;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f220, %rs43;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs46, %f225;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f223, %rs46;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs48, %f223;}

	// end inline asm
	// begin inline asm
	{  cvt.rn.f16.f32 %rs45, %f220;}

	// end inline asm
	st.global.v2.u16 	[%rd16+163872], {%rs45, %rs48};
	ret;

}

